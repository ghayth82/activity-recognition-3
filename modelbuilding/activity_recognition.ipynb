{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import scipy \n",
    "from scipy import optimize\n",
    "import scipy.signal as signal \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from detect_peaks import detect_peaks\n",
    "import quaternion as quat\n",
    "\n",
    "\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import requests\n",
    "import json\n",
    "# sns.set(style=\"darkgrid\")\n",
    "download_dir =  \"../G9_data/Downloaded\"\n",
    "# if not os.path.exists(download_dir):\n",
    "#     os.mkdir(download_dir)\n",
    "label_dict = {1:'walking',\n",
    "             2:'walking upstairs',\n",
    "             3:'walking downstairs',\n",
    "             4:'sitting',\n",
    "             5:'standing',\n",
    "             6:'laying',\n",
    "             7:'unknown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trained Model\n",
    "df_model = pd.read_csv(\"../modelbuilding/data/preprocessed/data.csv\", index_col=0)\n",
    "df_model.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 13., 14., 15.,\n",
       "       16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_model['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_training = df_model[df_model.columns[:-2]].values\n",
    "y_training = df_model['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_training, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interested_cols = [ 'AccX', 'AccY', 'AccZ', 'GyroX','GyroY', 'GyroZ']\n",
    "def create_datetime_index(df):\n",
    "    # convert unix time to german time and then to date\n",
    "    df['date'] = pd.to_datetime(df['system_time'],unit='ms')\n",
    "    # set datetime index\n",
    "    df.index = pd.DatetimeIndex(df[\"date\"])\n",
    "    df = df.drop(labels=[\"date\"],  axis=1)\n",
    "    return df\n",
    "def read_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "def get_sensor_data(df):\n",
    "\n",
    "    df = create_datetime_index(df)\n",
    "    df.sort_index(ascending = True, inplace = True)\n",
    "\n",
    "    # Sensor Selection\n",
    "    df_acc = df[df['sensor_name']=='Accelerometer']\n",
    "    df_gyro = df[df['sensor_name']=='Gyroscope']\n",
    "\n",
    "    # Merge accelerometer and gyroscope dataframes\n",
    "    df_merged = pd.merge(df_acc, df_gyro, left_index=True, right_index= True, how='inner')\n",
    "    df_merged.drop([\n",
    "     'sensor_name_x',\n",
    "     'value_x',\n",
    "     'id_y',\n",
    "     'sensor_name_y',\n",
    "     'system_time_y',\n",
    "     'value_y'], axis = 1, inplace = True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_merged = df_merged.loc[::2] # run ony once\n",
    "\n",
    "    # new column names\n",
    "    df_merged.columns = ['id','system_time', 'AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']\n",
    "    df_merged.sort_index(ascending = True, inplace=True)\n",
    "    return df_merged\n",
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "def extract_energy_variables(f, psdX):\n",
    "    \n",
    "    i_low = find_nearest(f,4.0)\n",
    "    i_high = find_nearest(f,7.0)\n",
    "    \n",
    "    energy_total = np.sum(psdX) \n",
    "    energy_interested = np.sum(psdX[i_low : i_high + 1]) \n",
    "    max_total = np.max(psdX)\n",
    "    max_interested = np.max(psdX[i_low : i_high + 1])\n",
    "    return energy_total,energy_interested, max_total, max_interested\n",
    "def average_over_axis(df):\n",
    "    aoa = df[interested_cols].mean(axis = 0)\n",
    "    aoa.index += '_aoa'\n",
    "    return aoa\n",
    "def average_time_elapse(df):\n",
    "    list_= []\n",
    "    for col in interested_cols:\n",
    "        a = df[col].values\n",
    "        mph = a.mean()\n",
    "        ind = detect_peaks(a, mph = mph, mpd=10, show=False)\n",
    "        list_.append(np.diff(ind).mean())\n",
    "    ate = pd.Series(list_, index=interested_cols)\n",
    "    ate.index += '_ate'\n",
    "    ate[pd.isnull(ate)]=0 #### TODO\n",
    "    return ate\n",
    "def average_peak_freq(df):\n",
    "    list_f= []\n",
    "    for col in interested_cols:\n",
    "        a = df[col].values\n",
    "        mph = a.mean()\n",
    "        ind = detect_peaks(a, mph = mph, mpd=10, show=False)\n",
    "        list_f.append(len(ind)/a.shape[0])\n",
    "    apf = pd.Series(list_f, index=interested_cols)\n",
    "    apf.index += '_apf'\n",
    "    return apf\n",
    "def rms_func(df):\n",
    "    list_= []\n",
    "    for col in interested_cols:\n",
    "        a = df[col].values\n",
    "        rms_temp = np.sqrt(np.mean(a**2))\n",
    "        list_.append(rms_temp)\n",
    "    rms = pd.Series(list_, index=interested_cols)\n",
    "    rms.index += '_rms'\n",
    "    return rms\n",
    "def std_func(df):\n",
    "    list_= []\n",
    "    for col in interested_cols:\n",
    "        a = df[col].values\n",
    "        std_temp = np.std(a)\n",
    "        list_.append(std_temp)\n",
    "    std = pd.Series(list_, index=interested_cols)\n",
    "    std.index += '_std'\n",
    "    return std\n",
    "def minmax_func(df):\n",
    "    list_= []\n",
    "    for col in interested_cols:\n",
    "        a = df[col].values\n",
    "        minmax_temp = np.max(a)-np.min(a)\n",
    "        list_.append(minmax_temp)\n",
    "    minmax = pd.Series(list_, index=interested_cols)\n",
    "    minmax.index += '_minmax'\n",
    "    return minmax\n",
    "def cor_func(df):\n",
    "    a = df[interested_cols[:3]].corr()\n",
    "    b= df[interested_cols[3:]].corr()\n",
    "    indexes = ['CorAccXAccY','CorAccXAccZ','CorAccYAccZ', 'CorGyroXGyroY','CorGyroXGyroZ','CorGyroYGyroZ']\n",
    "    Cor = (a['AccX'][1:]).append(a['AccY'][2:]).append((b['GyroX'][1:]).append(b['GyroY'][2:]))\n",
    "    Cor[pd.isnull(Cor)]=0 ### TODO\n",
    "    corr = pd.Series(Cor.values, indexes)\n",
    "    corr.index += '_corr'\n",
    "    return corr\n",
    "def get_all_features(df, file):\n",
    "    \n",
    "    aoa = average_over_axis(df)\n",
    "    ate = average_time_elapse(df)\n",
    "    apf = average_peak_freq(df)\n",
    "    rms = rms_func(df)\n",
    "    std = std_func(df)\n",
    "    minmax = minmax_func(df)\n",
    "    cor = cor_func(df)\n",
    "    ser_list = [aoa, ate,apf, rms,std, minmax, cor]\n",
    "    ser = pd.concat(ser_list)\n",
    "    ser.name = file\n",
    "    return ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Content is coherent with: https://pdfs.semanticscholar.org/7fa5/eebe34cd2efc0b48eb60fb62fbd0bb5f33ba.pdf\n",
    "    #In line (20) of the paper, only one step of gradient descent is performed. In this function, maximally ten steps are taken\n",
    "    \n",
    "    #Input:\n",
    "        #Window of signals (pandas df)\n",
    "        #samplerate: samplerate (Default: 62.5 Hz)\n",
    "        #alpha: parameter alpha according to (22) (Default: 1.1; needs to be adapted for each watch)\n",
    "        #beta: parameter beta according to (33) (Default: 0.026266837793051865 for Microsoft Band 2)\n",
    "\n",
    "    #Output:\n",
    "        #Numpy array q_est containing the estimated rotation quaternions for each sample number (not depending on time) \n",
    "\n",
    "def do_pose_estimation(window, samplerate = 62.5, alpha = 1.1, beta = 0.026266837793051865):\n",
    "    \n",
    "    #dependencies\n",
    "    import quaternion as quat\n",
    "    from scipy import optimize\n",
    "    import time\n",
    "    \n",
    "    starttime = time.time()\n",
    "    \n",
    "    'HELPER FUNCTIONS ##########################################################################################################'\n",
    "    #Function that normalizes a quaternion\n",
    "    def quat_normalize(q):\n",
    "        return np.divide(q, np.absolute(q))\n",
    "    \n",
    "    #Function that converts a quaternion into a 4 dimensional numpy array\n",
    "    def quat_as_array(q):\n",
    "        return quat.as_float_array(q)\n",
    "    \n",
    "    #Quaternion derivative according to (3)\n",
    "    def quat_der(t, q_est, gyr_quat):\n",
    "        return 1/2 * np.multiply(quat_normalize(q_est[t-1]), gyr_quat[t])\n",
    "    \n",
    "    #Pose Estimation according to gyro integration according to (4)\n",
    "    def q_omega(t, q_est, gyr_quat, delta_t):\n",
    "        return np.add(quat_normalize(q_est[t-1]), quat_der(t, q_est, gyr_quat) * delta_t[t-1])\n",
    "    \n",
    "    #Definition of vector valued objective function f_g for gravity based pose estimation according to (12) \n",
    "    def f_g(q, s):\n",
    "        return np.array([2*(q[1]*q[3] - q[0]*q[2]) - s[1],\n",
    "                         2*(q[0]*q[1] + q[2]*q[3]) - s[2],\n",
    "                         2*(1/2 - q[1]**2 - q[2]**2) - s[3]])\n",
    "    \n",
    "    #Jacobian matrix of objective function f_g according to (13)\n",
    "    def J_g(q):\n",
    "        return np.array([[-2*q[2], 2*q[3], -2*q[0], 2*q[1]], \n",
    "                         [2*q[1], 2*q[0], 2*q[3], 2*q[2]], \n",
    "                         [0, -4*q[1], -4*q[2], 0]])\n",
    "    \n",
    "    #Gradient of scalar valued objective function f according to (21)\n",
    "    def grad_f(q, s):\n",
    "        #Dot product is row wise and works like matrix product\n",
    "        return np.transpose(J_g(q)).dot(f_g(q, s))\n",
    "    \n",
    "    #Mu according to (22)\n",
    "    def mu(t, q_est, gyr_quat, delta_t):\n",
    "        return alpha * np.absolute(quat_der(t, q_est, gyr_quat)) * delta_t[t]\n",
    "    \n",
    "    #Gravity based pose estimation resulting from full gradient descent (adapted from (20)) \n",
    "    def q_nabla(t, q_est, acc_quat):\n",
    "        s = quat_as_array(acc_quat[t])\n",
    "        q_0 = quat_as_array(q_est[t-1])\n",
    "        #Objective function depending on only one input parameter for optimization\n",
    "        def func(q):\n",
    "            return 1/2 * np.linalg.norm(f_g(q, s))**2\n",
    "        #Gradient of objective function depending on only one input parameter for optimization \n",
    "        def grad_func(q):\n",
    "            return grad_f(q, s)\n",
    "        return quat.as_quat_array(scipy.optimize.minimize(fun = func, x0 = q_0, jac = grad_func, options={'maxiter': 10}).x)\n",
    "    \n",
    "    #Gamma according to (25) \n",
    "    def gamma(t, q_est, gyr_quat, delta_t):\n",
    "        return beta / ((mu(t, q_est, gyr_quat, delta_t)/delta_t[t]) + beta)\n",
    "    \n",
    "    #Pose estimation quaternion according to (23)\n",
    "    def q_estimate(t, q_est, acc_quat, gyr_quat, delta_t):\n",
    "        gamma_t = gamma(t, q_est, gyr_quat, delta_t)\n",
    "        return gamma_t * q_nabla(t, q_est, acc_quat) + (1 - gamma_t) * q_omega(t, q_est, gyr_quat, delta_t)\n",
    "    '###########################################################################################################################'\n",
    "    \n",
    "    acc = window[interested_cols[:3]]\n",
    "    gyr = window[interested_cols[3:]]\n",
    "    \n",
    "    #Data size for variable size definitions\n",
    "    nrows = acc.shape[0]\n",
    "    ncols = acc.shape[1]\n",
    "    \n",
    "    #Convert gyroscope data to quaternions according to (1): gyr_quat\n",
    "    gyr_quat = np.zeros((nrows, ncols + 1))\n",
    "    gyr_quat[:, range(1, ncols + 1)] = gyr\n",
    "    gyr_quat = quat.as_quat_array(gyr_quat)\n",
    "\n",
    "    #Generate accelerometer data quaternion according to (11): acc_quat\n",
    "    acc_quat = np.zeros((nrows, ncols + 1))\n",
    "    acc_quat[:, range(1, ncols + 1)] = acc\n",
    "    acc_quat = quat.as_quat_array(acc_quat)\n",
    "    \n",
    "    #delta_t vector for time between timestamps\n",
    "    'Current version: Assume constant delta_t. May be adapted in the future.'\n",
    "    delta_t = (1/samplerate) * np.ones(nrows)\n",
    "\n",
    "    #Initialize empty numpy Array containing pose estimation quaternions \n",
    "    q_est = quat.as_quat_array(np.zeros((nrows, ncols + 1)))\n",
    "\n",
    "    #First estimate of rotation quaternion: Rotation of 0Â° around the axis defined by [0, 0, 1], i. e., no rotation\n",
    "    q_0 = quat.from_rotation_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n",
    "    q_est[0] = q_0\n",
    "    \n",
    "    #Run pose estimation algorithm and save all estimated quaternions in q_est \n",
    "    for t in range(1,nrows):\n",
    "        q_est[t] = q_estimate(t, q_est, acc_quat, gyr_quat, delta_t)\n",
    "    \n",
    "    endtime = time.time()\n",
    "    print('Pose esimation finished! Elapsed time:', time.strftime(\"%H:%M:%S\", time.gmtime(endtime - starttime)))\n",
    "        \n",
    "    return np.degrees(quat.as_euler_angles(q_est))\n",
    "#quat.as_float_array(q_est) #euler angles as possible further output: euler = np.degrees(quat.as_euler_angles(q_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_to_str(t):\n",
    "    t_woM = t.replace(microsecond=0)\n",
    "\n",
    "    dt64 = np.datetime64(t_woM)\n",
    "    dt64\n",
    "\n",
    "    a = dt64.astype('datetime64[s]')\n",
    "\n",
    "    return np.datetime_as_string(a)+\"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Blob name: 2017-11-29/861452078_35dbed8e8c614be8aebdb5b5dbfee0d3_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-11-29/861452078_675bc4bf31d94968974e58c2a18f7519_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-11-30/873240569_c3066418061948fdbc1f1fdcde734e65_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-05/873240569_6644c7aa2d104486b60aa0428b6e5dce_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-06/873240569_34f3bf5800304afba5659dd3fe4ea167_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-07/873240569_4cd837cbf09141be8211a802db1b649b_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-08/873240569_3a862ff4cf5c40f08c26524da382e227_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-11/873240569_231439354b5a46ada32e77abee3ddbba_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-12/873240569_075dc9e3d8ea4e9b8647d834e571f15e_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-13/873240569_b4a0e0c810ae426aac376cefacb78ec8_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-14/873240569_d50457a7bb9b4e43bf5033eb8a9ac439_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-15/873240569_e466b7358be540cf95ac10e896964e8c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-18/873240569_54203aac25764c77a7d56831eec30b6b_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-19/873240569_58c8ed6c6c294185a39c89584581408c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-20/873240569_8aab9182b114415287f7c1bf24d8c403_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-26/873240569_30c58d773512438c8a64bf1280153088_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2017-12-29/873240569_dea7d6e9813e49b490b1205d7bc37e3f_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-02/873240569_6261be59649c49f98a8afe3fd4718430_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-03/873240569_84e3db400d73424e96e7a920534f198a_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-05/873240569_9f6bade5e0d646119c2ff489088de5a9_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-08/873240569_4899011d8b0d4bed930d5229e2d41f2c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-09/873240569_12c3f6c63b684a9cabb6d45a0421b13c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-11/873240569_334a69ba03ae4de1b4c7fcb80567beae_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-15/873240569_a4faeb39a9e1464dbfb859f07876b1d0_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-16/873240569_d9e36c5bd450485fa74d0184c6f16f9c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-17/873240569_c73211ce50e446e9bdfc05f82443deff_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-18/873240569_b47d919e3ca644b58613eac05f33d654_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-19/873240569_479af0405a5d4e71b674e9a9b5b523f2_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-22/873240569_cea874197f6841a6bddfc346fecbc655_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-23/873240569_f2aa2364b9424e8e99229a35aa4a30fd_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-24/873240569_43a5a195d2ab4308a9eca5fe740c9c2b_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-25/873240569_765be3d2c37f4117bece7df3df8247bb_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-26/873240569_139a3abeb07a451d9b9c91d4859f05f0_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-27/873240569_864fc781895f429fad8674f4e501f787_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-28/873240569_d5b5390328b447f9afa01022164acf36_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-29/873240569_4d33d6d5f0d040cda91b5928813bfffa_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-30/873240569_c9866a36be30476ca6f360a760f9140a_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-01-31/873240569_3d2d8ac39b8f4e9790241c5a88e9ebb5_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-01/873240569_53ed5daa2bc643cd9967c101a4269b53_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-02/873240569_d7508fea42b3402c8fa7cb7562e34546_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-05/873240569_7c22a39d0b9f493ca9bda4965db9036a_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-05/873240569_89cd715b56164cdd9f612f00d594f827_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-06/873240569_d283a77134814fc39e37f7353da68a5c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-07/873240569_6c9c8210bfb648b9bff5b7f86ed50452_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-08/873240569_9325cb924c264544890ca8ab39689493_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-09/873240569_0dd1ebce3e83455ea211f62ca01cc8b5_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-10/873240569_5e6f63b9cd2541f59bb314da9582ca39_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-11/873240569_a6a963db4840445a9a00479aec5b8ebf_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-12/873240569_320d652b14b847058762381c750ada64_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-13/873240569_4b2777140cb146ce81f5deaa2cd5e1dd_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-14/873240569_0e9f6bef6dcf476c933dcdbd38e49b89_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-20/873240569_ab7e9621dd8b4d79982d1364ebf386c3_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-21/873240569_c33eb28cf77f435c9e7912ea212ca77c_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-23/873240569_d9ec356b0a2d4826a473730043044e31_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-24/873240569_d0c67b995a8b4d33b900d3f801672fab_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-25/873240569_2d0be2b02d1648a1b261835208df796a_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-26/873240569_fde039701ef0480fba7e2035158b62e7_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-27/873240569_bafedc6f3e714c95b1083d6d79e48d0f_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-02-28/873240569_f9b87bf81a4a418c9617c980e7b8b45d_1.csv\n",
      "Already downloaded \n",
      "\t Blob name: 2018-03-01/873240569_1f4a36aba82e48a39a226c7544cc4823_1.csv\n",
      "Already downloaded \n"
     ]
    }
   ],
   "source": [
    "block_blob_service = BlockBlobService(account_name='watchstorage', account_key='TJWcjsCs4aK9Xorw4DIAZGvKz0AFb2kvgSh49t+3nADR2usZ1ED14GLBQ/klJsSSrKykxu0ghCXn46+0bv2J8Q==')\n",
    "\n",
    "container_name = 'jnj'\n",
    "generator = block_blob_service.list_blobs(container_name)\n",
    "\n",
    "# Download each day movement data \n",
    "for blob in generator:\n",
    "    print(\"\\t Blob name: \" + blob.name)\n",
    "    if not os.path.exists(os.path.join(download_dir, blob.name)) : # check filecmp.cmp()\n",
    "        if not os.path.exists(os.path.join(download_dir, blob.name.split('/')[0])):\n",
    "            os.mkdir(os.path.join(download_dir, blob.name.split('/')[0]))\n",
    "        print(\"downloading...\")\n",
    "        block_blob_service.get_blob_to_path(container_name, blob.name,os.path.join(download_dir, blob.name ))\n",
    "        print(\"downloaded\")\n",
    "    else:\n",
    "        print(\"Already downloaded \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-28\n",
      "2018-03-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "yesterday = date.today() - timedelta(1)\n",
    "yesterday = yesterday.strftime('%Y-%m-%d')\n",
    "today = date.today().strftime('%Y-%m-%d')\n",
    "dates_to_consider = [(date.today() - timedelta(i)).strftime('%Y-%m-%d') for i in range(0,6)]\n",
    "print(yesterday)\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for day in dates_to_consider:\n",
    "    path = os.path.join(download_dir, day)\n",
    "    if os.path.exists(path):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            if files[0].endswith(\".csv\"):\n",
    "                file_path = path+'/'+files[0]\n",
    "                df_list.append(pd.read_csv(file_path))\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_time</th>\n",
       "      <th>sensor_name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch-sensor-356969030512673</td>\n",
       "      <td>1519873288523</td>\n",
       "      <td>Accelerometer</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.095768</td>\n",
       "      <td>9.342175</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watch-sensor-356969030512673</td>\n",
       "      <td>1519873288551</td>\n",
       "      <td>Accelerometer</td>\n",
       "      <td>0.335188</td>\n",
       "      <td>0.105345</td>\n",
       "      <td>9.294291</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>watch-sensor-356969030512673</td>\n",
       "      <td>1519873288566</td>\n",
       "      <td>Accelerometer</td>\n",
       "      <td>0.325611</td>\n",
       "      <td>0.095768</td>\n",
       "      <td>9.346964</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch-sensor-356969030512673</td>\n",
       "      <td>1519873288586</td>\n",
       "      <td>Accelerometer</td>\n",
       "      <td>0.311246</td>\n",
       "      <td>0.114922</td>\n",
       "      <td>9.303867</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch-sensor-356969030512673</td>\n",
       "      <td>1519873288638</td>\n",
       "      <td>Accelerometer</td>\n",
       "      <td>0.344765</td>\n",
       "      <td>0.090980</td>\n",
       "      <td>9.313444</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id    system_time    sensor_name         x  \\\n",
       "0  watch-sensor-356969030512673  1519873288523  Accelerometer  0.330400   \n",
       "1  watch-sensor-356969030512673  1519873288551  Accelerometer  0.335188   \n",
       "2  watch-sensor-356969030512673  1519873288566  Accelerometer  0.325611   \n",
       "3  watch-sensor-356969030512673  1519873288586  Accelerometer  0.311246   \n",
       "4  watch-sensor-356969030512673  1519873288638  Accelerometer  0.344765   \n",
       "\n",
       "          y         z  value  \n",
       "0  0.095768  9.342175    NaN  \n",
       "1  0.105345  9.294291    NaN  \n",
       "2  0.095768  9.346964    NaN  \n",
       "3  0.114922  9.303867    NaN  \n",
       "4  0.090980  9.313444    NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watch-sensor-356969030512673\n",
      "watch-sensor-356969030312710\n",
      "watch-sensor-356969030312827\n",
      "watch-sensor-356969030312769\n",
      "watch-sensor-356969030312660\n"
     ]
    }
   ],
   "source": [
    " for x in list(df['id'].unique()):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Watch ', 'watch-sensor-356969030512673', ' is being processed')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing time:', Timestamp('2018-02-28 16:00:00'), ' - ', Timestamp('2018-02-28 16:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:05:00'), ' - ', Timestamp('2018-02-28 16:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:10:00'), ' - ', Timestamp('2018-02-28 16:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:15:00'), ' - ', Timestamp('2018-02-28 16:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:20:00'), ' - ', Timestamp('2018-02-28 16:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:25:00'), ' - ', Timestamp('2018-02-28 16:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:30:00'), ' - ', Timestamp('2018-02-28 16:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:35:00'), ' - ', Timestamp('2018-02-28 16:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:40:00'), ' - ', Timestamp('2018-02-28 16:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:45:00'), ' - ', Timestamp('2018-02-28 16:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:50:00'), ' - ', Timestamp('2018-02-28 16:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 16:55:00'), ' - ', Timestamp('2018-02-28 17:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:00:00'), ' - ', Timestamp('2018-02-28 17:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:05:00'), ' - ', Timestamp('2018-02-28 17:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:10:00'), ' - ', Timestamp('2018-02-28 17:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:15:00'), ' - ', Timestamp('2018-02-28 17:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:20:00'), ' - ', Timestamp('2018-02-28 17:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:25:00'), ' - ', Timestamp('2018-02-28 17:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:30:00'), ' - ', Timestamp('2018-02-28 17:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:35:00'), ' - ', Timestamp('2018-02-28 17:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:40:00'), ' - ', Timestamp('2018-02-28 17:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:45:00'), ' - ', Timestamp('2018-02-28 17:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:50:00'), ' - ', Timestamp('2018-02-28 17:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 17:55:00'), ' - ', Timestamp('2018-02-28 18:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:00:00'), ' - ', Timestamp('2018-02-28 18:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:05:00'), ' - ', Timestamp('2018-02-28 18:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:10:00'), ' - ', Timestamp('2018-02-28 18:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:15:00'), ' - ', Timestamp('2018-02-28 18:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:20:00'), ' - ', Timestamp('2018-02-28 18:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:25:00'), ' - ', Timestamp('2018-02-28 18:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:30:00'), ' - ', Timestamp('2018-02-28 18:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:35:00'), ' - ', Timestamp('2018-02-28 18:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:40:00'), ' - ', Timestamp('2018-02-28 18:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:45:00'), ' - ', Timestamp('2018-02-28 18:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:50:00'), ' - ', Timestamp('2018-02-28 18:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 18:55:00'), ' - ', Timestamp('2018-02-28 19:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:00:00'), ' - ', Timestamp('2018-02-28 19:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:05:00'), ' - ', Timestamp('2018-02-28 19:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:10:00'), ' - ', Timestamp('2018-02-28 19:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:15:00'), ' - ', Timestamp('2018-02-28 19:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:20:00'), ' - ', Timestamp('2018-02-28 19:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:25:00'), ' - ', Timestamp('2018-02-28 19:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:30:00'), ' - ', Timestamp('2018-02-28 19:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:35:00'), ' - ', Timestamp('2018-02-28 19:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:40:00'), ' - ', Timestamp('2018-02-28 19:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:45:00'), ' - ', Timestamp('2018-02-28 19:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:50:00'), ' - ', Timestamp('2018-02-28 19:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 19:55:00'), ' - ', Timestamp('2018-02-28 20:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:00:00'), ' - ', Timestamp('2018-02-28 20:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:05:00'), ' - ', Timestamp('2018-02-28 20:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:10:00'), ' - ', Timestamp('2018-02-28 20:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:15:00'), ' - ', Timestamp('2018-02-28 20:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:20:00'), ' - ', Timestamp('2018-02-28 20:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:25:00'), ' - ', Timestamp('2018-02-28 20:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:30:00'), ' - ', Timestamp('2018-02-28 20:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:35:00'), ' - ', Timestamp('2018-02-28 20:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:40:00'), ' - ', Timestamp('2018-02-28 20:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:45:00'), ' - ', Timestamp('2018-02-28 20:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:50:00'), ' - ', Timestamp('2018-02-28 20:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 20:55:00'), ' - ', Timestamp('2018-02-28 21:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:00:00'), ' - ', Timestamp('2018-02-28 21:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:05:00'), ' - ', Timestamp('2018-02-28 21:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:10:00'), ' - ', Timestamp('2018-02-28 21:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:15:00'), ' - ', Timestamp('2018-02-28 21:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:20:00'), ' - ', Timestamp('2018-02-28 21:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:25:00'), ' - ', Timestamp('2018-02-28 21:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:30:00'), ' - ', Timestamp('2018-02-28 21:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:35:00'), ' - ', Timestamp('2018-02-28 21:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:40:00'), ' - ', Timestamp('2018-02-28 21:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:45:00'), ' - ', Timestamp('2018-02-28 21:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:50:00'), ' - ', Timestamp('2018-02-28 21:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 21:55:00'), ' - ', Timestamp('2018-02-28 22:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:00:00'), ' - ', Timestamp('2018-02-28 22:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:05:00'), ' - ', Timestamp('2018-02-28 22:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:10:00'), ' - ', Timestamp('2018-02-28 22:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:15:00'), ' - ', Timestamp('2018-02-28 22:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:20:00'), ' - ', Timestamp('2018-02-28 22:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:25:00'), ' - ', Timestamp('2018-02-28 22:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:30:00'), ' - ', Timestamp('2018-02-28 22:35:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:35:00'), ' - ', Timestamp('2018-02-28 22:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:40:00'), ' - ', Timestamp('2018-02-28 22:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:45:00'), ' - ', Timestamp('2018-02-28 22:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:50:00'), ' - ', Timestamp('2018-02-28 22:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 22:55:00'), ' - ', Timestamp('2018-02-28 23:00:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:00:00'), ' - ', Timestamp('2018-02-28 23:05:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:05:00'), ' - ', Timestamp('2018-02-28 23:10:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:10:00'), ' - ', Timestamp('2018-02-28 23:15:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:15:00'), ' - ', Timestamp('2018-02-28 23:20:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:20:00'), ' - ', Timestamp('2018-02-28 23:25:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:25:00'), ' - ', Timestamp('2018-02-28 23:30:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:30:00'), ' - ', Timestamp('2018-02-28 23:35:00'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing time:', Timestamp('2018-02-28 23:35:00'), ' - ', Timestamp('2018-02-28 23:40:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:40:00'), ' - ', Timestamp('2018-02-28 23:45:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:45:00'), ' - ', Timestamp('2018-02-28 23:50:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:50:00'), ' - ', Timestamp('2018-02-28 23:55:00'))\n",
      "('doing time:', Timestamp('2018-02-28 23:55:00'), ' - ', Timestamp('2018-03-01 00:00:00'))\n",
      "('doing time:', Timestamp('2018-03-01 00:00:00'), ' - ', Timestamp('2018-03-01 00:05:00'))\n",
      "('doing time:', Timestamp('2018-03-01 00:05:00'), ' - ', Timestamp('2018-03-01 00:10:00'))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'detect_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a69035b0c3c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msnippet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mincrement\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnippet_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1abfb1a566bb>\u001b[0m in \u001b[0;36mget_all_features\u001b[0;34m(df, file)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0maoa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_over_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_time_elapse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mapf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_peak_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mrms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrms_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1abfb1a566bb>\u001b[0m in \u001b[0;36maverage_time_elapse\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_peaks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmpd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mlist_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterested_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'detect_peaks' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wPHFxyibnxtOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb52f03be50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for watch_id in [df['id'].unique():\n",
    "    #TODO: Select all watches and do the predictions\n",
    "    print(\"Watch \", watch_id,\" is being processed\" )\n",
    "    df_temp = get_sensor_data(df)\n",
    "    # TODO: Select Sensor name\n",
    "    df_temp = df_temp[df_temp['id']==watch_id]\n",
    "    df_temp.sort_index(ascending = True, inplace = True)\n",
    "    # Time to do analysis is specified\n",
    "    start = yesterday + 'T16:00:00.0000Z'\n",
    "    start_temp = np.datetime64(start)\n",
    "    t = pd.Timestamp(start_temp)\n",
    "    end = today + 'T16:00:00.0000Z'\n",
    "    end_temp = np.datetime64(end)\n",
    "    end_time = pd.Timestamp(end_temp)\n",
    "    \n",
    "    # Initialize \n",
    "    whole_window_size = datetime.timedelta(minutes = 5)\n",
    "    window_size = datetime.timedelta(seconds=2)\n",
    "    window_slide = datetime.timedelta(seconds=1)\n",
    "    samples_count = []\n",
    "    a = 0\n",
    "    df_out = pd.DataFrame()\n",
    "    t_start_list = []\n",
    "    t_end_list = []\n",
    "    outcome_list = []\n",
    "    while (t + whole_window_size < end_time):\n",
    "        label_list = []\n",
    "        increment = 0\n",
    "        DF = pd.DataFrame()\n",
    "        t_end5min= t + whole_window_size \n",
    "        print(\"doing time:\",t, ' - ', t_end5min)\n",
    "        t_start_list.append(time_to_str(t))\n",
    "        t_end_list.append(time_to_str(t_end5min))\n",
    "        while(t+window_slide< t_end5min):\n",
    "            t_end = t + window_size\n",
    "            snippet_df = df_temp.between_time(t.to_pydatetime().time(), t_end.to_pydatetime().time()\n",
    "                                           ,include_start=True, include_end=False)\n",
    "            if snippet_df.shape[0]>= 20:\n",
    "                increment +=1\n",
    "                ser = get_all_features(snippet_df, increment)\n",
    "                ser = ser.round(4)\n",
    "                DF = DF.append(ser, verify_integrity=True)\n",
    "            t = t_end\n",
    "        #DF.dropna(axis=0, how='any', inplace=True)\n",
    "        DF = DF.fillna(DF.mean())\n",
    "        if DF.shape[0]<=11:\n",
    "            outcome = 7.0\n",
    "        else:\n",
    "            X_test = DF.values\n",
    "            y_pred = logreg.predict(X_test)                \n",
    "            u, c = np.unique(y_pred, return_counts=True)\n",
    "            outcome = u[np.argmax(c)]\n",
    "        outcome_list.append(label_dict[int(outcome)])\n",
    "        out_ser = pd.Series(outcome,name=(t-whole_window_size, t) )\n",
    "        df_out = df_out.append(out_ser)\n",
    "        plt.plot(list(range(df_out.shape[0])), df_out[0], \"*\")\n",
    "        \n",
    "        ## Send predictions \n",
    "    plt.show()   \n",
    "    dict_list = []\n",
    "    for i in range(len(outcome_list)):\n",
    "        payload_dict = {'address':watch_id.split(\"-\")[2],\n",
    "             'starttime':t_start_list[i],\n",
    "             'endtime':t_end_list[i],\n",
    "             'tasklocation':'Activity',\n",
    "             'taskname':outcome_list[i],\n",
    "             'name':outcome_list[i],\n",
    "             'value':1}\n",
    "        dict_list.append(payload_dict)\n",
    "    payload = json.dumps(dict_list)\n",
    "    url = \"https://colife-dashboard.silverline.mobi/uploadActivityLabelForSmartWatch\"\n",
    "    headers = {\n",
    "        'content-type': \"application/json\",\n",
    "        'cache-control': \"no-cache\",\n",
    "        'postman-token': \"87b2b04f-175f-4a9b-f2c8-bf31de2cae7d\"\n",
    "        }\n",
    "\n",
    "    response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "    print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
