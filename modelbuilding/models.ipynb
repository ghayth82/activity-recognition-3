{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All Includes\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn import metrics ,preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# All Includes\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn import metrics ,preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[407]:\n",
    "\n",
    "\n",
    "class RNN_classifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"An example of classifier\"\"\"\n",
    "\n",
    "    def __init__(self, n_cells = 1, n_hidden = 32, n_classes = None,\n",
    "                 learning_rate = 0.0025,lambda_loss_amount = 0.0015,\n",
    "                training_iters = 100,  batch_size = None, look_back = 10):\n",
    "        \"\"\"\n",
    "        Called when initializing the classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input Data \n",
    "\n",
    "        self.training_data_count = None  # 7352 training series (with 50% overlap between each serie)\n",
    "        self.test_data_count = None  # 2947 testing series\n",
    "        self.n_steps = None\n",
    "        self.n_input = None  # 9 input parameters per timestep\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # LSTM Neural Network's internal structure\n",
    "\n",
    "        self.n_hidden = 32 # Hidden layer num of features\n",
    "        self.n_classes = 6 # Total classes (should go up, or should go down)\n",
    "        self.n_cells = n_cells\n",
    "        self.model = None\n",
    "        self._sess = None\n",
    "        \n",
    "        # To keep track of training's performance\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        \n",
    "        # Training \n",
    "        self.look_back = look_back\n",
    "        self.learning_rate = 0.0025\n",
    "        self.lambda_loss_amount = 0.0015\n",
    "        self.training_iters = training_iters\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self, X_training, y_training, X_val, y_val, verbose = 0, transform_data = True):\n",
    "\n",
    "\n",
    "        '''# In[414]:\n",
    "\n",
    "\n",
    "        model = RNN_classifier(n_classes=6, batch_size=3000,training_iters=10)\n",
    "\n",
    "\n",
    "        # In[415]:\n",
    "\n",
    "\n",
    "        df = pd.read_csv(\"/home/ahmet/notebooks/data/G9_data/processed.csv\")\n",
    "        df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "\n",
    "        # In[416]:\n",
    "\n",
    "\n",
    "\n",
    "        # convert an array of values into a dataset matrix\n",
    "        def create_dataset(X, y, look_back=3):\n",
    "\n",
    "            dataX, dataY = [], []\n",
    "            for i in range(look_back , len(X)):\n",
    "                a = X[i-look_back:i, :]\n",
    "                b = y[i]\n",
    "                dataX.append(a)\n",
    "                dataY.append(b)\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "        # In[417]:\n",
    "\n",
    "\n",
    "        df_training = df[np.logical_and(df['subject_id']!=15,df['subject_id']!=14 )]\n",
    "        df_test = df[np.logical_or(df['subject_id']==15, df['subject_id']==14 )]\n",
    "\n",
    "\n",
    "        # In[418]:\n",
    "\n",
    "\n",
    "        X_test_used = df_test[df_test.columns[:-3]].values\n",
    "        y_test = df_test['label'].values -1\n",
    "        X_train_used = df_training[df_training.columns[:-3]].values\n",
    "        y_train = df_training['label'].values -1\n",
    "\n",
    "\n",
    "        # In[419]:\n",
    "\n",
    "\n",
    "        X_test_used, y_test = create_dataset(X_test_used, y_test, look_back=10)\n",
    "        X_train_used, y_train = create_dataset(X_train_used, y_train, look_back=10)\n",
    "\n",
    "\n",
    "        # In[424]:\n",
    "\n",
    "\n",
    "        model.fit(X_training=X_train_used, y_training= y_train, X_val=X_test_used, y_val=y_test, verbose=1)\n",
    "\n",
    "\n",
    "        # In[425]:\n",
    "\n",
    "\n",
    "        a = model.predict(X_test_used, y_test)\n",
    "\n",
    "\n",
    "        # In[426]:\n",
    "\n",
    "\n",
    "        metrics.accuracy_score( y_test,a)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # Prepare dataset \n",
    "        if transform_data:\n",
    "            X_val, y_val = self.transform_dataset(X_val, y_val)\n",
    "            X_training, y_training = self.transform_dataset(X_training, y_training)\n",
    "        \n",
    "        # Set parameters from data \n",
    "        self.training_data_count = len(X_training)\n",
    "        self.test_data_count = len(X_val)\n",
    "        self.n_steps = len(X_training[0])  \n",
    "        self.n_input = len(X_training[0][0])\n",
    "        ops.reset_default_graph()\n",
    "        \n",
    "        # Graph input/output\n",
    "        self._x = tf.placeholder(tf.float32, [None, self.n_steps, self.n_input])\n",
    "        self._y = tf.placeholder(tf.float32, [None, self.n_classes])\n",
    "        # Graph weights\n",
    "        self._weights = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_input, self.n_hidden])), # Hidden layer weights\n",
    "            'out': tf.Variable(tf.random_normal([self.n_hidden, self.n_classes], mean=1.0))\n",
    "        }\n",
    "        self._biases = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_hidden])),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_classes]))\n",
    "        }\n",
    "\n",
    "        self.model = self._LSTM_RNN(self._x, self._weights, self._biases)\n",
    "\n",
    "        # Loss, optimizer and evaluation\n",
    "        l2 = self.lambda_loss_amount * sum(\n",
    "            tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "        ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self._y, logits=self.model)) + l2 # Softmax loss\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.model,1), tf.argmax(self._y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "        \n",
    "        \n",
    "        # To keep track of training's performance\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "        # Launch the graph\n",
    "        self._sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "        init = tf.global_variables_initializer()\n",
    "        self._sess.run(init)\n",
    "        \n",
    "\n",
    "\n",
    "        # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "        step = 1\n",
    "        epoch = 0\n",
    "        while step * self.batch_size <= self.training_iters * self.training_data_count:\n",
    "            batch_xs =         self.extract_batch_size(X_training, step)\n",
    "            batch_ys =         self.extract_batch_size(y_training, step)\n",
    "            batch_xs, batch_ys = shuffle(batch_xs, batch_ys, random_state=44)\n",
    "            # Fit training using batch data\n",
    "            _, loss, acc = self._sess.run(\n",
    "                [optimizer, cost, self.accuracy],\n",
    "                feed_dict={\n",
    "                    self._x: batch_xs, \n",
    "                    self._y: self.one_hot(batch_ys)\n",
    "                }\n",
    "            )\n",
    "            self.train_losses.append(loss)\n",
    "            self.train_accuracies.append(acc)\n",
    "            \n",
    "            if verbose ==1 :\n",
    "                # Evaluate network only at some steps for faster training: \n",
    "                if (step % (self.training_data_count //self.batch_size)) ==0:\n",
    "                    epoch += 1\n",
    "                    # To not spam console, show training accuracy/loss in this \"if\"\n",
    "                    print(\"Epoch #\" + str(epoch) +                           \":   Batch Loss = \" + \"{:.6f}\".format(loss) +                           \", Accuracy = {}\".format(acc))\n",
    "\n",
    "                    # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "                    loss, acc = self._sess.run(\n",
    "                        [cost, self.accuracy], \n",
    "                        feed_dict={\n",
    "                            self._x: X_val,\n",
    "                            self._y: self.one_hot(y_val)\n",
    "                        }\n",
    "                    )\n",
    "                    self.test_losses.append(loss)\n",
    "                    self.test_accuracies.append(acc)\n",
    "                    print(\"PERFORMANCE ON TEST SET: \" +                           \"Batch Loss = {}\".format(loss) +                           \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Accuracy for test data\n",
    "\n",
    "        one_hot_predictions, accuracy, final_loss = self._sess.run(\n",
    "            [self.model, self.accuracy, cost],\n",
    "            feed_dict={\n",
    "                self._x: X_val,\n",
    "                self._y: self.one_hot(y_val)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.test_losses.append(final_loss)\n",
    "        self.test_accuracies.append(accuracy)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None, transform_data = True):\n",
    "        one_hot_predictions = self.predict_proba(X,y, transform_data)\n",
    "\n",
    "\n",
    "        return(np.argmax(one_hot_predictions, 1))\n",
    "\n",
    "    def predict_proba(self, X, y=None, transform_data = True):\n",
    "        try:\n",
    "            getattr(self, \"model\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        if transform_data:\n",
    "            X,y = self.transform_data(X,np.ones(X.shape[0]))\n",
    "        else:\n",
    "            y = np.ones(X.shape[0])\n",
    "        one_hot_predictions = self._sess.run(\n",
    "            [self.model],\n",
    "            feed_dict={\n",
    "                self._x: X,\n",
    "                self._y:self.one_hot(y)\n",
    "            }\n",
    "        )\n",
    "        return np.array(one_hot_predictions).reshape(-1,self.n_classes)\n",
    "    def _LSTM_RNN(self, _X, _weights, _biases):\n",
    "        # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "        # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "        # Note, some code of this notebook is inspired from an slightly different \n",
    "        # RNN architecture used on another dataset, some of the credits goes to \n",
    "        # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "        # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "        # input shape: (batch_size, n_steps, n_input)\n",
    "        _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "        # Reshape to prepare input to hidden activation\n",
    "        _X = tf.reshape(_X, [-1, self.n_input]) \n",
    "        # new shape: (n_steps*batch_size, n_input)\n",
    "\n",
    "        # Linear activation\n",
    "        _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "        # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "        _X = tf.split(_X, self.n_steps, 0) \n",
    "        # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "        # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "        lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(self.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        cells = [tf.contrib.rnn.BasicLSTMCell(self.n_hidden, forget_bias=1.0, state_is_tuple=True) for _ in range(self.n_cells)]\n",
    "        lstm_cells = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "        # Get LSTM cell output\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "        # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "        # as in the image describing RNNs at the top of this page\n",
    "        lstm_last_output = outputs[-1]\n",
    "\n",
    "        # Linear activation\n",
    "        return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "    \n",
    "    def extract_batch_size(self,_train, step):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "        shape = list(_train.shape)\n",
    "        shape[0] = self.batch_size\n",
    "        batch_s = np.empty(shape)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Loop index\n",
    "            index = ((step-1)*self.batch_size + i) % len(_train)\n",
    "            batch_s[i] = _train[index] \n",
    "\n",
    "        return batch_s\n",
    "    def one_hot(self, y_):\n",
    "        # Function to encode output labels from number indexes \n",
    "        # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "\n",
    "        y_ = y_.reshape(len(y_))\n",
    "        return np.eye(self.n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "    \n",
    "    # convert an array of values into a dataset matrix\n",
    "    def transform_dataset(self,X, y, look_back=3):\n",
    "\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(look_back , len(X)):\n",
    "            a = X[i-look_back:i, :]\n",
    "            b = y[i]\n",
    "            dataX.append(a)\n",
    "            dataY.append(b)\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN_classifier(n_classes=6, batch_size=3000,training_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ahmet/notebooks/data/G9_data/processed.csv\")\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = df[np.logical_and(df['subject_id']!=15,df['subject_id']!=14 )]\n",
    "df_test = df[np.logical_or(df['subject_id']==15, df['subject_id']==14 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_used = df_test[df_test.columns[:-3]].values\n",
    "y_test = df_test['label'].values -1\n",
    "X_train_used = df_training[df_training.columns[:-3]].values\n",
    "y_train = df_training['label'].values -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1:   Batch Loss = 6.628261, Accuracy = 0.00133333331905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.73888254166, Accuracy = 0.346374541521\n",
      "Epoch #2:   Batch Loss = 7.082280, Accuracy = 0.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.49790525436, Accuracy = 0.347417831421\n",
      "Epoch #3:   Batch Loss = 6.827215, Accuracy = 0.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.30866909027, Accuracy = 0.37454354763\n",
      "Epoch #4:   Batch Loss = 4.477735, Accuracy = 0.244666665792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.08886671066, Accuracy = 0.425143450499\n",
      "Epoch #5:   Batch Loss = 2.983218, Accuracy = 0.546333312988\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.98666739464, Accuracy = 0.420448631048\n",
      "Epoch #6:   Batch Loss = 2.248672, Accuracy = 0.753333330154\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.87556600571, Accuracy = 0.444444447756\n",
      "Epoch #7:   Batch Loss = 2.260731, Accuracy = 0.731666684151\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.73802161217, Accuracy = 0.48513302207\n",
      "Epoch #8:   Batch Loss = 2.267910, Accuracy = 0.723666667938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.56422758102, Accuracy = 0.563901901245\n",
      "Epoch #9:   Batch Loss = 2.171565, Accuracy = 0.75966668129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.46068644524, Accuracy = 0.572248280048\n",
      "Epoch #10:   Batch Loss = 2.255903, Accuracy = 0.708666682243\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.4522023201, Accuracy = 0.585811138153\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN_classifier(batch_size=3000, lambda_loss_amount=0.0015,\n",
       "        learning_rate=0.0025, look_back=10, n_cells=1, n_classes=6,\n",
       "        n_hidden=32, training_iters=10)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training=X_train_used, y_training= y_train, X_val=X_test_used, y_val=y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_used, y_test = model.transform_dataset(X_test_used, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = model.predict(X_test_used, y_test,transform_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5858111632759521"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score( y_test,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
