{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers import LSTM,GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Conv2D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "import dsmuc.io as io\n",
    "import dsmuc.preprocessing as pp\n",
    "import dsmuc.features as ff\n",
    "import dsmuc.custom as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = 6\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(y):\n",
    "    y = np.asarray(y)\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weight = class_weight.compute_class_weight('balanced'\n",
    "                                               ,np.unique(y.reshape(y.shape[0],))\n",
    "                                               ,y.reshape(y.shape[0],))\n",
    "    return  {cls: float(weight) for cls,weight in zip (np.unique(y.reshape(y.shape[0],)),class_weight)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_roc_plot(y_test, y_score, classes  = 'all'):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    if classes is None:\n",
    "        classes = list(range(y_test.shape[1]))\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = len(classes)\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i],y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = itertools.cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(classes, colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm, classes, normalize = True):\n",
    "    import seaborn as sns\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap=\"Blues\"); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(classes); ax.yaxis.set_ticklabels(classes[::-1]);\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.yticks(rotation='horizontal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_windows_path = \"/home/ahmet/notebooks/data/G9_data/Raw/snippets/\"\n",
    "processed_file_path = \"/home/ahmet/notebooks/data/G9_data/processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34519 windows\n"
     ]
    }
   ],
   "source": [
    "win_paths = glob.glob(raw_windows_path+'*/*.csv')\n",
    "print(\"Found {} windows\".format(len(win_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_list = [pd.read_csv(win_paths[i], index_col='date', parse_dates=True) for i in range(len(win_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34519"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(win_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accX</th>\n",
       "      <th>accY</th>\n",
       "      <th>accZ</th>\n",
       "      <th>gyroX</th>\n",
       "      <th>gyroY</th>\n",
       "      <th>gyroZ</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-20 07:21:47.028</th>\n",
       "      <td>-9.797073</td>\n",
       "      <td>-2.552219</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.390070</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20 07:21:47.051</th>\n",
       "      <td>-9.797073</td>\n",
       "      <td>-2.533065</td>\n",
       "      <td>0.071826</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.086162</td>\n",
       "      <td>0.327219</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20 07:21:47.055</th>\n",
       "      <td>-9.619903</td>\n",
       "      <td>-2.542642</td>\n",
       "      <td>0.167594</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.083498</td>\n",
       "      <td>0.255847</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20 07:21:47.081</th>\n",
       "      <td>-9.543288</td>\n",
       "      <td>-2.537854</td>\n",
       "      <td>0.177171</td>\n",
       "      <td>-0.018883</td>\n",
       "      <td>0.077107</td>\n",
       "      <td>0.180213</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20 07:21:47.095</th>\n",
       "      <td>-9.504981</td>\n",
       "      <td>-2.523489</td>\n",
       "      <td>0.215478</td>\n",
       "      <td>-0.023677</td>\n",
       "      <td>0.069650</td>\n",
       "      <td>0.108307</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             accX      accY      accZ     gyroX     gyroY  \\\n",
       "date                                                                        \n",
       "2017-12-20 07:21:47.028 -9.797073 -2.552219  0.057461  0.044500  0.098412   \n",
       "2017-12-20 07:21:47.051 -9.797073 -2.533065  0.071826  0.030651  0.086162   \n",
       "2017-12-20 07:21:47.055 -9.619903 -2.542642  0.167594  0.001889  0.083498   \n",
       "2017-12-20 07:21:47.081 -9.543288 -2.537854  0.177171 -0.018883  0.077107   \n",
       "2017-12-20 07:21:47.095 -9.504981 -2.523489  0.215478 -0.023677  0.069650   \n",
       "\n",
       "                            gyroZ  subject_id  label  \n",
       "date                                                  \n",
       "2017-12-20 07:21:47.028  0.390070          11      5  \n",
       "2017-12-20 07:21:47.051  0.327219          11      5  \n",
       "2017-12-20 07:21:47.055  0.255847          11      5  \n",
       "2017-12-20 07:21:47.081  0.180213          11      5  \n",
       "2017-12-20 07:21:47.095  0.108307          11      5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_list[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw(win_list):\n",
    "    # Get data in raw format for models\n",
    "    \n",
    "    win_len = len(win_list)\n",
    "    X , y , s=[], [], []\n",
    "    for win in win_list:\n",
    "        # Append data, label and subject id \n",
    "        X.append(win[columns].values)\n",
    "        y.append(win['label'][0])\n",
    "        s.append(win['subject_id'][0])\n",
    "    return np.array(X), np.array(y), np.array(s)\n",
    "\n",
    "def get_processed(processed_file_path):\n",
    "    # Get Processed data to use in models\n",
    "    \n",
    "    df = pd.read_csv(processed_file_path)\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    X = df[df.columns[:-3]].values\n",
    "    y = df['label']\n",
    "    s = df['subject_id']\n",
    "    return X, y, s \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw, s_raw = get_raw(win_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep, y_prep, s_prep = get_processed(processed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    # 77% accuracy\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(8,\n",
    "                    64,\n",
    "                     input_shape=(timesteps,1),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=9))\n",
    "    model.add(Conv1D(16,\n",
    "                    16,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size = 7))\n",
    "    model.add(Conv1D(64,\n",
    "                    4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    # 85% accuracy\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(8,\n",
    "                    64,\n",
    "                     input_shape=(timesteps,1),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(16,\n",
    "                    16,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Conv1D(64,\n",
    "                    4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape =(timesteps,)))\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Model tested on subjects with no :  [  1 118]\n",
      "Test set and training set proportion :  0.0416427773921965\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33139 samples, validate on 1380 samples\n",
      "Epoch 1/50\n",
      "33139/33139 [==============================] - 6s 169us/step - loss: 1.3279 - acc: 0.4642 - val_loss: 1.3542 - val_acc: 0.4580\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.35425, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33139/33139 [==============================] - 4s 122us/step - loss: 1.1157 - acc: 0.5693 - val_loss: 1.1954 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.35425 to 1.19537, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33139/33139 [==============================] - 4s 125us/step - loss: 1.0509 - acc: 0.5978 - val_loss: 0.9773 - val_acc: 0.6058\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.19537 to 0.97729, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33139/33139 [==============================] - 4s 123us/step - loss: 1.0080 - acc: 0.6199 - val_loss: 1.2007 - val_acc: 0.4790\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.97729\n",
      "Epoch 5/50\n",
      "33139/33139 [==============================] - 4s 116us/step - loss: 0.9707 - acc: 0.6351 - val_loss: 1.1009 - val_acc: 0.6225\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.97729\n",
      "Epoch 6/50\n",
      "33139/33139 [==============================] - 4s 122us/step - loss: 0.9123 - acc: 0.6540 - val_loss: 1.0411 - val_acc: 0.6087\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.97729\n",
      "Epoch 7/50\n",
      "33139/33139 [==============================] - 4s 121us/step - loss: 0.8786 - acc: 0.6664 - val_loss: 1.0334 - val_acc: 0.7123\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.97729\n",
      "Epoch 8/50\n",
      "33139/33139 [==============================] - 4s 119us/step - loss: 0.8397 - acc: 0.6845 - val_loss: 0.9011 - val_acc: 0.6522\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.97729 to 0.90112, saving model to weights.hdf5\n",
      "Epoch 9/50\n",
      "33139/33139 [==============================] - 4s 117us/step - loss: 0.7916 - acc: 0.7045 - val_loss: 0.9397 - val_acc: 0.6971\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.90112\n",
      "Epoch 10/50\n",
      "33139/33139 [==============================] - 4s 123us/step - loss: 0.7840 - acc: 0.7111 - val_loss: 0.9326 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.90112\n",
      "Epoch 11/50\n",
      "33139/33139 [==============================] - 4s 125us/step - loss: 0.7454 - acc: 0.7214 - val_loss: 1.8800 - val_acc: 0.4899\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.90112\n",
      "Epoch 12/50\n",
      "33139/33139 [==============================] - 4s 122us/step - loss: 0.7525 - acc: 0.7169 - val_loss: 1.0052 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.90112\n",
      "Epoch 13/50\n",
      "33139/33139 [==============================] - 4s 128us/step - loss: 0.7114 - acc: 0.7304 - val_loss: 1.0235 - val_acc: 0.7101\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.90112\n",
      "Epoch 14/50\n",
      "33139/33139 [==============================] - 4s 125us/step - loss: 0.7108 - acc: 0.7316 - val_loss: 0.9503 - val_acc: 0.6804\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.90112\n",
      "Epoch 15/50\n",
      "33139/33139 [==============================] - 4s 120us/step - loss: 0.6832 - acc: 0.7394 - val_loss: 0.8818 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.90112 to 0.88181, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "33139/33139 [==============================] - 4s 121us/step - loss: 0.6815 - acc: 0.7428 - val_loss: 1.0834 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.88181\n",
      "Epoch 17/50\n",
      "33139/33139 [==============================] - 4s 124us/step - loss: 0.6844 - acc: 0.7383 - val_loss: 0.8747 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.88181 to 0.87475, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "33139/33139 [==============================] - 4s 114us/step - loss: 0.6498 - acc: 0.7558 - val_loss: 0.7111 - val_acc: 0.6522\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.87475 to 0.71106, saving model to weights.hdf5\n",
      "Epoch 19/50\n",
      "33139/33139 [==============================] - 4s 119us/step - loss: 0.6347 - acc: 0.7595 - val_loss: 1.1341 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.71106\n",
      "Epoch 20/50\n",
      "33139/33139 [==============================] - 4s 126us/step - loss: 0.6520 - acc: 0.7542 - val_loss: 1.0676 - val_acc: 0.6051\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.71106\n",
      "Epoch 21/50\n",
      "33139/33139 [==============================] - 4s 118us/step - loss: 0.6192 - acc: 0.7642 - val_loss: 1.0458 - val_acc: 0.7174\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.71106\n",
      "Epoch 22/50\n",
      "33139/33139 [==============================] - 4s 122us/step - loss: 0.6134 - acc: 0.7662 - val_loss: 1.0771 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.71106\n",
      "Epoch 23/50\n",
      "33139/33139 [==============================] - 4s 121us/step - loss: 0.6196 - acc: 0.7619 - val_loss: 0.8656 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.71106\n",
      "Epoch 24/50\n",
      "33139/33139 [==============================] - 4s 122us/step - loss: 0.6100 - acc: 0.7646 - val_loss: 1.0694 - val_acc: 0.6826\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.71106\n",
      "Epoch 25/50\n",
      "33139/33139 [==============================] - 4s 125us/step - loss: 0.5966 - acc: 0.7691 - val_loss: 0.9735 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.71106\n",
      "Epoch 26/50\n",
      "33139/33139 [==============================] - 4s 129us/step - loss: 0.5815 - acc: 0.7780 - val_loss: 0.9732 - val_acc: 0.6116\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.71106\n",
      "Epoch 27/50\n",
      "33139/33139 [==============================] - 4s 135us/step - loss: 0.5844 - acc: 0.7792 - val_loss: 0.9742 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.71106\n",
      "Epoch 28/50\n",
      "33139/33139 [==============================] - 4s 132us/step - loss: 0.5766 - acc: 0.7817 - val_loss: 1.0316 - val_acc: 0.6804\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.71106\n",
      "Epoch 29/50\n",
      "33139/33139 [==============================] - 4s 122us/step - loss: 0.5615 - acc: 0.7882 - val_loss: 1.1431 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.71106\n",
      "Epoch 30/50\n",
      "33139/33139 [==============================] - 4s 128us/step - loss: 0.5653 - acc: 0.7839 - val_loss: 0.9022 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.71106\n",
      "Epoch 31/50\n",
      "33139/33139 [==============================] - 4s 133us/step - loss: 0.5536 - acc: 0.7912 - val_loss: 0.9363 - val_acc: 0.7181\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.71106\n",
      "Epoch 32/50\n",
      "33139/33139 [==============================] - 4s 131us/step - loss: 0.5473 - acc: 0.7943 - val_loss: 1.0779 - val_acc: 0.6217\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.71106\n",
      "Epoch 33/50\n",
      "33139/33139 [==============================] - 4s 126us/step - loss: 0.5353 - acc: 0.7958 - val_loss: 0.9877 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.71106\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [  1 109]\n",
      "Test set and training set proportion :  0.04001084631375976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:104: FutureWarning: pd.rolling_mean is deprecated for ndarrays and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33191 samples, validate on 1328 samples\n",
      "Epoch 1/50\n",
      "33191/33191 [==============================] - 6s 172us/step - loss: 1.2828 - acc: 0.4640 - val_loss: 1.3379 - val_acc: 0.5346\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.33791, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33191/33191 [==============================] - 4s 132us/step - loss: 1.1251 - acc: 0.5596 - val_loss: 1.3038 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.33791 to 1.30381, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33191/33191 [==============================] - 4s 128us/step - loss: 1.0611 - acc: 0.5848 - val_loss: 1.1927 - val_acc: 0.5723\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30381 to 1.19272, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33191/33191 [==============================] - 4s 129us/step - loss: 1.0185 - acc: 0.6046 - val_loss: 1.1790 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.19272 to 1.17902, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33191/33191 [==============================] - 4s 133us/step - loss: 0.9532 - acc: 0.6370 - val_loss: 1.1115 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.17902 to 1.11147, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "33191/33191 [==============================] - 4s 123us/step - loss: 0.9047 - acc: 0.6579 - val_loss: 0.8235 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.11147 to 0.82353, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "33191/33191 [==============================] - 4s 124us/step - loss: 0.8169 - acc: 0.6967 - val_loss: 1.1376 - val_acc: 0.5678\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.82353\n",
      "Epoch 8/50\n",
      "33191/33191 [==============================] - 4s 135us/step - loss: 0.7754 - acc: 0.7114 - val_loss: 1.1909 - val_acc: 0.5896\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.82353\n",
      "Epoch 9/50\n",
      "33191/33191 [==============================] - 4s 122us/step - loss: 0.7678 - acc: 0.7054 - val_loss: 1.0476 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.82353\n",
      "Epoch 10/50\n",
      "33191/33191 [==============================] - 4s 129us/step - loss: 0.7360 - acc: 0.7226 - val_loss: 1.4898 - val_acc: 0.5090\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.82353\n",
      "Epoch 11/50\n",
      "33191/33191 [==============================] - 4s 123us/step - loss: 0.7049 - acc: 0.7331 - val_loss: 0.9893 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.82353\n",
      "Epoch 12/50\n",
      "33191/33191 [==============================] - 4s 122us/step - loss: 0.7426 - acc: 0.7233 - val_loss: 1.2254 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.82353\n",
      "Epoch 13/50\n",
      "33191/33191 [==============================] - 4s 123us/step - loss: 0.6673 - acc: 0.7485 - val_loss: 1.1464 - val_acc: 0.5392\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.82353\n",
      "Epoch 14/50\n",
      "33191/33191 [==============================] - 4s 124us/step - loss: 0.6776 - acc: 0.7411 - val_loss: 1.0735 - val_acc: 0.6581\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.82353\n",
      "Epoch 15/50\n",
      "33191/33191 [==============================] - 4s 130us/step - loss: 0.6989 - acc: 0.7349 - val_loss: 1.1213 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.82353\n",
      "Epoch 16/50\n",
      "33191/33191 [==============================] - 4s 129us/step - loss: 0.6571 - acc: 0.7554 - val_loss: 1.0632 - val_acc: 0.6069\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.82353\n",
      "Epoch 17/50\n",
      "33191/33191 [==============================] - 4s 119us/step - loss: 0.6309 - acc: 0.7636 - val_loss: 1.2348 - val_acc: 0.5941\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.82353\n",
      "Epoch 18/50\n",
      "33191/33191 [==============================] - 4s 128us/step - loss: 0.6460 - acc: 0.7517 - val_loss: 1.0260 - val_acc: 0.7048\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.82353\n",
      "Epoch 19/50\n",
      "33191/33191 [==============================] - 4s 128us/step - loss: 0.6320 - acc: 0.7621 - val_loss: 1.2122 - val_acc: 0.6672\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.82353\n",
      "Epoch 20/50\n",
      "33191/33191 [==============================] - 4s 129us/step - loss: 0.6016 - acc: 0.7699 - val_loss: 1.1367 - val_acc: 0.5693\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.82353\n",
      "Epoch 21/50\n",
      "33191/33191 [==============================] - 4s 134us/step - loss: 0.6144 - acc: 0.7738 - val_loss: 1.0799 - val_acc: 0.6069\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.82353\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [ 10 108]\n",
      "Test set and training set proportion :  0.041234314671814674\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33152 samples, validate on 1367 samples\n",
      "Epoch 1/50\n",
      "33152/33152 [==============================] - 6s 169us/step - loss: 1.3684 - acc: 0.4463 - val_loss: 1.0266 - val_acc: 0.6116\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02662, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33152/33152 [==============================] - 4s 125us/step - loss: 1.1633 - acc: 0.5278 - val_loss: 1.0784 - val_acc: 0.4060\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02662\n",
      "Epoch 3/50\n",
      "33152/33152 [==============================] - 4s 123us/step - loss: 1.1268 - acc: 0.5471 - val_loss: 0.9696 - val_acc: 0.5699\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02662 to 0.96964, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33152/33152 [==============================] - 4s 122us/step - loss: 1.0961 - acc: 0.5700 - val_loss: 1.0860 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.96964\n",
      "Epoch 5/50\n",
      "33152/33152 [==============================] - 4s 124us/step - loss: 1.0353 - acc: 0.5988 - val_loss: 0.8186 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.96964 to 0.81858, saving model to weights.hdf5\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33152/33152 [==============================] - 4s 129us/step - loss: 1.0134 - acc: 0.6075 - val_loss: 0.9796 - val_acc: 0.5084\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.81858\n",
      "Epoch 7/50\n",
      "33152/33152 [==============================] - 4s 121us/step - loss: 0.9525 - acc: 0.6285 - val_loss: 0.9010 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.81858\n",
      "Epoch 8/50\n",
      "33152/33152 [==============================] - 4s 124us/step - loss: 0.9234 - acc: 0.6458 - val_loss: 0.6607 - val_acc: 0.7549\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.81858 to 0.66074, saving model to weights.hdf5\n",
      "Epoch 9/50\n",
      "33152/33152 [==============================] - 4s 129us/step - loss: 0.8960 - acc: 0.6495 - val_loss: 0.6230 - val_acc: 0.8083\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.66074 to 0.62295, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "33152/33152 [==============================] - 4s 127us/step - loss: 0.8080 - acc: 0.6880 - val_loss: 0.8267 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.62295\n",
      "Epoch 11/50\n",
      "33152/33152 [==============================] - 4s 132us/step - loss: 0.8041 - acc: 0.6914 - val_loss: 0.8623 - val_acc: 0.6554\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.62295\n",
      "Epoch 12/50\n",
      "33152/33152 [==============================] - 4s 130us/step - loss: 0.7704 - acc: 0.7017 - val_loss: 0.5536 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.62295 to 0.55358, saving model to weights.hdf5\n",
      "Epoch 13/50\n",
      "33152/33152 [==============================] - 4s 126us/step - loss: 0.7555 - acc: 0.7120 - val_loss: 0.6764 - val_acc: 0.6796\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.55358\n",
      "Epoch 14/50\n",
      "33152/33152 [==============================] - 4s 125us/step - loss: 0.7617 - acc: 0.7077 - val_loss: 0.6162 - val_acc: 0.7279\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.55358\n",
      "Epoch 15/50\n",
      "33152/33152 [==============================] - 4s 127us/step - loss: 0.7177 - acc: 0.7252 - val_loss: 0.6930 - val_acc: 0.7081\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.55358\n",
      "Epoch 16/50\n",
      "33152/33152 [==============================] - 4s 125us/step - loss: 0.7367 - acc: 0.7171 - val_loss: 0.6104 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.55358\n",
      "Epoch 17/50\n",
      "33152/33152 [==============================] - 4s 125us/step - loss: 0.7100 - acc: 0.7260 - val_loss: 0.5100 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.55358 to 0.51004, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "33152/33152 [==============================] - 4s 123us/step - loss: 0.6784 - acc: 0.7393 - val_loss: 0.5274 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51004\n",
      "Epoch 19/50\n",
      "33152/33152 [==============================] - 4s 120us/step - loss: 0.6654 - acc: 0.7419 - val_loss: 0.5561 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51004\n",
      "Epoch 20/50\n",
      "33152/33152 [==============================] - 4s 134us/step - loss: 0.6600 - acc: 0.7458 - val_loss: 0.4793 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.51004 to 0.47933, saving model to weights.hdf5\n",
      "Epoch 21/50\n",
      "33152/33152 [==============================] - 5s 136us/step - loss: 0.6250 - acc: 0.7613 - val_loss: 0.6256 - val_acc: 0.7805\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47933\n",
      "Epoch 22/50\n",
      "33152/33152 [==============================] - 4s 111us/step - loss: 0.6401 - acc: 0.7524 - val_loss: 0.5497 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47933\n",
      "Epoch 23/50\n",
      "33152/33152 [==============================] - 4s 123us/step - loss: 0.6203 - acc: 0.7631 - val_loss: 0.4712 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.47933 to 0.47121, saving model to weights.hdf5\n",
      "Epoch 24/50\n",
      "33152/33152 [==============================] - 4s 120us/step - loss: 0.6134 - acc: 0.7651 - val_loss: 0.5587 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47121\n",
      "Epoch 25/50\n",
      "33152/33152 [==============================] - 4s 130us/step - loss: 0.6163 - acc: 0.7624 - val_loss: 0.6340 - val_acc: 0.7352\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47121\n",
      "Epoch 26/50\n",
      "33152/33152 [==============================] - 5s 137us/step - loss: 0.5994 - acc: 0.7705 - val_loss: 0.5026 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.47121\n",
      "Epoch 27/50\n",
      "33152/33152 [==============================] - 4s 118us/step - loss: 0.5921 - acc: 0.7749 - val_loss: 0.4812 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47121\n",
      "Epoch 28/50\n",
      "33152/33152 [==============================] - 4s 128us/step - loss: 0.5810 - acc: 0.7794 - val_loss: 0.4235 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.47121 to 0.42351, saving model to weights.hdf5\n",
      "Epoch 29/50\n",
      "33152/33152 [==============================] - 4s 131us/step - loss: 0.5826 - acc: 0.7816 - val_loss: 0.3922 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.42351 to 0.39219, saving model to weights.hdf5\n",
      "Epoch 30/50\n",
      "33152/33152 [==============================] - 4s 120us/step - loss: 0.5649 - acc: 0.7862 - val_loss: 0.5754 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.39219\n",
      "Epoch 31/50\n",
      "33152/33152 [==============================] - 4s 122us/step - loss: 0.5538 - acc: 0.7954 - val_loss: 0.5057 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.39219\n",
      "Epoch 32/50\n",
      "33152/33152 [==============================] - 4s 130us/step - loss: 0.5410 - acc: 0.7988 - val_loss: 0.6504 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.39219\n",
      "Epoch 33/50\n",
      "33152/33152 [==============================] - 4s 118us/step - loss: 0.5480 - acc: 0.7958 - val_loss: 0.6196 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.39219\n",
      "Epoch 34/50\n",
      "33152/33152 [==============================] - 4s 128us/step - loss: 0.5352 - acc: 0.8009 - val_loss: 0.7461 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.39219\n",
      "Epoch 35/50\n",
      "33152/33152 [==============================] - 4s 120us/step - loss: 0.5296 - acc: 0.8011 - val_loss: 0.7675 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.39219\n",
      "Epoch 36/50\n",
      "33152/33152 [==============================] - 4s 126us/step - loss: 0.5280 - acc: 0.8031 - val_loss: 0.4660 - val_acc: 0.8456\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.39219\n",
      "Epoch 37/50\n",
      "33152/33152 [==============================] - 4s 121us/step - loss: 0.5188 - acc: 0.8066 - val_loss: 0.8058 - val_acc: 0.6196\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.39219\n",
      "Epoch 38/50\n",
      "33152/33152 [==============================] - 4s 119us/step - loss: 0.5334 - acc: 0.8005 - val_loss: 0.4902 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.39219\n",
      "Epoch 39/50\n",
      "33152/33152 [==============================] - 4s 120us/step - loss: 0.4949 - acc: 0.8167 - val_loss: 0.8108 - val_acc: 0.7023\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.39219\n",
      "Epoch 40/50\n",
      "33152/33152 [==============================] - 4s 132us/step - loss: 0.5088 - acc: 0.8108 - val_loss: 0.6167 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.39219\n",
      "Epoch 41/50\n",
      "33152/33152 [==============================] - 4s 124us/step - loss: 0.5029 - acc: 0.8120 - val_loss: 0.4559 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.39219\n",
      "Epoch 42/50\n",
      "33152/33152 [==============================] - 5s 138us/step - loss: 0.4948 - acc: 0.8168 - val_loss: 0.4917 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.39219\n",
      "Epoch 43/50\n",
      "33152/33152 [==============================] - 4s 121us/step - loss: 0.4977 - acc: 0.8161 - val_loss: 0.4781 - val_acc: 0.8354\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.39219\n",
      "Epoch 44/50\n",
      "33152/33152 [==============================] - 4s 126us/step - loss: 0.4838 - acc: 0.8200 - val_loss: 0.5116 - val_acc: 0.7696\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.39219\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [ 3 25]\n",
      "Test set and training set proportion :  0.06556567371507949\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_19 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32395 samples, validate on 2124 samples\n",
      "Epoch 1/50\n",
      "32395/32395 [==============================] - 6s 176us/step - loss: 1.3262 - acc: 0.4555 - val_loss: 1.5242 - val_acc: 0.3159\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.52421, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32395/32395 [==============================] - 4s 129us/step - loss: 1.1361 - acc: 0.5527 - val_loss: 1.4466 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.52421 to 1.44656, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "32395/32395 [==============================] - 4s 122us/step - loss: 1.0686 - acc: 0.5834 - val_loss: 1.6279 - val_acc: 0.3390\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.44656\n",
      "Epoch 4/50\n",
      "32395/32395 [==============================] - 4s 120us/step - loss: 1.0193 - acc: 0.6101 - val_loss: 1.3549 - val_acc: 0.4586\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44656 to 1.35487, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "32395/32395 [==============================] - 4s 126us/step - loss: 0.9894 - acc: 0.6283 - val_loss: 1.3433 - val_acc: 0.4835\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.35487 to 1.34332, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "32395/32395 [==============================] - 4s 116us/step - loss: 0.9353 - acc: 0.6492 - val_loss: 1.2866 - val_acc: 0.5047\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.34332 to 1.28657, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.9185 - acc: 0.6557 - val_loss: 1.2469 - val_acc: 0.4685\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.28657 to 1.24686, saving model to weights.hdf5\n",
      "Epoch 8/50\n",
      "32395/32395 [==============================] - 4s 126us/step - loss: 0.8672 - acc: 0.6760 - val_loss: 1.2720 - val_acc: 0.5169\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.24686\n",
      "Epoch 9/50\n",
      "32395/32395 [==============================] - 4s 125us/step - loss: 0.8546 - acc: 0.6810 - val_loss: 1.2650 - val_acc: 0.4925\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.24686\n",
      "Epoch 10/50\n",
      "32395/32395 [==============================] - 4s 125us/step - loss: 0.8399 - acc: 0.6893 - val_loss: 1.2914 - val_acc: 0.4967\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.24686\n",
      "Epoch 11/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.8058 - acc: 0.6998 - val_loss: 1.2986 - val_acc: 0.4209\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.24686\n",
      "Epoch 12/50\n",
      "32395/32395 [==============================] - 4s 126us/step - loss: 0.7717 - acc: 0.7159 - val_loss: 1.2368 - val_acc: 0.4981\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.24686 to 1.23679, saving model to weights.hdf5\n",
      "Epoch 13/50\n",
      "32395/32395 [==============================] - 4s 124us/step - loss: 0.7533 - acc: 0.7157 - val_loss: 1.1976 - val_acc: 0.5151\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.23679 to 1.19759, saving model to weights.hdf5\n",
      "Epoch 14/50\n",
      "32395/32395 [==============================] - 4s 126us/step - loss: 0.7099 - acc: 0.7380 - val_loss: 1.3655 - val_acc: 0.4054\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.19759\n",
      "Epoch 15/50\n",
      "32395/32395 [==============================] - 4s 127us/step - loss: 0.7020 - acc: 0.7345 - val_loss: 1.1202 - val_acc: 0.5330\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.19759 to 1.12017, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "32395/32395 [==============================] - 4s 124us/step - loss: 0.6961 - acc: 0.7384 - val_loss: 1.1173 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.12017 to 1.11727, saving model to weights.hdf5\n",
      "Epoch 17/50\n",
      "32395/32395 [==============================] - 4s 120us/step - loss: 0.6640 - acc: 0.7466 - val_loss: 1.4081 - val_acc: 0.4557\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.11727\n",
      "Epoch 18/50\n",
      "32395/32395 [==============================] - 4s 121us/step - loss: 0.6495 - acc: 0.7530 - val_loss: 1.1759 - val_acc: 0.5160\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.11727\n",
      "Epoch 19/50\n",
      "32395/32395 [==============================] - 4s 118us/step - loss: 0.6564 - acc: 0.7473 - val_loss: 1.2031 - val_acc: 0.4962\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.11727\n",
      "Epoch 20/50\n",
      "32395/32395 [==============================] - 4s 124us/step - loss: 0.6301 - acc: 0.7612 - val_loss: 1.1644 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.11727\n",
      "Epoch 21/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.6197 - acc: 0.7621 - val_loss: 1.2267 - val_acc: 0.5047\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.11727\n",
      "Epoch 22/50\n",
      "32395/32395 [==============================] - 4s 123us/step - loss: 0.6104 - acc: 0.7697 - val_loss: 1.1656 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.11727\n",
      "Epoch 23/50\n",
      "32395/32395 [==============================] - 4s 121us/step - loss: 0.5949 - acc: 0.7767 - val_loss: 1.1726 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.11727\n",
      "Epoch 24/50\n",
      "32395/32395 [==============================] - 4s 122us/step - loss: 0.6010 - acc: 0.7718 - val_loss: 1.1540 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.11727\n",
      "Epoch 25/50\n",
      "32395/32395 [==============================] - 4s 127us/step - loss: 0.5847 - acc: 0.7810 - val_loss: 1.1117 - val_acc: 0.5212\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.11727 to 1.11172, saving model to weights.hdf5\n",
      "Epoch 26/50\n",
      "32395/32395 [==============================] - 4s 117us/step - loss: 0.5730 - acc: 0.7802 - val_loss: 1.2251 - val_acc: 0.5075\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.11172\n",
      "Epoch 27/50\n",
      "32395/32395 [==============================] - 4s 114us/step - loss: 0.5638 - acc: 0.7860 - val_loss: 1.0690 - val_acc: 0.5311\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.11172 to 1.06903, saving model to weights.hdf5\n",
      "Epoch 28/50\n",
      "32395/32395 [==============================] - 4s 118us/step - loss: 0.5611 - acc: 0.7890 - val_loss: 1.0634 - val_acc: 0.5438\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.06903 to 1.06343, saving model to weights.hdf5\n",
      "Epoch 29/50\n",
      "32395/32395 [==============================] - 4s 123us/step - loss: 0.5433 - acc: 0.7940 - val_loss: 1.2494 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.06343\n",
      "Epoch 30/50\n",
      "32395/32395 [==============================] - 4s 121us/step - loss: 0.5501 - acc: 0.7935 - val_loss: 1.1892 - val_acc: 0.5452\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.06343\n",
      "Epoch 31/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.5246 - acc: 0.8005 - val_loss: 1.2093 - val_acc: 0.5386\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.06343\n",
      "Epoch 32/50\n",
      "32395/32395 [==============================] - 4s 111us/step - loss: 0.5447 - acc: 0.7942 - val_loss: 1.0247 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.06343 to 1.02469, saving model to weights.hdf5\n",
      "Epoch 33/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.5161 - acc: 0.8059 - val_loss: 1.2498 - val_acc: 0.5334\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.02469\n",
      "Epoch 34/50\n",
      "32395/32395 [==============================] - 4s 121us/step - loss: 0.5179 - acc: 0.8044 - val_loss: 1.0796 - val_acc: 0.5433\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.02469\n",
      "Epoch 35/50\n",
      "32395/32395 [==============================] - 4s 121us/step - loss: 0.5101 - acc: 0.8080 - val_loss: 1.2312 - val_acc: 0.5466\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.02469\n",
      "Epoch 36/50\n",
      "32395/32395 [==============================] - 4s 113us/step - loss: 0.5028 - acc: 0.8101 - val_loss: 1.2064 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.02469\n",
      "Epoch 37/50\n",
      "32395/32395 [==============================] - 4s 115us/step - loss: 0.5012 - acc: 0.8120 - val_loss: 1.2240 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.02469\n",
      "Epoch 38/50\n",
      "32395/32395 [==============================] - 4s 114us/step - loss: 0.4931 - acc: 0.8141 - val_loss: 1.3504 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.02469\n",
      "Epoch 39/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.4967 - acc: 0.8128 - val_loss: 1.0747 - val_acc: 0.5697\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.02469\n",
      "Epoch 40/50\n",
      "32395/32395 [==============================] - 4s 124us/step - loss: 0.4841 - acc: 0.8170 - val_loss: 1.2805 - val_acc: 0.5348\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.02469\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32395/32395 [==============================] - 4s 114us/step - loss: 0.4764 - acc: 0.8205 - val_loss: 1.2024 - val_acc: 0.5617\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.02469\n",
      "Epoch 42/50\n",
      "32395/32395 [==============================] - 4s 119us/step - loss: 0.4695 - acc: 0.8250 - val_loss: 1.0604 - val_acc: 0.5838\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.02469\n",
      "Epoch 43/50\n",
      "32395/32395 [==============================] - 4s 115us/step - loss: 0.4732 - acc: 0.8227 - val_loss: 1.1476 - val_acc: 0.5584\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.02469\n",
      "Epoch 44/50\n",
      "32395/32395 [==============================] - 4s 114us/step - loss: 0.4642 - acc: 0.8245 - val_loss: 1.2601 - val_acc: 0.5612\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.02469\n",
      "Epoch 45/50\n",
      "32395/32395 [==============================] - 4s 113us/step - loss: 0.4631 - acc: 0.8253 - val_loss: 1.1514 - val_acc: 0.5565\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.02469\n",
      "Epoch 46/50\n",
      "32395/32395 [==============================] - 4s 114us/step - loss: 0.4591 - acc: 0.8286 - val_loss: 1.2333 - val_acc: 0.5457\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.02469\n",
      "Epoch 47/50\n",
      "32395/32395 [==============================] - 4s 125us/step - loss: 0.4562 - acc: 0.8299 - val_loss: 1.1941 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.02469\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [118 119]\n",
      "Test set and training set proportion :  0.016999587531671675\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_22 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33942 samples, validate on 577 samples\n",
      "Epoch 1/50\n",
      "33942/33942 [==============================] - 6s 181us/step - loss: 1.3299 - acc: 0.4454 - val_loss: 1.6858 - val_acc: 0.1768\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.68578, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33942/33942 [==============================] - 4s 123us/step - loss: 1.1141 - acc: 0.5713 - val_loss: 1.6100 - val_acc: 0.3518\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.68578 to 1.61004, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33942/33942 [==============================] - 4s 114us/step - loss: 1.0532 - acc: 0.5813 - val_loss: 1.5180 - val_acc: 0.4385\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.61004 to 1.51802, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33942/33942 [==============================] - 4s 124us/step - loss: 1.0005 - acc: 0.6128 - val_loss: 1.4649 - val_acc: 0.4506\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.51802 to 1.46492, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33942/33942 [==============================] - 4s 120us/step - loss: 0.9313 - acc: 0.6492 - val_loss: 1.1865 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.46492 to 1.18649, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "33942/33942 [==============================] - 4s 110us/step - loss: 0.8959 - acc: 0.6598 - val_loss: 1.1728 - val_acc: 0.6568\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.18649 to 1.17280, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "33942/33942 [==============================] - 4s 124us/step - loss: 0.8787 - acc: 0.6688 - val_loss: 1.7186 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.17280\n",
      "Epoch 8/50\n",
      "33942/33942 [==============================] - 4s 119us/step - loss: 0.8488 - acc: 0.6790 - val_loss: 1.0766 - val_acc: 0.6828\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.17280 to 1.07658, saving model to weights.hdf5\n",
      "Epoch 9/50\n",
      "33942/33942 [==============================] - 4s 113us/step - loss: 0.8000 - acc: 0.7013 - val_loss: 1.3079 - val_acc: 0.4558\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.07658\n",
      "Epoch 10/50\n",
      "33942/33942 [==============================] - 4s 118us/step - loss: 0.7557 - acc: 0.7187 - val_loss: 1.3290 - val_acc: 0.3778\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.07658\n",
      "Epoch 11/50\n",
      "33942/33942 [==============================] - 4s 120us/step - loss: 0.7349 - acc: 0.7254 - val_loss: 0.9515 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.07658 to 0.95146, saving model to weights.hdf5\n",
      "Epoch 12/50\n",
      "33942/33942 [==============================] - 4s 125us/step - loss: 0.7294 - acc: 0.7268 - val_loss: 0.8913 - val_acc: 0.6672\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.95146 to 0.89132, saving model to weights.hdf5\n",
      "Epoch 13/50\n",
      "33942/33942 [==============================] - 4s 121us/step - loss: 0.6897 - acc: 0.7371 - val_loss: 0.8178 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.89132 to 0.81778, saving model to weights.hdf5\n",
      "Epoch 14/50\n",
      "33942/33942 [==============================] - 4s 125us/step - loss: 0.6620 - acc: 0.7473 - val_loss: 1.0011 - val_acc: 0.6412\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.81778\n",
      "Epoch 15/50\n",
      "33942/33942 [==============================] - 4s 116us/step - loss: 0.6584 - acc: 0.7526 - val_loss: 1.0530 - val_acc: 0.5754\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.81778\n",
      "Epoch 16/50\n",
      "33942/33942 [==============================] - 4s 117us/step - loss: 0.6590 - acc: 0.7481 - val_loss: 0.8525 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.81778\n",
      "Epoch 17/50\n",
      "33942/33942 [==============================] - 4s 122us/step - loss: 0.6203 - acc: 0.7631 - val_loss: 0.7548 - val_acc: 0.7140\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.81778 to 0.75480, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "33942/33942 [==============================] - 4s 121us/step - loss: 0.6205 - acc: 0.7604 - val_loss: 0.8899 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.75480\n",
      "Epoch 19/50\n",
      "33942/33942 [==============================] - 4s 118us/step - loss: 0.6316 - acc: 0.7577 - val_loss: 0.9458 - val_acc: 0.6222\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.75480\n",
      "Epoch 20/50\n",
      "33942/33942 [==============================] - 4s 116us/step - loss: 0.5928 - acc: 0.7748 - val_loss: 0.7208 - val_acc: 0.7192\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.75480 to 0.72079, saving model to weights.hdf5\n",
      "Epoch 21/50\n",
      "33942/33942 [==============================] - 4s 116us/step - loss: 0.5990 - acc: 0.7727 - val_loss: 0.7473 - val_acc: 0.6898\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72079\n",
      "Epoch 22/50\n",
      "33942/33942 [==============================] - 4s 115us/step - loss: 0.5789 - acc: 0.7824 - val_loss: 0.7911 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72079\n",
      "Epoch 23/50\n",
      "33942/33942 [==============================] - 4s 123us/step - loss: 0.5897 - acc: 0.7781 - val_loss: 0.7558 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.72079\n",
      "Epoch 24/50\n",
      "33942/33942 [==============================] - 4s 116us/step - loss: 0.5463 - acc: 0.7960 - val_loss: 0.7779 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72079\n",
      "Epoch 25/50\n",
      "33942/33942 [==============================] - 4s 125us/step - loss: 0.5577 - acc: 0.7901 - val_loss: 0.7206 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.72079 to 0.72059, saving model to weights.hdf5\n",
      "Epoch 26/50\n",
      "33942/33942 [==============================] - 4s 128us/step - loss: 0.5478 - acc: 0.7916 - val_loss: 0.7020 - val_acc: 0.6395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss improved from 0.72059 to 0.70202, saving model to weights.hdf5\n",
      "Epoch 27/50\n",
      "33942/33942 [==============================] - 4s 111us/step - loss: 0.5352 - acc: 0.7972 - val_loss: 0.8404 - val_acc: 0.6153\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.70202\n",
      "Epoch 28/50\n",
      "33942/33942 [==============================] - 4s 119us/step - loss: 0.5356 - acc: 0.8018 - val_loss: 0.7801 - val_acc: 0.6066\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.70202\n",
      "Epoch 29/50\n",
      "33942/33942 [==============================] - 4s 114us/step - loss: 0.5299 - acc: 0.8038 - val_loss: 0.6069 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.70202 to 0.60686, saving model to weights.hdf5\n",
      "Epoch 30/50\n",
      "33942/33942 [==============================] - 4s 124us/step - loss: 0.5212 - acc: 0.8023 - val_loss: 0.5598 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.60686 to 0.55976, saving model to weights.hdf5\n",
      "Epoch 31/50\n",
      "33942/33942 [==============================] - 4s 121us/step - loss: 0.5140 - acc: 0.8070 - val_loss: 0.6152 - val_acc: 0.6932\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.55976\n",
      "Epoch 32/50\n",
      "33942/33942 [==============================] - 4s 114us/step - loss: 0.5046 - acc: 0.8128 - val_loss: 0.6816 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.55976\n",
      "Epoch 33/50\n",
      "33942/33942 [==============================] - 4s 122us/step - loss: 0.5144 - acc: 0.8074 - val_loss: 0.5760 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.55976\n",
      "Epoch 34/50\n",
      "33942/33942 [==============================] - 4s 113us/step - loss: 0.4912 - acc: 0.8139 - val_loss: 0.5750 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.55976\n",
      "Epoch 35/50\n",
      "33942/33942 [==============================] - 4s 113us/step - loss: 0.4870 - acc: 0.8186 - val_loss: 0.6127 - val_acc: 0.6984\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.55976\n",
      "Epoch 36/50\n",
      "33942/33942 [==============================] - 4s 127us/step - loss: 0.4944 - acc: 0.8158 - val_loss: 0.6750 - val_acc: 0.6586\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.55976\n",
      "Epoch 37/50\n",
      "33942/33942 [==============================] - 4s 121us/step - loss: 0.4805 - acc: 0.8226 - val_loss: 0.6144 - val_acc: 0.7140\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.55976\n",
      "Epoch 38/50\n",
      "33942/33942 [==============================] - 4s 118us/step - loss: 0.4703 - acc: 0.8244 - val_loss: 0.6325 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.55976\n",
      "Epoch 39/50\n",
      "33942/33942 [==============================] - 4s 116us/step - loss: 0.4751 - acc: 0.8213 - val_loss: 0.6805 - val_acc: 0.6828\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.55976\n",
      "Epoch 40/50\n",
      "33942/33942 [==============================] - 4s 112us/step - loss: 0.4733 - acc: 0.8241 - val_loss: 0.5898 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.55976\n",
      "Epoch 41/50\n",
      "33942/33942 [==============================] - 4s 118us/step - loss: 0.4582 - acc: 0.8287 - val_loss: 0.6589 - val_acc: 0.6724\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.55976\n",
      "Epoch 42/50\n",
      "33942/33942 [==============================] - 4s 114us/step - loss: 0.4592 - acc: 0.8266 - val_loss: 0.5608 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.55976\n",
      "Epoch 43/50\n",
      "33942/33942 [==============================] - 4s 112us/step - loss: 0.4576 - acc: 0.8274 - val_loss: 0.5922 - val_acc: 0.7071\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.55976\n",
      "Epoch 44/50\n",
      "33942/33942 [==============================] - 4s 119us/step - loss: 0.4484 - acc: 0.8328 - val_loss: 0.4883 - val_acc: 0.8024\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.55976 to 0.48834, saving model to weights.hdf5\n",
      "Epoch 45/50\n",
      "33942/33942 [==============================] - 4s 111us/step - loss: 0.4461 - acc: 0.8324 - val_loss: 0.6937 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.48834\n",
      "Epoch 46/50\n",
      "33942/33942 [==============================] - 4s 118us/step - loss: 0.4476 - acc: 0.8297 - val_loss: 0.6000 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48834\n",
      "Epoch 47/50\n",
      "33942/33942 [==============================] - 4s 120us/step - loss: 0.4339 - acc: 0.8378 - val_loss: 0.5765 - val_acc: 0.7262\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48834\n",
      "Epoch 48/50\n",
      "33942/33942 [==============================] - 4s 119us/step - loss: 0.4362 - acc: 0.8338 - val_loss: 0.5779 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48834\n",
      "Epoch 49/50\n",
      "33942/33942 [==============================] - 4s 119us/step - loss: 0.4341 - acc: 0.8365 - val_loss: 0.5865 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.48834\n",
      "Epoch 50/50\n",
      "33942/33942 [==============================] - 4s 117us/step - loss: 0.4265 - acc: 0.8373 - val_loss: 0.5442 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48834\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [114 117]\n",
      "Test set and training set proportion :  0.020728606067774557\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33818 samples, validate on 701 samples\n",
      "Epoch 1/50\n",
      "33818/33818 [==============================] - 6s 175us/step - loss: 1.2886 - acc: 0.4769 - val_loss: 2.1107 - val_acc: 0.1170\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.11074, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33818/33818 [==============================] - 4s 114us/step - loss: 1.1189 - acc: 0.5599 - val_loss: 1.6460 - val_acc: 0.3338\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.11074 to 1.64598, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33818/33818 [==============================] - 4s 117us/step - loss: 1.0465 - acc: 0.5929 - val_loss: 1.9564 - val_acc: 0.1098\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.64598\n",
      "Epoch 4/50\n",
      "33818/33818 [==============================] - 4s 120us/step - loss: 0.9781 - acc: 0.6214 - val_loss: 1.9587 - val_acc: 0.2111\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.64598\n",
      "Epoch 5/50\n",
      "33818/33818 [==============================] - 4s 111us/step - loss: 0.9291 - acc: 0.6482 - val_loss: 1.5382 - val_acc: 0.3951\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.64598 to 1.53820, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "33818/33818 [==============================] - 3s 102us/step - loss: 0.8885 - acc: 0.6664 - val_loss: 1.5669 - val_acc: 0.3923\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.53820\n",
      "Epoch 7/50\n",
      "33818/33818 [==============================] - 4s 115us/step - loss: 0.8760 - acc: 0.6704 - val_loss: 1.9356 - val_acc: 0.3081\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.53820\n",
      "Epoch 8/50\n",
      "33818/33818 [==============================] - 4s 109us/step - loss: 0.8254 - acc: 0.6874 - val_loss: 2.1395 - val_acc: 0.2183\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.53820\n",
      "Epoch 9/50\n",
      "33818/33818 [==============================] - 4s 118us/step - loss: 0.8174 - acc: 0.6884 - val_loss: 1.7939 - val_acc: 0.2568\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.53820\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33818/33818 [==============================] - 4s 115us/step - loss: 0.7857 - acc: 0.7060 - val_loss: 1.2161 - val_acc: 0.4807\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.53820 to 1.21610, saving model to weights.hdf5\n",
      "Epoch 11/50\n",
      "33818/33818 [==============================] - 4s 119us/step - loss: 0.7659 - acc: 0.7114 - val_loss: 2.4062 - val_acc: 0.2696\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.21610\n",
      "Epoch 12/50\n",
      "33818/33818 [==============================] - 4s 124us/step - loss: 0.7484 - acc: 0.7194 - val_loss: 1.9402 - val_acc: 0.2910\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.21610\n",
      "Epoch 13/50\n",
      "33818/33818 [==============================] - 4s 110us/step - loss: 0.7137 - acc: 0.7328 - val_loss: 2.1952 - val_acc: 0.2011\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.21610\n",
      "Epoch 14/50\n",
      "33818/33818 [==============================] - 4s 118us/step - loss: 0.7022 - acc: 0.7360 - val_loss: 1.7362 - val_acc: 0.3951\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.21610\n",
      "Epoch 15/50\n",
      "33818/33818 [==============================] - 4s 117us/step - loss: 0.6908 - acc: 0.7394 - val_loss: 1.5295 - val_acc: 0.4479\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.21610\n",
      "Epoch 16/50\n",
      "33818/33818 [==============================] - 4s 115us/step - loss: 0.6559 - acc: 0.7547 - val_loss: 1.6303 - val_acc: 0.3481\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.21610\n",
      "Epoch 17/50\n",
      "33818/33818 [==============================] - 4s 123us/step - loss: 0.6570 - acc: 0.7536 - val_loss: 1.1866 - val_acc: 0.4322\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.21610 to 1.18662, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "33818/33818 [==============================] - 4s 122us/step - loss: 0.6282 - acc: 0.7653 - val_loss: 0.9389 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.18662 to 0.93890, saving model to weights.hdf5\n",
      "Epoch 19/50\n",
      "33818/33818 [==============================] - 4s 121us/step - loss: 0.6232 - acc: 0.7644 - val_loss: 1.0560 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.93890\n",
      "Epoch 20/50\n",
      "33818/33818 [==============================] - 4s 119us/step - loss: 0.6092 - acc: 0.7721 - val_loss: 0.9921 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.93890\n",
      "Epoch 21/50\n",
      "33818/33818 [==============================] - 4s 114us/step - loss: 0.5919 - acc: 0.7788 - val_loss: 1.5496 - val_acc: 0.3752\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.93890\n",
      "Epoch 22/50\n",
      "33818/33818 [==============================] - 4s 119us/step - loss: 0.5862 - acc: 0.7806 - val_loss: 0.8655 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.93890 to 0.86548, saving model to weights.hdf5\n",
      "Epoch 23/50\n",
      "33818/33818 [==============================] - 4s 118us/step - loss: 0.5849 - acc: 0.7804 - val_loss: 0.9116 - val_acc: 0.5449\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.86548\n",
      "Epoch 24/50\n",
      "33818/33818 [==============================] - 4s 110us/step - loss: 0.5666 - acc: 0.7898 - val_loss: 0.9844 - val_acc: 0.5692\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.86548\n",
      "Epoch 25/50\n",
      "33818/33818 [==============================] - 4s 119us/step - loss: 0.5642 - acc: 0.7906 - val_loss: 0.9181 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.86548\n",
      "Epoch 26/50\n",
      "33818/33818 [==============================] - 4s 112us/step - loss: 0.5520 - acc: 0.7949 - val_loss: 0.9502 - val_acc: 0.5335\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.86548\n",
      "Epoch 27/50\n",
      "33818/33818 [==============================] - 4s 127us/step - loss: 0.5568 - acc: 0.7948 - val_loss: 0.9383 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.86548\n",
      "Epoch 28/50\n",
      "33818/33818 [==============================] - 4s 119us/step - loss: 0.5354 - acc: 0.8025 - val_loss: 0.8954 - val_acc: 0.5478\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.86548\n",
      "Epoch 29/50\n",
      "33818/33818 [==============================] - 4s 114us/step - loss: 0.5504 - acc: 0.7935 - val_loss: 0.9244 - val_acc: 0.5621\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.86548\n",
      "Epoch 30/50\n",
      "33818/33818 [==============================] - 4s 116us/step - loss: 0.5179 - acc: 0.8102 - val_loss: 1.1690 - val_acc: 0.4836\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.86548\n",
      "Epoch 31/50\n",
      "33818/33818 [==============================] - 4s 114us/step - loss: 0.5124 - acc: 0.8109 - val_loss: 0.7799 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.86548 to 0.77988, saving model to weights.hdf5\n",
      "Epoch 32/50\n",
      "33818/33818 [==============================] - 4s 119us/step - loss: 0.5198 - acc: 0.8076 - val_loss: 0.7505 - val_acc: 0.5991\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.77988 to 0.75051, saving model to weights.hdf5\n",
      "Epoch 33/50\n",
      "33818/33818 [==============================] - 4s 124us/step - loss: 0.5111 - acc: 0.8124 - val_loss: 0.7763 - val_acc: 0.5863\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.75051\n",
      "Epoch 34/50\n",
      "33818/33818 [==============================] - 4s 115us/step - loss: 0.5084 - acc: 0.8131 - val_loss: 1.0095 - val_acc: 0.4993\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.75051\n",
      "Epoch 35/50\n",
      "33818/33818 [==============================] - 4s 118us/step - loss: 0.4924 - acc: 0.8182 - val_loss: 0.7782 - val_acc: 0.5735\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.75051\n",
      "Epoch 36/50\n",
      "33818/33818 [==============================] - 4s 114us/step - loss: 0.4952 - acc: 0.8142 - val_loss: 0.7795 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.75051\n",
      "Epoch 37/50\n",
      "33818/33818 [==============================] - 4s 111us/step - loss: 0.4876 - acc: 0.8186 - val_loss: 0.8375 - val_acc: 0.5549\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.75051\n",
      "Epoch 38/50\n",
      "33818/33818 [==============================] - 4s 114us/step - loss: 0.4809 - acc: 0.8201 - val_loss: 0.8087 - val_acc: 0.5663\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.75051\n",
      "Epoch 39/50\n",
      "33818/33818 [==============================] - 4s 106us/step - loss: 0.4799 - acc: 0.8213 - val_loss: 0.7563 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.75051\n",
      "Epoch 40/50\n",
      "33818/33818 [==============================] - 4s 126us/step - loss: 0.4704 - acc: 0.8273 - val_loss: 0.6571 - val_acc: 0.6191\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.75051 to 0.65714, saving model to weights.hdf5\n",
      "Epoch 41/50\n",
      "33818/33818 [==============================] - 4s 118us/step - loss: 0.4714 - acc: 0.8255 - val_loss: 0.7725 - val_acc: 0.6006\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.65714\n",
      "Epoch 42/50\n",
      "33818/33818 [==============================] - 4s 120us/step - loss: 0.4618 - acc: 0.8290 - val_loss: 0.6881 - val_acc: 0.6191\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.65714\n",
      "Epoch 43/50\n",
      "33818/33818 [==============================] - 4s 125us/step - loss: 0.4635 - acc: 0.8267 - val_loss: 0.7496 - val_acc: 0.5934\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.65714\n",
      "Epoch 44/50\n",
      "33818/33818 [==============================] - 4s 111us/step - loss: 0.4490 - acc: 0.8338 - val_loss: 0.7444 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.65714\n",
      "Epoch 45/50\n",
      "33818/33818 [==============================] - 4s 120us/step - loss: 0.4533 - acc: 0.8327 - val_loss: 0.7971 - val_acc: 0.5863\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.65714\n",
      "Epoch 46/50\n",
      "33818/33818 [==============================] - 4s 124us/step - loss: 0.4457 - acc: 0.8357 - val_loss: 0.7403 - val_acc: 0.6063\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.65714\n",
      "Epoch 47/50\n",
      "33818/33818 [==============================] - 4s 121us/step - loss: 0.4399 - acc: 0.8362 - val_loss: 0.7043 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.65714\n",
      "Epoch 48/50\n",
      "33818/33818 [==============================] - 4s 120us/step - loss: 0.4392 - acc: 0.8398 - val_loss: 0.7461 - val_acc: 0.6106\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.65714\n",
      "Epoch 49/50\n",
      "33818/33818 [==============================] - 4s 115us/step - loss: 0.4257 - acc: 0.8438 - val_loss: 0.7147 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.65714\n",
      "Epoch 50/50\n",
      "33818/33818 [==============================] - 4s 122us/step - loss: 0.4312 - acc: 0.8404 - val_loss: 0.8384 - val_acc: 0.5749\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.65714\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [ 3 29]\n",
      "Test set and training set proportion :  0.06428439292100882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32434 samples, validate on 2085 samples\n",
      "Epoch 1/50\n",
      "32434/32434 [==============================] - 6s 193us/step - loss: 1.3550 - acc: 0.4442 - val_loss: 1.6090 - val_acc: 0.3108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60897, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32434/32434 [==============================] - 4s 121us/step - loss: 1.1287 - acc: 0.5482 - val_loss: 1.3997 - val_acc: 0.3540\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60897 to 1.39973, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "32434/32434 [==============================] - 4s 110us/step - loss: 1.0645 - acc: 0.5842 - val_loss: 1.3105 - val_acc: 0.4854\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.39973 to 1.31047, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "32434/32434 [==============================] - 4s 113us/step - loss: 0.9788 - acc: 0.6276 - val_loss: 1.1250 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.31047 to 1.12497, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "32434/32434 [==============================] - 4s 118us/step - loss: 0.9447 - acc: 0.6365 - val_loss: 1.3493 - val_acc: 0.4532\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12497\n",
      "Epoch 6/50\n",
      "32434/32434 [==============================] - 4s 116us/step - loss: 0.9215 - acc: 0.6508 - val_loss: 1.2410 - val_acc: 0.5319\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.12497\n",
      "Epoch 7/50\n",
      "32434/32434 [==============================] - 3s 107us/step - loss: 0.8783 - acc: 0.6676 - val_loss: 1.2501 - val_acc: 0.4532\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12497\n",
      "Epoch 8/50\n",
      "32434/32434 [==============================] - 4s 120us/step - loss: 0.8719 - acc: 0.6746 - val_loss: 1.3067 - val_acc: 0.5113\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12497\n",
      "Epoch 9/50\n",
      "32434/32434 [==============================] - 4s 116us/step - loss: 0.8329 - acc: 0.6874 - val_loss: 1.4634 - val_acc: 0.4240\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.12497\n",
      "Epoch 10/50\n",
      "32434/32434 [==============================] - 4s 122us/step - loss: 0.8302 - acc: 0.6899 - val_loss: 1.2807 - val_acc: 0.4796\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.12497\n",
      "Epoch 11/50\n",
      "32434/32434 [==============================] - 4s 124us/step - loss: 0.7698 - acc: 0.7103 - val_loss: 1.5317 - val_acc: 0.3305\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.12497\n",
      "Epoch 12/50\n",
      "32434/32434 [==============================] - 4s 117us/step - loss: 0.7867 - acc: 0.7063 - val_loss: 1.3328 - val_acc: 0.4556\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.12497\n",
      "Epoch 13/50\n",
      "32434/32434 [==============================] - 4s 125us/step - loss: 0.7313 - acc: 0.7273 - val_loss: 1.3369 - val_acc: 0.4451\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.12497\n",
      "Epoch 14/50\n",
      "32434/32434 [==============================] - 4s 130us/step - loss: 0.7337 - acc: 0.7282 - val_loss: 1.2470 - val_acc: 0.5060\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.12497\n",
      "Epoch 15/50\n",
      "32434/32434 [==============================] - 4s 118us/step - loss: 0.7056 - acc: 0.7319 - val_loss: 1.3531 - val_acc: 0.5228\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.12497\n",
      "Epoch 16/50\n",
      "32434/32434 [==============================] - 4s 121us/step - loss: 0.6934 - acc: 0.7383 - val_loss: 1.2223 - val_acc: 0.5496\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.12497\n",
      "Epoch 17/50\n",
      "32434/32434 [==============================] - 4s 113us/step - loss: 0.6601 - acc: 0.7490 - val_loss: 1.2145 - val_acc: 0.5022\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.12497\n",
      "Epoch 18/50\n",
      "32434/32434 [==============================] - 4s 110us/step - loss: 0.6518 - acc: 0.7520 - val_loss: 1.7463 - val_acc: 0.3573\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.12497\n",
      "Epoch 19/50\n",
      "32434/32434 [==============================] - 4s 109us/step - loss: 0.6437 - acc: 0.7532 - val_loss: 1.3617 - val_acc: 0.4408\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.12497\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [14 24]\n",
      "Test set and training set proportion :  0.06320263652323899\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_31 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32467 samples, validate on 2052 samples\n",
      "Epoch 1/50\n",
      "32467/32467 [==============================] - 6s 196us/step - loss: 1.3694 - acc: 0.4264 - val_loss: 1.1954 - val_acc: 0.4308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19541, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32467/32467 [==============================] - 4s 126us/step - loss: 1.1443 - acc: 0.5441 - val_loss: 1.0986 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.19541 to 1.09862, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "32467/32467 [==============================] - 4s 121us/step - loss: 1.0676 - acc: 0.5808 - val_loss: 1.1790 - val_acc: 0.4503\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.09862\n",
      "Epoch 4/50\n",
      "32467/32467 [==============================] - 4s 125us/step - loss: 1.0071 - acc: 0.6135 - val_loss: 1.1433 - val_acc: 0.5565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.09862\n",
      "Epoch 5/50\n",
      "32467/32467 [==============================] - 4s 124us/step - loss: 0.9652 - acc: 0.6322 - val_loss: 0.6908 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.09862 to 0.69078, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "32467/32467 [==============================] - 4s 126us/step - loss: 0.9205 - acc: 0.6490 - val_loss: 1.1861 - val_acc: 0.4532\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69078\n",
      "Epoch 7/50\n",
      "32467/32467 [==============================] - 4s 126us/step - loss: 0.8684 - acc: 0.6741 - val_loss: 0.9629 - val_acc: 0.6155\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69078\n",
      "Epoch 8/50\n",
      "32467/32467 [==============================] - 4s 123us/step - loss: 0.8347 - acc: 0.6878 - val_loss: 0.8943 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69078\n",
      "Epoch 9/50\n",
      "32467/32467 [==============================] - 4s 127us/step - loss: 0.7891 - acc: 0.7064 - val_loss: 0.8621 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69078\n",
      "Epoch 10/50\n",
      "32467/32467 [==============================] - 4s 129us/step - loss: 0.7685 - acc: 0.7107 - val_loss: 0.7067 - val_acc: 0.7393\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69078\n",
      "Epoch 11/50\n",
      "32467/32467 [==============================] - 4s 127us/step - loss: 0.7187 - acc: 0.7277 - val_loss: 0.8785 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69078\n",
      "Epoch 12/50\n",
      "32467/32467 [==============================] - 4s 130us/step - loss: 0.7071 - acc: 0.7322 - val_loss: 1.0663 - val_acc: 0.6559\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69078\n",
      "Epoch 13/50\n",
      "32467/32467 [==============================] - 4s 119us/step - loss: 0.7121 - acc: 0.7281 - val_loss: 1.0327 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69078\n",
      "Epoch 14/50\n",
      "32467/32467 [==============================] - 4s 120us/step - loss: 0.6784 - acc: 0.7431 - val_loss: 0.8797 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69078\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32467/32467 [==============================] - 4s 130us/step - loss: 0.6473 - acc: 0.7550 - val_loss: 0.9554 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69078\n",
      "Epoch 16/50\n",
      "32467/32467 [==============================] - 4s 119us/step - loss: 0.6608 - acc: 0.7489 - val_loss: 1.1234 - val_acc: 0.5414\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69078\n",
      "Epoch 17/50\n",
      "32467/32467 [==============================] - 4s 129us/step - loss: 0.6421 - acc: 0.7556 - val_loss: 0.8730 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.69078\n",
      "Epoch 18/50\n",
      "32467/32467 [==============================] - 4s 118us/step - loss: 0.6322 - acc: 0.7567 - val_loss: 1.0076 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.69078\n",
      "Epoch 19/50\n",
      "32467/32467 [==============================] - 4s 126us/step - loss: 0.6138 - acc: 0.7693 - val_loss: 1.1797 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69078\n",
      "Epoch 20/50\n",
      "32467/32467 [==============================] - 4s 131us/step - loss: 0.6064 - acc: 0.7682 - val_loss: 0.8850 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.69078\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [22 26]\n",
      "Test set and training set proportion :  0.06385798378894814\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_34 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32447 samples, validate on 2072 samples\n",
      "Epoch 1/50\n",
      "32447/32447 [==============================] - 6s 196us/step - loss: 1.3172 - acc: 0.4490 - val_loss: 1.2215 - val_acc: 0.6071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22154, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32447/32447 [==============================] - 4s 125us/step - loss: 1.1332 - acc: 0.5490 - val_loss: 1.2576 - val_acc: 0.5072\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.22154\n",
      "Epoch 3/50\n",
      "32447/32447 [==============================] - 4s 118us/step - loss: 1.0486 - acc: 0.5836 - val_loss: 1.1448 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22154 to 1.14476, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "32447/32447 [==============================] - 4s 125us/step - loss: 1.0015 - acc: 0.6093 - val_loss: 1.2858 - val_acc: 0.5101\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14476\n",
      "Epoch 5/50\n",
      "32447/32447 [==============================] - 4s 124us/step - loss: 0.9574 - acc: 0.6298 - val_loss: 1.0348 - val_acc: 0.6617\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.14476 to 1.03484, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "32447/32447 [==============================] - 4s 121us/step - loss: 0.9436 - acc: 0.6303 - val_loss: 1.0045 - val_acc: 0.6646\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.03484 to 1.00453, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "32447/32447 [==============================] - 4s 126us/step - loss: 0.8730 - acc: 0.6641 - val_loss: 1.0055 - val_acc: 0.6602\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00453\n",
      "Epoch 8/50\n",
      "32447/32447 [==============================] - 4s 117us/step - loss: 0.8387 - acc: 0.6802 - val_loss: 1.0496 - val_acc: 0.6569\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00453\n",
      "Epoch 9/50\n",
      "32447/32447 [==============================] - 4s 125us/step - loss: 0.8003 - acc: 0.6973 - val_loss: 0.9823 - val_acc: 0.6704\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00453 to 0.98231, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "32447/32447 [==============================] - 4s 126us/step - loss: 0.7666 - acc: 0.7102 - val_loss: 0.9218 - val_acc: 0.6737\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.98231 to 0.92180, saving model to weights.hdf5\n",
      "Epoch 11/50\n",
      "32447/32447 [==============================] - 4s 124us/step - loss: 0.7201 - acc: 0.7295 - val_loss: 0.9674 - val_acc: 0.6834\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.92180\n",
      "Epoch 12/50\n",
      "32447/32447 [==============================] - 4s 133us/step - loss: 0.7073 - acc: 0.7345 - val_loss: 1.0161 - val_acc: 0.6511\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.92180\n",
      "Epoch 13/50\n",
      "32447/32447 [==============================] - 4s 130us/step - loss: 0.6985 - acc: 0.7332 - val_loss: 0.9799 - val_acc: 0.6810\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.92180\n",
      "Epoch 14/50\n",
      "32447/32447 [==============================] - 4s 121us/step - loss: 0.6654 - acc: 0.7473 - val_loss: 0.9216 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.92180 to 0.92159, saving model to weights.hdf5\n",
      "Epoch 15/50\n",
      "32447/32447 [==============================] - 4s 116us/step - loss: 0.6612 - acc: 0.7476 - val_loss: 0.9539 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.92159\n",
      "Epoch 16/50\n",
      "32447/32447 [==============================] - 4s 113us/step - loss: 0.6480 - acc: 0.7565 - val_loss: 0.9122 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.92159 to 0.91224, saving model to weights.hdf5\n",
      "Epoch 17/50\n",
      "32447/32447 [==============================] - 4s 123us/step - loss: 0.6486 - acc: 0.7540 - val_loss: 0.9615 - val_acc: 0.6718\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.91224\n",
      "Epoch 18/50\n",
      "32447/32447 [==============================] - 4s 125us/step - loss: 0.6109 - acc: 0.7726 - val_loss: 0.9293 - val_acc: 0.6791\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.91224\n",
      "Epoch 19/50\n",
      "32447/32447 [==============================] - 4s 126us/step - loss: 0.6483 - acc: 0.7552 - val_loss: 0.9315 - val_acc: 0.6834\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.91224\n",
      "Epoch 20/50\n",
      "32447/32447 [==============================] - 4s 126us/step - loss: 0.5873 - acc: 0.7817 - val_loss: 0.9841 - val_acc: 0.6718\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.91224\n",
      "Epoch 21/50\n",
      "32447/32447 [==============================] - 4s 117us/step - loss: 0.5937 - acc: 0.7783 - val_loss: 1.0183 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.91224\n",
      "Epoch 22/50\n",
      "32447/32447 [==============================] - 4s 117us/step - loss: 0.5841 - acc: 0.7795 - val_loss: 0.9225 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.91224\n",
      "Epoch 23/50\n",
      "32447/32447 [==============================] - 4s 120us/step - loss: 0.5825 - acc: 0.7825 - val_loss: 0.9625 - val_acc: 0.6737\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.91224\n",
      "Epoch 24/50\n",
      "32447/32447 [==============================] - 4s 118us/step - loss: 0.5636 - acc: 0.7902 - val_loss: 1.0167 - val_acc: 0.6602\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.91224\n",
      "Epoch 25/50\n",
      "32447/32447 [==============================] - 4s 121us/step - loss: 0.5519 - acc: 0.7931 - val_loss: 0.9518 - val_acc: 0.6704\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.91224\n",
      "Epoch 26/50\n",
      "32447/32447 [==============================] - 4s 115us/step - loss: 0.5483 - acc: 0.7957 - val_loss: 0.9373 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.91224\n",
      "Epoch 27/50\n",
      "32447/32447 [==============================] - 4s 114us/step - loss: 0.5435 - acc: 0.7975 - val_loss: 0.9474 - val_acc: 0.6762\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.91224\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32447/32447 [==============================] - 4s 111us/step - loss: 0.5398 - acc: 0.7986 - val_loss: 0.9286 - val_acc: 0.6757\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.91224\n",
      "Epoch 29/50\n",
      "32447/32447 [==============================] - 4s 118us/step - loss: 0.5396 - acc: 0.8010 - val_loss: 0.9649 - val_acc: 0.6892\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.91224\n",
      "Epoch 30/50\n",
      "32447/32447 [==============================] - 4s 121us/step - loss: 0.5153 - acc: 0.8110 - val_loss: 1.1196 - val_acc: 0.6332\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.91224\n",
      "Epoch 31/50\n",
      "32447/32447 [==============================] - 4s 112us/step - loss: 0.5156 - acc: 0.8089 - val_loss: 1.0022 - val_acc: 0.6443\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.91224\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [ 21 102]\n",
      "Test set and training set proportion :  0.03982287556104467\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_37 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33197 samples, validate on 1322 samples\n",
      "Epoch 1/50\n",
      "33197/33197 [==============================] - 6s 174us/step - loss: 1.3243 - acc: 0.4575 - val_loss: 1.1653 - val_acc: 0.5514\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.16527, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33197/33197 [==============================] - 4s 117us/step - loss: 1.1285 - acc: 0.5539 - val_loss: 1.0992 - val_acc: 0.5484\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.16527 to 1.09923, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33197/33197 [==============================] - 4s 121us/step - loss: 1.0397 - acc: 0.5905 - val_loss: 1.5365 - val_acc: 0.3525\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.09923\n",
      "Epoch 4/50\n",
      "33197/33197 [==============================] - 4s 115us/step - loss: 1.0011 - acc: 0.6104 - val_loss: 1.0408 - val_acc: 0.5408\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.09923 to 1.04080, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33197/33197 [==============================] - 4s 112us/step - loss: 0.9805 - acc: 0.6239 - val_loss: 0.8812 - val_acc: 0.6082\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.04080 to 0.88116, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "33197/33197 [==============================] - 4s 117us/step - loss: 0.9308 - acc: 0.6408 - val_loss: 1.1765 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.88116\n",
      "Epoch 7/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.8899 - acc: 0.6616 - val_loss: 0.8482 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.88116 to 0.84823, saving model to weights.hdf5\n",
      "Epoch 8/50\n",
      "33197/33197 [==============================] - 4s 111us/step - loss: 0.8507 - acc: 0.6802 - val_loss: 0.7079 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.84823 to 0.70792, saving model to weights.hdf5\n",
      "Epoch 9/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.8049 - acc: 0.6978 - val_loss: 0.8182 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.70792\n",
      "Epoch 10/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.8017 - acc: 0.6991 - val_loss: 0.7047 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.70792 to 0.70466, saving model to weights.hdf5\n",
      "Epoch 11/50\n",
      "33197/33197 [==============================] - 4s 112us/step - loss: 0.7426 - acc: 0.7207 - val_loss: 0.8817 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70466\n",
      "Epoch 12/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.7444 - acc: 0.7194 - val_loss: 0.6070 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.70466 to 0.60703, saving model to weights.hdf5\n",
      "Epoch 13/50\n",
      "33197/33197 [==============================] - 4s 112us/step - loss: 0.7302 - acc: 0.7228 - val_loss: 0.7235 - val_acc: 0.6626\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60703\n",
      "Epoch 14/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.6943 - acc: 0.7371 - val_loss: 0.9366 - val_acc: 0.5151\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60703\n",
      "Epoch 15/50\n",
      "33197/33197 [==============================] - 4s 114us/step - loss: 0.7022 - acc: 0.7330 - val_loss: 0.5991 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60703 to 0.59910, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "33197/33197 [==============================] - 4s 108us/step - loss: 0.6650 - acc: 0.7463 - val_loss: 0.5910 - val_acc: 0.8169\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.59910 to 0.59105, saving model to weights.hdf5\n",
      "Epoch 17/50\n",
      "33197/33197 [==============================] - 4s 113us/step - loss: 0.6662 - acc: 0.7471 - val_loss: 0.6284 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.59105\n",
      "Epoch 18/50\n",
      "33197/33197 [==============================] - 3s 104us/step - loss: 0.6516 - acc: 0.7526 - val_loss: 0.5738 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.59105 to 0.57376, saving model to weights.hdf5\n",
      "Epoch 19/50\n",
      "33197/33197 [==============================] - 4s 112us/step - loss: 0.6461 - acc: 0.7567 - val_loss: 0.5292 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.57376 to 0.52920, saving model to weights.hdf5\n",
      "Epoch 20/50\n",
      "33197/33197 [==============================] - 4s 113us/step - loss: 0.6233 - acc: 0.7642 - val_loss: 0.5165 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.52920 to 0.51652, saving model to weights.hdf5\n",
      "Epoch 21/50\n",
      "33197/33197 [==============================] - 4s 107us/step - loss: 0.6227 - acc: 0.7627 - val_loss: 0.5755 - val_acc: 0.7262\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51652\n",
      "Epoch 22/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.6050 - acc: 0.7716 - val_loss: 0.4985 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.51652 to 0.49854, saving model to weights.hdf5\n",
      "Epoch 23/50\n",
      "33197/33197 [==============================] - 3s 104us/step - loss: 0.5931 - acc: 0.7724 - val_loss: 0.5871 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49854\n",
      "Epoch 24/50\n",
      "33197/33197 [==============================] - 4s 113us/step - loss: 0.5844 - acc: 0.7780 - val_loss: 0.5035 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.49854\n",
      "Epoch 25/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.5771 - acc: 0.7817 - val_loss: 0.5373 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49854\n",
      "Epoch 26/50\n",
      "33197/33197 [==============================] - 4s 113us/step - loss: 0.5833 - acc: 0.7750 - val_loss: 0.5030 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49854\n",
      "Epoch 27/50\n",
      "33197/33197 [==============================] - 4s 114us/step - loss: 0.5796 - acc: 0.7822 - val_loss: 0.5544 - val_acc: 0.7821\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49854\n",
      "Epoch 28/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.5517 - acc: 0.7920 - val_loss: 0.4718 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.49854 to 0.47178, saving model to weights.hdf5\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33197/33197 [==============================] - 4s 115us/step - loss: 0.5549 - acc: 0.7904 - val_loss: 0.4042 - val_acc: 0.8707\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.47178 to 0.40418, saving model to weights.hdf5\n",
      "Epoch 30/50\n",
      "33197/33197 [==============================] - 4s 108us/step - loss: 0.5569 - acc: 0.7900 - val_loss: 0.5597 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.40418\n",
      "Epoch 31/50\n",
      "33197/33197 [==============================] - 4s 112us/step - loss: 0.5402 - acc: 0.7960 - val_loss: 0.4794 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.40418\n",
      "Epoch 32/50\n",
      "33197/33197 [==============================] - 4s 108us/step - loss: 0.5482 - acc: 0.7928 - val_loss: 0.5103 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.40418\n",
      "Epoch 33/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.5313 - acc: 0.8018 - val_loss: 0.5242 - val_acc: 0.8313\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.40418\n",
      "Epoch 34/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.5164 - acc: 0.8063 - val_loss: 0.5285 - val_acc: 0.7912\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.40418\n",
      "Epoch 35/50\n",
      "33197/33197 [==============================] - 4s 106us/step - loss: 0.5331 - acc: 0.7998 - val_loss: 0.5155 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.40418\n",
      "Epoch 36/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.5135 - acc: 0.8069 - val_loss: 0.5398 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.40418\n",
      "Epoch 37/50\n",
      "33197/33197 [==============================] - 3s 104us/step - loss: 0.5081 - acc: 0.8084 - val_loss: 0.5300 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.40418\n",
      "Epoch 38/50\n",
      "33197/33197 [==============================] - 4s 109us/step - loss: 0.5221 - acc: 0.8025 - val_loss: 0.5048 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.40418\n",
      "Epoch 39/50\n",
      "33197/33197 [==============================] - 4s 108us/step - loss: 0.5022 - acc: 0.8113 - val_loss: 0.8083 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.40418\n",
      "Epoch 40/50\n",
      "33197/33197 [==============================] - 4s 112us/step - loss: 0.5038 - acc: 0.8109 - val_loss: 0.4977 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.40418\n",
      "Epoch 41/50\n",
      "33197/33197 [==============================] - 4s 107us/step - loss: 0.5054 - acc: 0.8107 - val_loss: 0.6794 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.40418\n",
      "Epoch 42/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.4849 - acc: 0.8192 - val_loss: 0.4441 - val_acc: 0.8449\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.40418\n",
      "Epoch 43/50\n",
      "33197/33197 [==============================] - 4s 107us/step - loss: 0.4849 - acc: 0.8175 - val_loss: 0.5291 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.40418\n",
      "Epoch 44/50\n",
      "33197/33197 [==============================] - 4s 110us/step - loss: 0.4874 - acc: 0.8161 - val_loss: 0.5551 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.40418\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [  1 112]\n",
      "Test set and training set proportion :  0.0412657235075865\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_40 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33151 samples, validate on 1368 samples\n",
      "Epoch 1/50\n",
      "33151/33151 [==============================] - 6s 177us/step - loss: 1.3556 - acc: 0.4248 - val_loss: 1.3639 - val_acc: 0.3399\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36388, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33151/33151 [==============================] - 4s 120us/step - loss: 1.1390 - acc: 0.5464 - val_loss: 1.2431 - val_acc: 0.4978\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36388 to 1.24312, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 1.0701 - acc: 0.5805 - val_loss: 1.2240 - val_acc: 0.6133\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.24312 to 1.22404, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33151/33151 [==============================] - 4s 122us/step - loss: 0.9973 - acc: 0.6269 - val_loss: 1.1276 - val_acc: 0.6425\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.22404 to 1.12759, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.9274 - acc: 0.6512 - val_loss: 1.2237 - val_acc: 0.5585\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12759\n",
      "Epoch 6/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.9203 - acc: 0.6533 - val_loss: 1.0106 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.12759 to 1.01061, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "33151/33151 [==============================] - 4s 108us/step - loss: 0.8567 - acc: 0.6826 - val_loss: 1.1650 - val_acc: 0.6499\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.01061\n",
      "Epoch 8/50\n",
      "33151/33151 [==============================] - 4s 107us/step - loss: 0.8735 - acc: 0.6746 - val_loss: 0.9452 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.01061 to 0.94522, saving model to weights.hdf5\n",
      "Epoch 9/50\n",
      "33151/33151 [==============================] - 4s 110us/step - loss: 0.8300 - acc: 0.6928 - val_loss: 1.3147 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.94522\n",
      "Epoch 10/50\n",
      "33151/33151 [==============================] - 3s 105us/step - loss: 0.7881 - acc: 0.7131 - val_loss: 1.1315 - val_acc: 0.6425\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.94522\n",
      "Epoch 11/50\n",
      "33151/33151 [==============================] - 4s 114us/step - loss: 0.7892 - acc: 0.7079 - val_loss: 1.1249 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.94522\n",
      "Epoch 12/50\n",
      "33151/33151 [==============================] - 4s 110us/step - loss: 0.7439 - acc: 0.7249 - val_loss: 1.1445 - val_acc: 0.5694\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.94522\n",
      "Epoch 13/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.7357 - acc: 0.7294 - val_loss: 0.9904 - val_acc: 0.7025\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.94522\n",
      "Epoch 14/50\n",
      "33151/33151 [==============================] - 3s 105us/step - loss: 0.7067 - acc: 0.7388 - val_loss: 0.9973 - val_acc: 0.6762\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.94522\n",
      "Epoch 15/50\n",
      "33151/33151 [==============================] - 4s 110us/step - loss: 0.6819 - acc: 0.7466 - val_loss: 1.0828 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.94522\n",
      "Epoch 16/50\n",
      "33151/33151 [==============================] - 4s 112us/step - loss: 0.6599 - acc: 0.7536 - val_loss: 1.1190 - val_acc: 0.6265\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.94522\n",
      "Epoch 17/50\n",
      "33151/33151 [==============================] - 3s 105us/step - loss: 0.6613 - acc: 0.7523 - val_loss: 1.0345 - val_acc: 0.6572\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.94522\n",
      "Epoch 18/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.6455 - acc: 0.7600 - val_loss: 1.0463 - val_acc: 0.6784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_loss did not improve from 0.94522\n",
      "Epoch 19/50\n",
      "33151/33151 [==============================] - 4s 106us/step - loss: 0.6302 - acc: 0.7656 - val_loss: 0.9618 - val_acc: 0.7171\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.94522\n",
      "Epoch 20/50\n",
      "33151/33151 [==============================] - 4s 112us/step - loss: 0.6343 - acc: 0.7613 - val_loss: 0.9905 - val_acc: 0.7354\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.94522\n",
      "Epoch 21/50\n",
      "33151/33151 [==============================] - 4s 106us/step - loss: 0.6154 - acc: 0.7717 - val_loss: 0.9243 - val_acc: 0.7405\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.94522 to 0.92433, saving model to weights.hdf5\n",
      "Epoch 22/50\n",
      "33151/33151 [==============================] - 4s 110us/step - loss: 0.5900 - acc: 0.7798 - val_loss: 1.2928 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.92433\n",
      "Epoch 23/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.6001 - acc: 0.7800 - val_loss: 1.0125 - val_acc: 0.6681\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.92433\n",
      "Epoch 24/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.5964 - acc: 0.7758 - val_loss: 0.9559 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.92433\n",
      "Epoch 25/50\n",
      "33151/33151 [==============================] - 4s 124us/step - loss: 0.5700 - acc: 0.7858 - val_loss: 1.1914 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.92433\n",
      "Epoch 26/50\n",
      "33151/33151 [==============================] - 4s 107us/step - loss: 0.5695 - acc: 0.7892 - val_loss: 1.1866 - val_acc: 0.6352\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.92433\n",
      "Epoch 27/50\n",
      "33151/33151 [==============================] - 4s 118us/step - loss: 0.5579 - acc: 0.7904 - val_loss: 1.1474 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.92433\n",
      "Epoch 28/50\n",
      "33151/33151 [==============================] - 3s 105us/step - loss: 0.5560 - acc: 0.7945 - val_loss: 0.9721 - val_acc: 0.7471\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.92433\n",
      "Epoch 29/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.5427 - acc: 0.7962 - val_loss: 0.9655 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.92433\n",
      "Epoch 30/50\n",
      "33151/33151 [==============================] - 4s 108us/step - loss: 0.5413 - acc: 0.7997 - val_loss: 1.0091 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.92433\n",
      "Epoch 31/50\n",
      "33151/33151 [==============================] - 4s 111us/step - loss: 0.5457 - acc: 0.7999 - val_loss: 0.9837 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.92433\n",
      "Epoch 32/50\n",
      "33151/33151 [==============================] - 4s 108us/step - loss: 0.5215 - acc: 0.8036 - val_loss: 1.0402 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.92433\n",
      "Epoch 33/50\n",
      "33151/33151 [==============================] - 4s 108us/step - loss: 0.5229 - acc: 0.8054 - val_loss: 1.2631 - val_acc: 0.6177\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.92433\n",
      "Epoch 34/50\n",
      "33151/33151 [==============================] - 4s 114us/step - loss: 0.5171 - acc: 0.8093 - val_loss: 0.9568 - val_acc: 0.6674\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.92433\n",
      "Epoch 35/50\n",
      "33151/33151 [==============================] - 4s 109us/step - loss: 0.5050 - acc: 0.8126 - val_loss: 1.0812 - val_acc: 0.6952\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.92433\n",
      "Epoch 36/50\n",
      "33151/33151 [==============================] - 4s 110us/step - loss: 0.5084 - acc: 0.8086 - val_loss: 1.0432 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.92433\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [117 119]\n",
      "Test set and training set proportion :  0.014727497207360808\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_43 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 34018 samples, validate on 501 samples\n",
      "Epoch 1/50\n",
      "34018/34018 [==============================] - 6s 176us/step - loss: 1.2802 - acc: 0.4788 - val_loss: 1.7410 - val_acc: 0.3253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74103, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "34018/34018 [==============================] - 4s 111us/step - loss: 1.1024 - acc: 0.5700 - val_loss: 1.6275 - val_acc: 0.4311\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74103 to 1.62751, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "34018/34018 [==============================] - 4s 104us/step - loss: 1.0389 - acc: 0.5964 - val_loss: 2.2567 - val_acc: 0.0778\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.62751\n",
      "Epoch 4/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.9891 - acc: 0.6182 - val_loss: 1.3969 - val_acc: 0.6287\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.62751 to 1.39685, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "34018/34018 [==============================] - 3s 102us/step - loss: 0.9280 - acc: 0.6458 - val_loss: 1.3629 - val_acc: 0.5629\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.39685 to 1.36288, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.9073 - acc: 0.6499 - val_loss: 1.9737 - val_acc: 0.1577\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36288\n",
      "Epoch 7/50\n",
      "34018/34018 [==============================] - 3s 101us/step - loss: 0.8890 - acc: 0.6572 - val_loss: 1.4751 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.36288\n",
      "Epoch 8/50\n",
      "34018/34018 [==============================] - 4s 107us/step - loss: 0.8449 - acc: 0.6816 - val_loss: 1.1430 - val_acc: 0.5389\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.36288 to 1.14296, saving model to weights.hdf5\n",
      "Epoch 9/50\n",
      "34018/34018 [==============================] - 4s 107us/step - loss: 0.8336 - acc: 0.6871 - val_loss: 1.3727 - val_acc: 0.4451\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.14296\n",
      "Epoch 10/50\n",
      "34018/34018 [==============================] - 4s 109us/step - loss: 0.7716 - acc: 0.7096 - val_loss: 1.2069 - val_acc: 0.4671\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.14296\n",
      "Epoch 11/50\n",
      "34018/34018 [==============================] - 4s 110us/step - loss: 0.7420 - acc: 0.7196 - val_loss: 1.0797 - val_acc: 0.6627\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.14296 to 1.07974, saving model to weights.hdf5\n",
      "Epoch 12/50\n",
      "34018/34018 [==============================] - 4s 113us/step - loss: 0.7302 - acc: 0.7201 - val_loss: 1.3204 - val_acc: 0.5170\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.07974\n",
      "Epoch 13/50\n",
      "34018/34018 [==============================] - 4s 113us/step - loss: 0.7262 - acc: 0.7269 - val_loss: 1.3272 - val_acc: 0.4411\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.07974\n",
      "Epoch 14/50\n",
      "34018/34018 [==============================] - 3s 98us/step - loss: 0.7060 - acc: 0.7286 - val_loss: 0.9679 - val_acc: 0.6128\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.07974 to 0.96789, saving model to weights.hdf5\n",
      "Epoch 15/50\n",
      "34018/34018 [==============================] - 4s 106us/step - loss: 0.6648 - acc: 0.7466 - val_loss: 1.0290 - val_acc: 0.5449\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.96789\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34018/34018 [==============================] - 4s 104us/step - loss: 0.6706 - acc: 0.7493 - val_loss: 0.9388 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.96789 to 0.93879, saving model to weights.hdf5\n",
      "Epoch 17/50\n",
      "34018/34018 [==============================] - 4s 110us/step - loss: 0.6441 - acc: 0.7547 - val_loss: 1.0376 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.93879\n",
      "Epoch 18/50\n",
      "34018/34018 [==============================] - 4s 105us/step - loss: 0.6435 - acc: 0.7579 - val_loss: 0.9639 - val_acc: 0.6148\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.93879\n",
      "Epoch 19/50\n",
      "34018/34018 [==============================] - 4s 105us/step - loss: 0.6268 - acc: 0.7628 - val_loss: 1.0207 - val_acc: 0.5210\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.93879\n",
      "Epoch 20/50\n",
      "34018/34018 [==============================] - 4s 114us/step - loss: 0.6113 - acc: 0.7721 - val_loss: 0.8173 - val_acc: 0.7146\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.93879 to 0.81729, saving model to weights.hdf5\n",
      "Epoch 21/50\n",
      "34018/34018 [==============================] - 3s 99us/step - loss: 0.6238 - acc: 0.7652 - val_loss: 0.9197 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.81729\n",
      "Epoch 22/50\n",
      "34018/34018 [==============================] - 4s 109us/step - loss: 0.5882 - acc: 0.7799 - val_loss: 0.8603 - val_acc: 0.6707\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.81729\n",
      "Epoch 23/50\n",
      "34018/34018 [==============================] - 3s 102us/step - loss: 0.5851 - acc: 0.7840 - val_loss: 0.7937 - val_acc: 0.7026\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.81729 to 0.79369, saving model to weights.hdf5\n",
      "Epoch 24/50\n",
      "34018/34018 [==============================] - 4s 109us/step - loss: 0.5803 - acc: 0.7811 - val_loss: 1.2475 - val_acc: 0.5090\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.79369\n",
      "Epoch 25/50\n",
      "34018/34018 [==============================] - 4s 104us/step - loss: 0.5623 - acc: 0.7931 - val_loss: 0.6600 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.79369 to 0.66001, saving model to weights.hdf5\n",
      "Epoch 26/50\n",
      "34018/34018 [==============================] - 4s 115us/step - loss: 0.5685 - acc: 0.7900 - val_loss: 0.7157 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.66001\n",
      "Epoch 27/50\n",
      "34018/34018 [==============================] - 4s 107us/step - loss: 0.5737 - acc: 0.7873 - val_loss: 0.8829 - val_acc: 0.6088\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66001\n",
      "Epoch 28/50\n",
      "34018/34018 [==============================] - 3s 103us/step - loss: 0.5416 - acc: 0.8003 - val_loss: 1.0440 - val_acc: 0.5689\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66001\n",
      "Epoch 29/50\n",
      "34018/34018 [==============================] - 4s 110us/step - loss: 0.5387 - acc: 0.8020 - val_loss: 0.8232 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.66001\n",
      "Epoch 30/50\n",
      "34018/34018 [==============================] - 3s 102us/step - loss: 0.5464 - acc: 0.7987 - val_loss: 0.7877 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66001\n",
      "Epoch 31/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.5345 - acc: 0.8028 - val_loss: 0.7534 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66001\n",
      "Epoch 32/50\n",
      "34018/34018 [==============================] - 4s 105us/step - loss: 0.5195 - acc: 0.8078 - val_loss: 0.6944 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.66001\n",
      "Epoch 33/50\n",
      "34018/34018 [==============================] - 4s 110us/step - loss: 0.5257 - acc: 0.8020 - val_loss: 0.7866 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66001\n",
      "Epoch 34/50\n",
      "34018/34018 [==============================] - 4s 105us/step - loss: 0.5243 - acc: 0.8057 - val_loss: 0.5915 - val_acc: 0.7725\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.66001 to 0.59153, saving model to weights.hdf5\n",
      "Epoch 35/50\n",
      "34018/34018 [==============================] - 4s 112us/step - loss: 0.5048 - acc: 0.8127 - val_loss: 0.5383 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.59153 to 0.53833, saving model to weights.hdf5\n",
      "Epoch 36/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.5119 - acc: 0.8115 - val_loss: 0.6526 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.53833\n",
      "Epoch 37/50\n",
      "34018/34018 [==============================] - 3s 100us/step - loss: 0.4973 - acc: 0.8158 - val_loss: 0.8222 - val_acc: 0.6407\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.53833\n",
      "Epoch 38/50\n",
      "34018/34018 [==============================] - 4s 113us/step - loss: 0.5025 - acc: 0.8157 - val_loss: 0.6051 - val_acc: 0.7665\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.53833\n",
      "Epoch 39/50\n",
      "34018/34018 [==============================] - 4s 105us/step - loss: 0.4911 - acc: 0.8188 - val_loss: 0.6345 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.53833\n",
      "Epoch 40/50\n",
      "34018/34018 [==============================] - 4s 113us/step - loss: 0.4855 - acc: 0.8207 - val_loss: 0.6433 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.53833\n",
      "Epoch 41/50\n",
      "34018/34018 [==============================] - 4s 104us/step - loss: 0.4892 - acc: 0.8194 - val_loss: 0.5081 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.53833 to 0.50806, saving model to weights.hdf5\n",
      "Epoch 42/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.4799 - acc: 0.8210 - val_loss: 0.7981 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.50806\n",
      "Epoch 43/50\n",
      "34018/34018 [==============================] - 4s 104us/step - loss: 0.4684 - acc: 0.8255 - val_loss: 0.6667 - val_acc: 0.7405\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.50806\n",
      "Epoch 44/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.4762 - acc: 0.8227 - val_loss: 0.7598 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.50806\n",
      "Epoch 45/50\n",
      "34018/34018 [==============================] - 4s 105us/step - loss: 0.4667 - acc: 0.8280 - val_loss: 0.5234 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.50806\n",
      "Epoch 46/50\n",
      "34018/34018 [==============================] - 4s 103us/step - loss: 0.4750 - acc: 0.8225 - val_loss: 0.5289 - val_acc: 0.7924\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.50806\n",
      "Epoch 47/50\n",
      "34018/34018 [==============================] - 4s 109us/step - loss: 0.4592 - acc: 0.8277 - val_loss: 0.6583 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.50806\n",
      "Epoch 48/50\n",
      "34018/34018 [==============================] - 4s 108us/step - loss: 0.4463 - acc: 0.8331 - val_loss: 0.5580 - val_acc: 0.7824\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.50806\n",
      "Epoch 49/50\n",
      "34018/34018 [==============================] - 4s 110us/step - loss: 0.4581 - acc: 0.8295 - val_loss: 0.6352 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.50806\n",
      "Epoch 50/50\n",
      "34018/34018 [==============================] - 3s 101us/step - loss: 0.4494 - acc: 0.8350 - val_loss: 0.7427 - val_acc: 0.7066\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.50806\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [  7 102]\n",
      "Test set and training set proportion :  0.04051243406179352\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33175 samples, validate on 1344 samples\n",
      "Epoch 1/50\n",
      "33175/33175 [==============================] - 6s 180us/step - loss: 1.3522 - acc: 0.4580 - val_loss: 1.0705 - val_acc: 0.6443\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.07051, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33175/33175 [==============================] - 4s 111us/step - loss: 1.1585 - acc: 0.5533 - val_loss: 0.9751 - val_acc: 0.6287\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.07051 to 0.97509, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33175/33175 [==============================] - 4s 106us/step - loss: 1.0921 - acc: 0.5715 - val_loss: 0.9551 - val_acc: 0.5350\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.97509 to 0.95512, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33175/33175 [==============================] - 4s 120us/step - loss: 1.0493 - acc: 0.5905 - val_loss: 0.8598 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.95512 to 0.85978, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33175/33175 [==============================] - 4s 119us/step - loss: 1.0073 - acc: 0.6053 - val_loss: 0.8085 - val_acc: 0.7418\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85978 to 0.80850, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "33175/33175 [==============================] - 4s 106us/step - loss: 0.9533 - acc: 0.6377 - val_loss: 0.9478 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.80850\n",
      "Epoch 7/50\n",
      "33175/33175 [==============================] - 4s 119us/step - loss: 0.9577 - acc: 0.6360 - val_loss: 0.6845 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.80850 to 0.68451, saving model to weights.hdf5\n",
      "Epoch 8/50\n",
      "33175/33175 [==============================] - 4s 111us/step - loss: 0.9161 - acc: 0.6563 - val_loss: 0.9240 - val_acc: 0.5885\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.68451\n",
      "Epoch 9/50\n",
      "33175/33175 [==============================] - 4s 118us/step - loss: 0.8700 - acc: 0.6758 - val_loss: 0.5943 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.68451 to 0.59434, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "33175/33175 [==============================] - 4s 118us/step - loss: 0.8599 - acc: 0.6720 - val_loss: 0.7164 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.59434\n",
      "Epoch 11/50\n",
      "33175/33175 [==============================] - 4s 116us/step - loss: 0.8199 - acc: 0.6943 - val_loss: 0.8474 - val_acc: 0.6964\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.59434\n",
      "Epoch 12/50\n",
      "33175/33175 [==============================] - 4s 119us/step - loss: 0.8117 - acc: 0.6951 - val_loss: 0.9336 - val_acc: 0.6027\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.59434\n",
      "Epoch 13/50\n",
      "33175/33175 [==============================] - 4s 111us/step - loss: 0.7878 - acc: 0.7053 - val_loss: 1.4295 - val_acc: 0.5260\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.59434\n",
      "Epoch 14/50\n",
      "33175/33175 [==============================] - 4s 114us/step - loss: 0.7870 - acc: 0.7066 - val_loss: 0.4719 - val_acc: 0.8393\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.59434 to 0.47190, saving model to weights.hdf5\n",
      "Epoch 15/50\n",
      "33175/33175 [==============================] - 4s 113us/step - loss: 0.7462 - acc: 0.7184 - val_loss: 0.6641 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47190\n",
      "Epoch 16/50\n",
      "33175/33175 [==============================] - 4s 121us/step - loss: 0.7181 - acc: 0.7292 - val_loss: 0.6958 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47190\n",
      "Epoch 17/50\n",
      "33175/33175 [==============================] - 4s 117us/step - loss: 0.6811 - acc: 0.7459 - val_loss: 0.5303 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47190\n",
      "Epoch 18/50\n",
      "33175/33175 [==============================] - 4s 117us/step - loss: 0.6954 - acc: 0.7387 - val_loss: 0.7807 - val_acc: 0.7106\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.47190\n",
      "Epoch 19/50\n",
      "33175/33175 [==============================] - 4s 115us/step - loss: 0.6598 - acc: 0.7532 - val_loss: 0.3999 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47190 to 0.39993, saving model to weights.hdf5\n",
      "Epoch 20/50\n",
      "33175/33175 [==============================] - 4s 113us/step - loss: 0.6548 - acc: 0.7538 - val_loss: 0.6048 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.39993\n",
      "Epoch 21/50\n",
      "33175/33175 [==============================] - 4s 115us/step - loss: 0.6382 - acc: 0.7580 - val_loss: 1.0581 - val_acc: 0.5997\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.39993\n",
      "Epoch 22/50\n",
      "33175/33175 [==============================] - 4s 111us/step - loss: 0.6378 - acc: 0.7634 - val_loss: 0.9716 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.39993\n",
      "Epoch 23/50\n",
      "33175/33175 [==============================] - 3s 105us/step - loss: 0.6299 - acc: 0.7664 - val_loss: 0.6785 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.39993\n",
      "Epoch 24/50\n",
      "33175/33175 [==============================] - 4s 115us/step - loss: 0.6126 - acc: 0.7693 - val_loss: 0.6342 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.39993\n",
      "Epoch 25/50\n",
      "33175/33175 [==============================] - 4s 109us/step - loss: 0.5956 - acc: 0.7782 - val_loss: 0.6110 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.39993\n",
      "Epoch 26/50\n",
      "33175/33175 [==============================] - 4s 118us/step - loss: 0.5879 - acc: 0.7825 - val_loss: 0.5097 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.39993\n",
      "Epoch 27/50\n",
      "33175/33175 [==============================] - 4s 113us/step - loss: 0.5928 - acc: 0.7794 - val_loss: 0.5915 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.39993\n",
      "Epoch 28/50\n",
      "33175/33175 [==============================] - 4s 116us/step - loss: 0.5761 - acc: 0.7864 - val_loss: 0.4502 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.39993\n",
      "Epoch 29/50\n",
      "33175/33175 [==============================] - 4s 113us/step - loss: 0.5805 - acc: 0.7828 - val_loss: 0.4542 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.39993\n",
      "Epoch 30/50\n",
      "33175/33175 [==============================] - 4s 113us/step - loss: 0.5559 - acc: 0.7898 - val_loss: 0.5920 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.39993\n",
      "Epoch 31/50\n",
      "33175/33175 [==============================] - 4s 112us/step - loss: 0.5611 - acc: 0.7910 - val_loss: 0.8545 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.39993\n",
      "Epoch 32/50\n",
      "33175/33175 [==============================] - 4s 109us/step - loss: 0.5539 - acc: 0.7956 - val_loss: 1.1172 - val_acc: 0.6533\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.39993\n",
      "Epoch 33/50\n",
      "33175/33175 [==============================] - 4s 113us/step - loss: 0.5462 - acc: 0.7973 - val_loss: 0.3198 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.39993 to 0.31984, saving model to weights.hdf5\n",
      "Epoch 34/50\n",
      "33175/33175 [==============================] - 4s 106us/step - loss: 0.5359 - acc: 0.8030 - val_loss: 0.5858 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.31984\n",
      "Epoch 35/50\n",
      "33175/33175 [==============================] - 4s 108us/step - loss: 0.5388 - acc: 0.7996 - val_loss: 0.8625 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.31984\n",
      "Epoch 36/50\n",
      "33175/33175 [==============================] - 3s 105us/step - loss: 0.5241 - acc: 0.8053 - val_loss: 0.8467 - val_acc: 0.7738\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.31984\n",
      "Epoch 37/50\n",
      "33175/33175 [==============================] - 3s 105us/step - loss: 0.5284 - acc: 0.8028 - val_loss: 0.6452 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.31984\n",
      "Epoch 38/50\n",
      "33175/33175 [==============================] - 4s 111us/step - loss: 0.5164 - acc: 0.8076 - val_loss: 0.6600 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.31984\n",
      "Epoch 39/50\n",
      "33175/33175 [==============================] - 3s 104us/step - loss: 0.5084 - acc: 0.8106 - val_loss: 0.9518 - val_acc: 0.7560\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.31984\n",
      "Epoch 40/50\n",
      "33175/33175 [==============================] - 4s 110us/step - loss: 0.5042 - acc: 0.8108 - val_loss: 0.8340 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.31984\n",
      "Epoch 41/50\n",
      "33175/33175 [==============================] - 4s 110us/step - loss: 0.5021 - acc: 0.8131 - val_loss: 1.0129 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.31984\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33175/33175 [==============================] - 4s 110us/step - loss: 0.5050 - acc: 0.8118 - val_loss: 0.8395 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.31984\n",
      "Epoch 43/50\n",
      "33175/33175 [==============================] - 4s 108us/step - loss: 0.4906 - acc: 0.8154 - val_loss: 0.7835 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.31984\n",
      "Epoch 44/50\n",
      "33175/33175 [==============================] - 4s 116us/step - loss: 0.4896 - acc: 0.8195 - val_loss: 0.9952 - val_acc: 0.7560\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.31984\n",
      "Epoch 45/50\n",
      "33175/33175 [==============================] - 4s 108us/step - loss: 0.4965 - acc: 0.8157 - val_loss: 0.5599 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.31984\n",
      "Epoch 46/50\n",
      "33175/33175 [==============================] - 4s 109us/step - loss: 0.4718 - acc: 0.8249 - val_loss: 0.8753 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.31984\n",
      "Epoch 47/50\n",
      "33175/33175 [==============================] - 4s 110us/step - loss: 0.4779 - acc: 0.8219 - val_loss: 0.5935 - val_acc: 0.7738\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.31984\n",
      "Epoch 48/50\n",
      "33175/33175 [==============================] - 4s 110us/step - loss: 0.4834 - acc: 0.8175 - val_loss: 0.8544 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.31984\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [ 9 15]\n",
      "Test set and training set proportion :  0.06448131244603429\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32428 samples, validate on 2091 samples\n",
      "Epoch 1/50\n",
      "32428/32428 [==============================] - 6s 193us/step - loss: 1.3444 - acc: 0.4500 - val_loss: 0.9814 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98139, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32428/32428 [==============================] - 3s 107us/step - loss: 1.1635 - acc: 0.5397 - val_loss: 0.9036 - val_acc: 0.6681\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98139 to 0.90356, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "32428/32428 [==============================] - 3s 106us/step - loss: 1.0905 - acc: 0.5719 - val_loss: 0.8315 - val_acc: 0.7164\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90356 to 0.83147, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "32428/32428 [==============================] - 4s 108us/step - loss: 1.0376 - acc: 0.5915 - val_loss: 0.9467 - val_acc: 0.5839\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.83147\n",
      "Epoch 5/50\n",
      "32428/32428 [==============================] - 4s 115us/step - loss: 1.0098 - acc: 0.6051 - val_loss: 0.7901 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.83147 to 0.79007, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.9847 - acc: 0.6186 - val_loss: 0.8643 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.79007\n",
      "Epoch 7/50\n",
      "32428/32428 [==============================] - 4s 113us/step - loss: 0.9440 - acc: 0.6428 - val_loss: 0.8298 - val_acc: 0.6628\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.79007\n",
      "Epoch 8/50\n",
      "32428/32428 [==============================] - 3s 108us/step - loss: 0.8991 - acc: 0.6551 - val_loss: 1.0660 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.79007\n",
      "Epoch 9/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.8732 - acc: 0.6689 - val_loss: 0.8534 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.79007\n",
      "Epoch 10/50\n",
      "32428/32428 [==============================] - 4s 113us/step - loss: 0.8553 - acc: 0.6762 - val_loss: 0.8586 - val_acc: 0.7126\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79007\n",
      "Epoch 11/50\n",
      "32428/32428 [==============================] - 3s 108us/step - loss: 0.7914 - acc: 0.6993 - val_loss: 0.7419 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79007 to 0.74193, saving model to weights.hdf5\n",
      "Epoch 12/50\n",
      "32428/32428 [==============================] - 4s 115us/step - loss: 0.7984 - acc: 0.6968 - val_loss: 0.8126 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.74193\n",
      "Epoch 13/50\n",
      "32428/32428 [==============================] - 3s 106us/step - loss: 0.7463 - acc: 0.7161 - val_loss: 0.9180 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.74193\n",
      "Epoch 14/50\n",
      "32428/32428 [==============================] - 4s 114us/step - loss: 0.7538 - acc: 0.7142 - val_loss: 0.7187 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.74193 to 0.71869, saving model to weights.hdf5\n",
      "Epoch 15/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.7252 - acc: 0.7259 - val_loss: 0.7801 - val_acc: 0.7489\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.71869\n",
      "Epoch 16/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.6871 - acc: 0.7388 - val_loss: 0.7891 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.71869\n",
      "Epoch 17/50\n",
      "32428/32428 [==============================] - 4s 110us/step - loss: 0.6777 - acc: 0.7433 - val_loss: 0.6919 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.71869 to 0.69187, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.6805 - acc: 0.7417 - val_loss: 0.7257 - val_acc: 0.7432\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.69187\n",
      "Epoch 19/50\n",
      "32428/32428 [==============================] - 4s 112us/step - loss: 0.6470 - acc: 0.7572 - val_loss: 0.7646 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69187\n",
      "Epoch 20/50\n",
      "32428/32428 [==============================] - 4s 111us/step - loss: 0.6576 - acc: 0.7506 - val_loss: 0.6155 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.69187 to 0.61549, saving model to weights.hdf5\n",
      "Epoch 21/50\n",
      "32428/32428 [==============================] - 4s 115us/step - loss: 0.6300 - acc: 0.7621 - val_loss: 0.6088 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.61549 to 0.60879, saving model to weights.hdf5\n",
      "Epoch 22/50\n",
      "32428/32428 [==============================] - 4s 108us/step - loss: 0.6037 - acc: 0.7727 - val_loss: 0.6732 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.60879\n",
      "Epoch 23/50\n",
      "32428/32428 [==============================] - 4s 114us/step - loss: 0.6116 - acc: 0.7714 - val_loss: 0.6053 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.60879 to 0.60531, saving model to weights.hdf5\n",
      "Epoch 24/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.5893 - acc: 0.7788 - val_loss: 0.7491 - val_acc: 0.6624\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.60531\n",
      "Epoch 25/50\n",
      "32428/32428 [==============================] - 4s 112us/step - loss: 0.5867 - acc: 0.7795 - val_loss: 0.8127 - val_acc: 0.6246\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.60531\n",
      "Epoch 26/50\n",
      "32428/32428 [==============================] - 3s 105us/step - loss: 0.5798 - acc: 0.7810 - val_loss: 0.5631 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.60531 to 0.56314, saving model to weights.hdf5\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32428/32428 [==============================] - 4s 110us/step - loss: 0.6071 - acc: 0.7728 - val_loss: 0.6866 - val_acc: 0.6958\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.56314\n",
      "Epoch 28/50\n",
      "32428/32428 [==============================] - 4s 108us/step - loss: 0.5332 - acc: 0.7994 - val_loss: 0.6461 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.56314\n",
      "Epoch 29/50\n",
      "32428/32428 [==============================] - 3s 108us/step - loss: 0.5539 - acc: 0.7935 - val_loss: 0.5436 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.56314 to 0.54363, saving model to weights.hdf5\n",
      "Epoch 30/50\n",
      "32428/32428 [==============================] - 4s 112us/step - loss: 0.5449 - acc: 0.7949 - val_loss: 0.5912 - val_acc: 0.7513\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.54363\n",
      "Epoch 31/50\n",
      "32428/32428 [==============================] - 3s 107us/step - loss: 0.5343 - acc: 0.7977 - val_loss: 0.6574 - val_acc: 0.7226\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.54363\n",
      "Epoch 32/50\n",
      "32428/32428 [==============================] - 4s 112us/step - loss: 0.5401 - acc: 0.7967 - val_loss: 0.7347 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.54363\n",
      "Epoch 33/50\n",
      "32428/32428 [==============================] - 4s 116us/step - loss: 0.5233 - acc: 0.8021 - val_loss: 0.5877 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.54363\n",
      "Epoch 34/50\n",
      "32428/32428 [==============================] - 4s 111us/step - loss: 0.5285 - acc: 0.8017 - val_loss: 0.4749 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.54363 to 0.47493, saving model to weights.hdf5\n",
      "Epoch 35/50\n",
      "32428/32428 [==============================] - 4s 112us/step - loss: 0.5166 - acc: 0.8057 - val_loss: 0.5189 - val_acc: 0.8355\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47493\n",
      "Epoch 36/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.5000 - acc: 0.8126 - val_loss: 0.5542 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47493\n",
      "Epoch 37/50\n",
      "32428/32428 [==============================] - 4s 108us/step - loss: 0.5234 - acc: 0.8029 - val_loss: 0.5714 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47493\n",
      "Epoch 38/50\n",
      "32428/32428 [==============================] - 4s 112us/step - loss: 0.4902 - acc: 0.8176 - val_loss: 0.5647 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47493\n",
      "Epoch 39/50\n",
      "32428/32428 [==============================] - 4s 109us/step - loss: 0.4899 - acc: 0.8151 - val_loss: 0.5298 - val_acc: 0.8436\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47493\n",
      "Epoch 40/50\n",
      "32428/32428 [==============================] - 4s 111us/step - loss: 0.4952 - acc: 0.8138 - val_loss: 0.5549 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47493\n",
      "Epoch 41/50\n",
      "32428/32428 [==============================] - 4s 108us/step - loss: 0.4857 - acc: 0.8191 - val_loss: 0.7366 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47493\n",
      "Epoch 42/50\n",
      "32428/32428 [==============================] - 3s 106us/step - loss: 0.4817 - acc: 0.8185 - val_loss: 0.6234 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47493\n",
      "Epoch 43/50\n",
      "32428/32428 [==============================] - 4s 113us/step - loss: 0.4803 - acc: 0.8176 - val_loss: 0.6101 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47493\n",
      "Epoch 44/50\n",
      "32428/32428 [==============================] - 3s 104us/step - loss: 0.4804 - acc: 0.8194 - val_loss: 0.5168 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47493\n",
      "Epoch 45/50\n",
      "32428/32428 [==============================] - 4s 110us/step - loss: 0.4712 - acc: 0.8221 - val_loss: 0.6749 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47493\n",
      "Epoch 46/50\n",
      "32428/32428 [==============================] - 3s 104us/step - loss: 0.4600 - acc: 0.8269 - val_loss: 0.5735 - val_acc: 0.7767\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47493\n",
      "Epoch 47/50\n",
      "32428/32428 [==============================] - 4s 113us/step - loss: 0.4672 - acc: 0.8241 - val_loss: 0.5453 - val_acc: 0.8331\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47493\n",
      "Epoch 48/50\n",
      "32428/32428 [==============================] - 4s 111us/step - loss: 0.4545 - acc: 0.8270 - val_loss: 0.4951 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47493\n",
      "Epoch 49/50\n",
      "32428/32428 [==============================] - 3s 108us/step - loss: 0.4595 - acc: 0.8249 - val_loss: 0.5242 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47493\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [14 28]\n",
      "Test set and training set proportion :  0.06271165568622622\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_52 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32482 samples, validate on 2037 samples\n",
      "Epoch 1/50\n",
      "32482/32482 [==============================] - 6s 200us/step - loss: 1.3546 - acc: 0.4406 - val_loss: 1.3650 - val_acc: 0.3927\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36502, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32482/32482 [==============================] - 4s 111us/step - loss: 1.1474 - acc: 0.5398 - val_loss: 1.4050 - val_acc: 0.3486\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.36502\n",
      "Epoch 3/50\n",
      "32482/32482 [==============================] - 4s 114us/step - loss: 1.0978 - acc: 0.5590 - val_loss: 1.3159 - val_acc: 0.4340\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.36502 to 1.31589, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "32482/32482 [==============================] - 3s 105us/step - loss: 1.0355 - acc: 0.6008 - val_loss: 1.4144 - val_acc: 0.5110\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.31589\n",
      "Epoch 5/50\n",
      "32482/32482 [==============================] - 4s 114us/step - loss: 0.9887 - acc: 0.6187 - val_loss: 1.2461 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31589 to 1.24609, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "32482/32482 [==============================] - 4s 108us/step - loss: 0.9545 - acc: 0.6343 - val_loss: 1.2904 - val_acc: 0.5675\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24609\n",
      "Epoch 7/50\n",
      "32482/32482 [==============================] - 4s 118us/step - loss: 0.9137 - acc: 0.6485 - val_loss: 1.2720 - val_acc: 0.4566\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24609\n",
      "Epoch 8/50\n",
      "32482/32482 [==============================] - 4s 111us/step - loss: 0.8745 - acc: 0.6694 - val_loss: 1.4405 - val_acc: 0.3652\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.24609\n",
      "Epoch 9/50\n",
      "32482/32482 [==============================] - 4s 113us/step - loss: 0.8379 - acc: 0.6812 - val_loss: 1.5413 - val_acc: 0.3633\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.24609\n",
      "Epoch 10/50\n",
      "32482/32482 [==============================] - 4s 109us/step - loss: 0.8242 - acc: 0.6858 - val_loss: 1.7081 - val_acc: 0.3667\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.24609\n",
      "Epoch 11/50\n",
      "32482/32482 [==============================] - 4s 109us/step - loss: 0.8176 - acc: 0.6922 - val_loss: 1.4423 - val_acc: 0.4845\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.24609\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32482/32482 [==============================] - 4s 111us/step - loss: 0.7458 - acc: 0.7251 - val_loss: 1.3902 - val_acc: 0.4629\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.24609\n",
      "Epoch 13/50\n",
      "32482/32482 [==============================] - 3s 107us/step - loss: 0.7175 - acc: 0.7333 - val_loss: 1.3455 - val_acc: 0.4772\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.24609\n",
      "Epoch 14/50\n",
      "32482/32482 [==============================] - 4s 114us/step - loss: 0.6803 - acc: 0.7505 - val_loss: 1.4533 - val_acc: 0.5218\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.24609\n",
      "Epoch 15/50\n",
      "32482/32482 [==============================] - 4s 109us/step - loss: 0.6939 - acc: 0.7400 - val_loss: 1.4179 - val_acc: 0.4438\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.24609\n",
      "Epoch 16/50\n",
      "32482/32482 [==============================] - 4s 110us/step - loss: 0.6561 - acc: 0.7609 - val_loss: 1.4080 - val_acc: 0.5474\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.24609\n",
      "Epoch 17/50\n",
      "32482/32482 [==============================] - 3s 107us/step - loss: 0.6440 - acc: 0.7604 - val_loss: 1.3026 - val_acc: 0.5739\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.24609\n",
      "Epoch 18/50\n",
      "32482/32482 [==============================] - 4s 113us/step - loss: 0.6430 - acc: 0.7626 - val_loss: 1.5124 - val_acc: 0.5101\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.24609\n",
      "Epoch 19/50\n",
      "32482/32482 [==============================] - 4s 113us/step - loss: 0.6553 - acc: 0.7554 - val_loss: 1.3736 - val_acc: 0.5037\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.24609\n",
      "Epoch 20/50\n",
      "32482/32482 [==============================] - 3s 104us/step - loss: 0.6002 - acc: 0.7792 - val_loss: 1.4047 - val_acc: 0.4772\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.24609\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [102 108]\n",
      "Test set and training set proportion :  0.015981869554979985\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33976 samples, validate on 543 samples\n",
      "Epoch 1/50\n",
      "33976/33976 [==============================] - 7s 205us/step - loss: 1.3655 - acc: 0.4553 - val_loss: 1.4368 - val_acc: 0.2192\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43684, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33976/33976 [==============================] - 4s 108us/step - loss: 1.1618 - acc: 0.5425 - val_loss: 1.2347 - val_acc: 0.6298\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43684 to 1.23467, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33976/33976 [==============================] - 4s 106us/step - loss: 1.1082 - acc: 0.5659 - val_loss: 1.4436 - val_acc: 0.4180\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.23467\n",
      "Epoch 4/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 1.0627 - acc: 0.5858 - val_loss: 1.0025 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.23467 to 1.00250, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.9997 - acc: 0.6133 - val_loss: 0.9377 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00250 to 0.93769, saving model to weights.hdf5\n",
      "Epoch 6/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.9722 - acc: 0.6267 - val_loss: 0.9336 - val_acc: 0.6611\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.93769 to 0.93364, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "33976/33976 [==============================] - 4s 109us/step - loss: 0.9422 - acc: 0.6431 - val_loss: 0.9602 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.93364\n",
      "Epoch 8/50\n",
      "33976/33976 [==============================] - 4s 106us/step - loss: 0.8936 - acc: 0.6664 - val_loss: 0.9898 - val_acc: 0.5727\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.93364\n",
      "Epoch 9/50\n",
      "33976/33976 [==============================] - 3s 98us/step - loss: 0.8882 - acc: 0.6591 - val_loss: 0.7760 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.93364 to 0.77596, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "33976/33976 [==============================] - 4s 111us/step - loss: 0.8400 - acc: 0.6849 - val_loss: 0.6270 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.77596 to 0.62696, saving model to weights.hdf5\n",
      "Epoch 11/50\n",
      "33976/33976 [==============================] - 4s 104us/step - loss: 0.8124 - acc: 0.6963 - val_loss: 0.5616 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.62696 to 0.56163, saving model to weights.hdf5\n",
      "Epoch 12/50\n",
      "33976/33976 [==============================] - 4s 113us/step - loss: 0.7894 - acc: 0.7058 - val_loss: 0.5245 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56163 to 0.52449, saving model to weights.hdf5\n",
      "Epoch 13/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 0.7489 - acc: 0.7185 - val_loss: 0.6561 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.52449\n",
      "Epoch 14/50\n",
      "33976/33976 [==============================] - 3s 102us/step - loss: 0.7179 - acc: 0.7296 - val_loss: 0.4768 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.52449 to 0.47677, saving model to weights.hdf5\n",
      "Epoch 15/50\n",
      "33976/33976 [==============================] - 4s 114us/step - loss: 0.7153 - acc: 0.7296 - val_loss: 0.4142 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47677 to 0.41416, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 0.6852 - acc: 0.7360 - val_loss: 0.4668 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.41416\n",
      "Epoch 17/50\n",
      "33976/33976 [==============================] - 4s 111us/step - loss: 0.6677 - acc: 0.7433 - val_loss: 0.3806 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.41416 to 0.38064, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "33976/33976 [==============================] - 4s 103us/step - loss: 0.6549 - acc: 0.7492 - val_loss: 0.4326 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.38064\n",
      "Epoch 19/50\n",
      "33976/33976 [==============================] - 4s 109us/step - loss: 0.6457 - acc: 0.7516 - val_loss: 0.4054 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.38064\n",
      "Epoch 20/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 0.6299 - acc: 0.7611 - val_loss: 0.3711 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.38064 to 0.37112, saving model to weights.hdf5\n",
      "Epoch 21/50\n",
      "33976/33976 [==============================] - 4s 108us/step - loss: 0.6154 - acc: 0.7661 - val_loss: 0.3992 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.37112\n",
      "Epoch 22/50\n",
      "33976/33976 [==============================] - 4s 109us/step - loss: 0.6111 - acc: 0.7697 - val_loss: 0.5026 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.37112\n",
      "Epoch 23/50\n",
      "33976/33976 [==============================] - 4s 112us/step - loss: 0.6016 - acc: 0.7734 - val_loss: 0.4349 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.37112\n",
      "Epoch 24/50\n",
      "33976/33976 [==============================] - 4s 106us/step - loss: 0.5906 - acc: 0.7772 - val_loss: 0.4112 - val_acc: 0.8085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss did not improve from 0.37112\n",
      "Epoch 25/50\n",
      "33976/33976 [==============================] - 3s 101us/step - loss: 0.5754 - acc: 0.7816 - val_loss: 0.3700 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.37112 to 0.37002, saving model to weights.hdf5\n",
      "Epoch 26/50\n",
      "33976/33976 [==============================] - 4s 105us/step - loss: 0.5804 - acc: 0.7808 - val_loss: 0.3359 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.37002 to 0.33586, saving model to weights.hdf5\n",
      "Epoch 27/50\n",
      "33976/33976 [==============================] - 4s 104us/step - loss: 0.5770 - acc: 0.7824 - val_loss: 0.3443 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.33586\n",
      "Epoch 28/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.5532 - acc: 0.7909 - val_loss: 0.3625 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.33586\n",
      "Epoch 29/50\n",
      "33976/33976 [==============================] - 4s 105us/step - loss: 0.5580 - acc: 0.7913 - val_loss: 0.3774 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.33586\n",
      "Epoch 30/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 0.5495 - acc: 0.7934 - val_loss: 0.4171 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.33586\n",
      "Epoch 31/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 0.5388 - acc: 0.7989 - val_loss: 0.3381 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.33586\n",
      "Epoch 32/50\n",
      "33976/33976 [==============================] - 4s 107us/step - loss: 0.5472 - acc: 0.7949 - val_loss: 0.3433 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.33586\n",
      "Epoch 33/50\n",
      "33976/33976 [==============================] - 4s 113us/step - loss: 0.5235 - acc: 0.8042 - val_loss: 0.3502 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.33586\n",
      "Epoch 34/50\n",
      "33976/33976 [==============================] - 4s 105us/step - loss: 0.5303 - acc: 0.8001 - val_loss: 0.4191 - val_acc: 0.8379\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.33586\n",
      "Epoch 35/50\n",
      "33976/33976 [==============================] - 4s 115us/step - loss: 0.5114 - acc: 0.8072 - val_loss: 0.4348 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.33586\n",
      "Epoch 36/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.5128 - acc: 0.8078 - val_loss: 0.4333 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.33586\n",
      "Epoch 37/50\n",
      "33976/33976 [==============================] - 4s 105us/step - loss: 0.5066 - acc: 0.8102 - val_loss: 0.3279 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.33586 to 0.32788, saving model to weights.hdf5\n",
      "Epoch 38/50\n",
      "33976/33976 [==============================] - 4s 111us/step - loss: 0.5052 - acc: 0.8115 - val_loss: 0.4001 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.32788\n",
      "Epoch 39/50\n",
      "33976/33976 [==============================] - 4s 108us/step - loss: 0.4891 - acc: 0.8204 - val_loss: 0.4443 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.32788\n",
      "Epoch 40/50\n",
      "33976/33976 [==============================] - 4s 111us/step - loss: 0.5030 - acc: 0.8096 - val_loss: 0.3455 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.32788\n",
      "Epoch 41/50\n",
      "33976/33976 [==============================] - 4s 104us/step - loss: 0.4794 - acc: 0.8212 - val_loss: 0.3412 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.32788\n",
      "Epoch 42/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.4745 - acc: 0.8229 - val_loss: 0.3089 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.32788 to 0.30890, saving model to weights.hdf5\n",
      "Epoch 43/50\n",
      "33976/33976 [==============================] - 4s 104us/step - loss: 0.4776 - acc: 0.8216 - val_loss: 0.3270 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.30890\n",
      "Epoch 44/50\n",
      "33976/33976 [==============================] - 4s 111us/step - loss: 0.4707 - acc: 0.8239 - val_loss: 0.3474 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.30890\n",
      "Epoch 45/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.4773 - acc: 0.8226 - val_loss: 0.3176 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.30890\n",
      "Epoch 46/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.4603 - acc: 0.8271 - val_loss: 0.3087 - val_acc: 0.8877\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.30890 to 0.30873, saving model to weights.hdf5\n",
      "Epoch 47/50\n",
      "33976/33976 [==============================] - 4s 112us/step - loss: 0.4628 - acc: 0.8247 - val_loss: 0.3787 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.30873\n",
      "Epoch 48/50\n",
      "33976/33976 [==============================] - 4s 110us/step - loss: 0.4497 - acc: 0.8322 - val_loss: 0.3292 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.30873\n",
      "Epoch 49/50\n",
      "33976/33976 [==============================] - 4s 115us/step - loss: 0.4551 - acc: 0.8299 - val_loss: 0.3198 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.30873\n",
      "Epoch 50/50\n",
      "33976/33976 [==============================] - 4s 109us/step - loss: 0.4516 - acc: 0.8316 - val_loss: 0.3461 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.30873\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [113 119]\n",
      "Test set and training set proportion :  0.021907102045649662\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33779 samples, validate on 740 samples\n",
      "Epoch 1/50\n",
      "33779/33779 [==============================] - 7s 198us/step - loss: 1.3507 - acc: 0.4541 - val_loss: 1.7217 - val_acc: 0.3068\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72172, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "33779/33779 [==============================] - 4s 108us/step - loss: 1.1223 - acc: 0.5722 - val_loss: 1.5383 - val_acc: 0.4041\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.72172 to 1.53826, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "33779/33779 [==============================] - 4s 111us/step - loss: 1.0762 - acc: 0.5803 - val_loss: 1.5030 - val_acc: 0.4068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53826 to 1.50298, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "33779/33779 [==============================] - 4s 107us/step - loss: 1.0128 - acc: 0.6058 - val_loss: 1.4261 - val_acc: 0.4608\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50298 to 1.42614, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "33779/33779 [==============================] - 4s 112us/step - loss: 0.9843 - acc: 0.6244 - val_loss: 1.7202 - val_acc: 0.3446\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42614\n",
      "Epoch 6/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.9385 - acc: 0.6416 - val_loss: 1.6023 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42614\n",
      "Epoch 7/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.9026 - acc: 0.6555 - val_loss: 1.6888 - val_acc: 0.2324\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.42614\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33779/33779 [==============================] - 4s 118us/step - loss: 0.8606 - acc: 0.6733 - val_loss: 1.8288 - val_acc: 0.2351\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.42614\n",
      "Epoch 9/50\n",
      "33779/33779 [==============================] - 4s 108us/step - loss: 0.8197 - acc: 0.6928 - val_loss: 1.3317 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.42614 to 1.33174, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.7969 - acc: 0.7022 - val_loss: 1.2293 - val_acc: 0.4838\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.33174 to 1.22934, saving model to weights.hdf5\n",
      "Epoch 11/50\n",
      "33779/33779 [==============================] - 4s 113us/step - loss: 0.7818 - acc: 0.7055 - val_loss: 1.2217 - val_acc: 0.5243\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.22934 to 1.22168, saving model to weights.hdf5\n",
      "Epoch 12/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.7383 - acc: 0.7220 - val_loss: 1.3168 - val_acc: 0.5108\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.22168\n",
      "Epoch 13/50\n",
      "33779/33779 [==============================] - 4s 108us/step - loss: 0.6986 - acc: 0.7358 - val_loss: 1.3096 - val_acc: 0.5081\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.22168\n",
      "Epoch 14/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.7011 - acc: 0.7387 - val_loss: 0.9930 - val_acc: 0.5581\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.22168 to 0.99302, saving model to weights.hdf5\n",
      "Epoch 15/50\n",
      "33779/33779 [==============================] - 4s 111us/step - loss: 0.6753 - acc: 0.7415 - val_loss: 0.9376 - val_acc: 0.5581\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.99302 to 0.93758, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.6588 - acc: 0.7497 - val_loss: 0.9689 - val_acc: 0.5608\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.93758\n",
      "Epoch 17/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.6386 - acc: 0.7589 - val_loss: 0.9181 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.93758 to 0.91808, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "33779/33779 [==============================] - 3s 103us/step - loss: 0.6551 - acc: 0.7557 - val_loss: 1.0323 - val_acc: 0.5459\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.91808\n",
      "Epoch 19/50\n",
      "33779/33779 [==============================] - 4s 114us/step - loss: 0.6235 - acc: 0.7676 - val_loss: 0.9197 - val_acc: 0.5541\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.91808\n",
      "Epoch 20/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.6073 - acc: 0.7754 - val_loss: 0.9350 - val_acc: 0.5189\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.91808\n",
      "Epoch 21/50\n",
      "33779/33779 [==============================] - 4s 109us/step - loss: 0.6009 - acc: 0.7768 - val_loss: 0.7945 - val_acc: 0.5851\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.91808 to 0.79446, saving model to weights.hdf5\n",
      "Epoch 22/50\n",
      "33779/33779 [==============================] - 4s 114us/step - loss: 0.5964 - acc: 0.7796 - val_loss: 0.8365 - val_acc: 0.5770\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.79446\n",
      "Epoch 23/50\n",
      "33779/33779 [==============================] - 4s 106us/step - loss: 0.5882 - acc: 0.7815 - val_loss: 0.8474 - val_acc: 0.5743\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.79446\n",
      "Epoch 24/50\n",
      "33779/33779 [==============================] - 4s 111us/step - loss: 0.5666 - acc: 0.7924 - val_loss: 0.8851 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.79446\n",
      "Epoch 25/50\n",
      "33779/33779 [==============================] - 4s 106us/step - loss: 0.5730 - acc: 0.7853 - val_loss: 0.8294 - val_acc: 0.5730\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.79446\n",
      "Epoch 26/50\n",
      "33779/33779 [==============================] - 4s 113us/step - loss: 0.5538 - acc: 0.7957 - val_loss: 0.7579 - val_acc: 0.5986\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.79446 to 0.75788, saving model to weights.hdf5\n",
      "Epoch 27/50\n",
      "33779/33779 [==============================] - 4s 109us/step - loss: 0.5631 - acc: 0.7916 - val_loss: 0.7776 - val_acc: 0.5932\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.75788\n",
      "Epoch 28/50\n",
      "33779/33779 [==============================] - 4s 107us/step - loss: 0.5405 - acc: 0.7972 - val_loss: 0.8008 - val_acc: 0.5716\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.75788\n",
      "Epoch 29/50\n",
      "33779/33779 [==============================] - 4s 114us/step - loss: 0.5477 - acc: 0.7972 - val_loss: 0.7798 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.75788\n",
      "Epoch 30/50\n",
      "33779/33779 [==============================] - 3s 102us/step - loss: 0.5332 - acc: 0.8051 - val_loss: 0.7908 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.75788\n",
      "Epoch 31/50\n",
      "33779/33779 [==============================] - 4s 117us/step - loss: 0.5121 - acc: 0.8132 - val_loss: 0.8019 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.75788\n",
      "Epoch 32/50\n",
      "33779/33779 [==============================] - 3s 103us/step - loss: 0.5230 - acc: 0.8102 - val_loss: 0.7719 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.75788\n",
      "Epoch 33/50\n",
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.5119 - acc: 0.8156 - val_loss: 0.8359 - val_acc: 0.5743\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.75788\n",
      "Epoch 34/50\n",
      "33779/33779 [==============================] - 4s 107us/step - loss: 0.4996 - acc: 0.8155 - val_loss: 0.7790 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.75788\n",
      "Epoch 35/50\n",
      "33779/33779 [==============================] - 4s 112us/step - loss: 0.5094 - acc: 0.8115 - val_loss: 0.7474 - val_acc: 0.6027\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.75788 to 0.74738, saving model to weights.hdf5\n",
      "Epoch 36/50\n",
      "33779/33779 [==============================] - 4s 112us/step - loss: 0.4984 - acc: 0.8155 - val_loss: 0.7886 - val_acc: 0.5878\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.74738\n",
      "Epoch 37/50\n",
      "33779/33779 [==============================] - 4s 105us/step - loss: 0.4856 - acc: 0.8219 - val_loss: 0.7670 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.74738\n",
      "Epoch 38/50\n",
      "33779/33779 [==============================] - 4s 109us/step - loss: 0.4858 - acc: 0.8255 - val_loss: 0.7791 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.74738\n",
      "Epoch 39/50\n",
      "33779/33779 [==============================] - 3s 102us/step - loss: 0.4847 - acc: 0.8233 - val_loss: 0.7296 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.74738 to 0.72958, saving model to weights.hdf5\n",
      "Epoch 40/50\n",
      "33779/33779 [==============================] - 4s 111us/step - loss: 0.4697 - acc: 0.8263 - val_loss: 0.7867 - val_acc: 0.5757\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.72958\n",
      "Epoch 41/50\n",
      "33779/33779 [==============================] - 4s 109us/step - loss: 0.4728 - acc: 0.8265 - val_loss: 0.8938 - val_acc: 0.5689\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.72958\n",
      "Epoch 42/50\n",
      "33779/33779 [==============================] - 4s 113us/step - loss: 0.4653 - acc: 0.8310 - val_loss: 0.7136 - val_acc: 0.6135\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.72958 to 0.71361, saving model to weights.hdf5\n",
      "Epoch 43/50\n",
      "33779/33779 [==============================] - 4s 112us/step - loss: 0.4605 - acc: 0.8325 - val_loss: 0.7169 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.71361\n",
      "Epoch 44/50\n",
      "33779/33779 [==============================] - 4s 108us/step - loss: 0.4647 - acc: 0.8306 - val_loss: 0.7801 - val_acc: 0.5878\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.71361\n",
      "Epoch 45/50\n",
      "33779/33779 [==============================] - 4s 107us/step - loss: 0.4667 - acc: 0.8274 - val_loss: 0.7858 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.71361\n",
      "Epoch 46/50\n",
      "33779/33779 [==============================] - 4s 104us/step - loss: 0.4443 - acc: 0.8368 - val_loss: 0.7206 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.71361\n",
      "Epoch 47/50\n",
      "33779/33779 [==============================] - 4s 113us/step - loss: 0.4572 - acc: 0.8329 - val_loss: 0.7798 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.71361\n",
      "Epoch 48/50\n",
      "33779/33779 [==============================] - 4s 109us/step - loss: 0.4497 - acc: 0.8360 - val_loss: 0.7273 - val_acc: 0.5838\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.71361\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33779/33779 [==============================] - 4s 110us/step - loss: 0.4412 - acc: 0.8388 - val_loss: 0.7475 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.71361\n",
      "Epoch 50/50\n",
      "33779/33779 [==============================] - 4s 109us/step - loss: 0.4370 - acc: 0.8420 - val_loss: 0.7893 - val_acc: 0.5878\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.71361\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [  4 115]\n",
      "Test set and training set proportion :  0.05167108430064284\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_61 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32823 samples, validate on 1696 samples\n",
      "Epoch 1/50\n",
      "32823/32823 [==============================] - 7s 223us/step - loss: 1.3733 - acc: 0.4292 - val_loss: 1.3433 - val_acc: 0.4334\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34327, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32823/32823 [==============================] - 4s 111us/step - loss: 1.1462 - acc: 0.5358 - val_loss: 1.1062 - val_acc: 0.5761\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34327 to 1.10620, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "32823/32823 [==============================] - 4s 108us/step - loss: 1.0890 - acc: 0.5848 - val_loss: 1.0363 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.10620 to 1.03633, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "32823/32823 [==============================] - 4s 107us/step - loss: 1.0013 - acc: 0.6050 - val_loss: 0.9938 - val_acc: 0.6221\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.03633 to 0.99381, saving model to weights.hdf5\n",
      "Epoch 5/50\n",
      "32823/32823 [==============================] - 4s 111us/step - loss: 0.9775 - acc: 0.6179 - val_loss: 1.1147 - val_acc: 0.5371\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99381\n",
      "Epoch 6/50\n",
      "32823/32823 [==============================] - 4s 110us/step - loss: 0.9327 - acc: 0.6392 - val_loss: 0.9488 - val_acc: 0.6362\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.99381 to 0.94880, saving model to weights.hdf5\n",
      "Epoch 7/50\n",
      "32823/32823 [==============================] - 4s 108us/step - loss: 0.8896 - acc: 0.6514 - val_loss: 0.9423 - val_acc: 0.6221\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.94880 to 0.94226, saving model to weights.hdf5\n",
      "Epoch 8/50\n",
      "32823/32823 [==============================] - 4s 115us/step - loss: 0.8642 - acc: 0.6634 - val_loss: 1.2995 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.94226\n",
      "Epoch 9/50\n",
      "32823/32823 [==============================] - 4s 107us/step - loss: 0.8341 - acc: 0.6799 - val_loss: 0.8752 - val_acc: 0.6374\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.94226 to 0.87519, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "32823/32823 [==============================] - 4s 112us/step - loss: 0.8351 - acc: 0.6834 - val_loss: 0.8041 - val_acc: 0.6728\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.87519 to 0.80406, saving model to weights.hdf5\n",
      "Epoch 11/50\n",
      "32823/32823 [==============================] - 4s 113us/step - loss: 0.7517 - acc: 0.7120 - val_loss: 0.7703 - val_acc: 0.6757\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.80406 to 0.77033, saving model to weights.hdf5\n",
      "Epoch 12/50\n",
      "32823/32823 [==============================] - 4s 110us/step - loss: 0.7394 - acc: 0.7159 - val_loss: 0.7966 - val_acc: 0.6144\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.77033\n",
      "Epoch 13/50\n",
      "32823/32823 [==============================] - 4s 112us/step - loss: 0.7398 - acc: 0.7178 - val_loss: 0.7556 - val_acc: 0.7017\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.77033 to 0.75562, saving model to weights.hdf5\n",
      "Epoch 14/50\n",
      "32823/32823 [==============================] - 4s 116us/step - loss: 0.6805 - acc: 0.7358 - val_loss: 0.7656 - val_acc: 0.6014\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.75562\n",
      "Epoch 15/50\n",
      "32823/32823 [==============================] - 4s 114us/step - loss: 0.6869 - acc: 0.7373 - val_loss: 0.6830 - val_acc: 0.7040\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.75562 to 0.68298, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "32823/32823 [==============================] - 4s 108us/step - loss: 0.6649 - acc: 0.7465 - val_loss: 1.8817 - val_acc: 0.3532\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.68298\n",
      "Epoch 17/50\n",
      "32823/32823 [==============================] - 4s 111us/step - loss: 0.7044 - acc: 0.7374 - val_loss: 0.6820 - val_acc: 0.6963\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.68298 to 0.68202, saving model to weights.hdf5\n",
      "Epoch 18/50\n",
      "32823/32823 [==============================] - 4s 112us/step - loss: 0.6314 - acc: 0.7575 - val_loss: 0.6591 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.68202 to 0.65912, saving model to weights.hdf5\n",
      "Epoch 19/50\n",
      "32823/32823 [==============================] - 4s 116us/step - loss: 0.6305 - acc: 0.7604 - val_loss: 0.7168 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.65912\n",
      "Epoch 20/50\n",
      "32823/32823 [==============================] - 4s 117us/step - loss: 0.6548 - acc: 0.7518 - val_loss: 0.7163 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.65912\n",
      "Epoch 21/50\n",
      "32823/32823 [==============================] - 3s 107us/step - loss: 0.6122 - acc: 0.7706 - val_loss: 0.7912 - val_acc: 0.6993\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.65912\n",
      "Epoch 22/50\n",
      "32823/32823 [==============================] - 4s 114us/step - loss: 0.6088 - acc: 0.7686 - val_loss: 0.7505 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.65912\n",
      "Epoch 23/50\n",
      "32823/32823 [==============================] - 4s 107us/step - loss: 0.6120 - acc: 0.7673 - val_loss: 0.7594 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.65912\n",
      "Epoch 24/50\n",
      "32823/32823 [==============================] - 4s 112us/step - loss: 0.5888 - acc: 0.7774 - val_loss: 0.7125 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.65912\n",
      "Epoch 25/50\n",
      "32823/32823 [==============================] - 3s 105us/step - loss: 0.5788 - acc: 0.7820 - val_loss: 0.7756 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.65912\n",
      "Epoch 26/50\n",
      "32823/32823 [==============================] - 4s 108us/step - loss: 0.5982 - acc: 0.7742 - val_loss: 0.8242 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.65912\n",
      "Epoch 27/50\n",
      "32823/32823 [==============================] - 3s 106us/step - loss: 0.5570 - acc: 0.7938 - val_loss: 0.6957 - val_acc: 0.7017\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.65912\n",
      "Epoch 28/50\n",
      "32823/32823 [==============================] - 4s 117us/step - loss: 0.5677 - acc: 0.7888 - val_loss: 0.6653 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.65912\n",
      "Epoch 29/50\n",
      "32823/32823 [==============================] - 4s 120us/step - loss: 0.5655 - acc: 0.7900 - val_loss: 0.7056 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.65912\n",
      "Epoch 30/50\n",
      "32823/32823 [==============================] - 4s 107us/step - loss: 0.5546 - acc: 0.7924 - val_loss: 0.6641 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.65912\n",
      "Epoch 31/50\n",
      "32823/32823 [==============================] - 4s 109us/step - loss: 0.5514 - acc: 0.7958 - val_loss: 0.6408 - val_acc: 0.6981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss improved from 0.65912 to 0.64083, saving model to weights.hdf5\n",
      "Epoch 32/50\n",
      "32823/32823 [==============================] - 4s 110us/step - loss: 0.5302 - acc: 0.8018 - val_loss: 0.6730 - val_acc: 0.7034\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.64083\n",
      "Epoch 33/50\n",
      "32823/32823 [==============================] - 4s 113us/step - loss: 0.5308 - acc: 0.8012 - val_loss: 0.7299 - val_acc: 0.7022\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.64083\n",
      "Epoch 34/50\n",
      "32823/32823 [==============================] - 3s 106us/step - loss: 0.5372 - acc: 0.8018 - val_loss: 0.6493 - val_acc: 0.7093\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.64083\n",
      "Epoch 35/50\n",
      "32823/32823 [==============================] - 4s 122us/step - loss: 0.5202 - acc: 0.8065 - val_loss: 0.6786 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.64083\n",
      "Epoch 36/50\n",
      "32823/32823 [==============================] - 4s 111us/step - loss: 0.5070 - acc: 0.8132 - val_loss: 0.6746 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.64083\n",
      "Epoch 37/50\n",
      "32823/32823 [==============================] - 4s 107us/step - loss: 0.5123 - acc: 0.8099 - val_loss: 0.6881 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.64083\n",
      "Epoch 38/50\n",
      "32823/32823 [==============================] - 4s 118us/step - loss: 0.5041 - acc: 0.8094 - val_loss: 0.6463 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.64083\n",
      "Epoch 39/50\n",
      "32823/32823 [==============================] - 4s 110us/step - loss: 0.4996 - acc: 0.8154 - val_loss: 0.6505 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.64083\n",
      "Epoch 40/50\n",
      "32823/32823 [==============================] - 4s 119us/step - loss: 0.4880 - acc: 0.8204 - val_loss: 0.7120 - val_acc: 0.7022\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.64083\n",
      "Epoch 41/50\n",
      "32823/32823 [==============================] - 4s 112us/step - loss: 0.4914 - acc: 0.8165 - val_loss: 0.6439 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.64083\n",
      "Epoch 42/50\n",
      "32823/32823 [==============================] - 4s 109us/step - loss: 0.4729 - acc: 0.8288 - val_loss: 0.6718 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.64083\n",
      "Epoch 43/50\n",
      "32823/32823 [==============================] - 4s 112us/step - loss: 0.4872 - acc: 0.8207 - val_loss: 0.6611 - val_acc: 0.7123\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.64083\n",
      "Epoch 44/50\n",
      "32823/32823 [==============================] - 4s 121us/step - loss: 0.4718 - acc: 0.8272 - val_loss: 0.6379 - val_acc: 0.7152\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.64083 to 0.63791, saving model to weights.hdf5\n",
      "Epoch 45/50\n",
      "32823/32823 [==============================] - 4s 125us/step - loss: 0.4697 - acc: 0.8265 - val_loss: 0.6616 - val_acc: 0.7028\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.63791\n",
      "Epoch 46/50\n",
      "32823/32823 [==============================] - 4s 122us/step - loss: 0.4789 - acc: 0.8224 - val_loss: 0.7224 - val_acc: 0.6999\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.63791\n",
      "Epoch 47/50\n",
      "32823/32823 [==============================] - 4s 122us/step - loss: 0.4575 - acc: 0.8311 - val_loss: 0.6361 - val_acc: 0.7117\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.63791 to 0.63612, saving model to weights.hdf5\n",
      "Epoch 48/50\n",
      "32823/32823 [==============================] - 4s 128us/step - loss: 0.4558 - acc: 0.8318 - val_loss: 0.6578 - val_acc: 0.7064\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.63612\n",
      "Epoch 49/50\n",
      "32823/32823 [==============================] - 4s 124us/step - loss: 0.4483 - acc: 0.8344 - val_loss: 0.7368 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.63612\n",
      "Epoch 50/50\n",
      "32823/32823 [==============================] - 4s 113us/step - loss: 0.4637 - acc: 0.8288 - val_loss: 0.6607 - val_acc: 0.7087\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.63612\n",
      "\n",
      "==================\n",
      "Model tested on subjects with no :  [21 26]\n",
      "Test set and training set proportion :  0.06307166394629053\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_64 (Conv1D)           (None, 705, 8)            520       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 352, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 337, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 168, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 165, 64)           4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 40,158\n",
      "Trainable params: 40,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32471 samples, validate on 2048 samples\n",
      "Epoch 1/50\n",
      "32471/32471 [==============================] - 7s 226us/step - loss: 1.3715 - acc: 0.4389 - val_loss: 1.1089 - val_acc: 0.5356\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10895, saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "32471/32471 [==============================] - 4s 111us/step - loss: 1.1787 - acc: 0.5310 - val_loss: 0.9806 - val_acc: 0.6147\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10895 to 0.98055, saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "32471/32471 [==============================] - 4s 117us/step - loss: 1.1411 - acc: 0.5390 - val_loss: 0.9279 - val_acc: 0.6611\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.98055 to 0.92791, saving model to weights.hdf5\n",
      "Epoch 4/50\n",
      "32471/32471 [==============================] - 4s 118us/step - loss: 1.1072 - acc: 0.5636 - val_loss: 1.1514 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.92791\n",
      "Epoch 5/50\n",
      "32471/32471 [==============================] - 4s 124us/step - loss: 1.0753 - acc: 0.5743 - val_loss: 1.0568 - val_acc: 0.6060\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.92791\n",
      "Epoch 6/50\n",
      "32471/32471 [==============================] - 4s 114us/step - loss: 1.0398 - acc: 0.5982 - val_loss: 1.0069 - val_acc: 0.6147\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.92791\n",
      "Epoch 7/50\n",
      "32471/32471 [==============================] - 4s 120us/step - loss: 0.9999 - acc: 0.6227 - val_loss: 0.7407 - val_acc: 0.7886\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.92791 to 0.74074, saving model to weights.hdf5\n",
      "Epoch 8/50\n",
      "32471/32471 [==============================] - 4s 121us/step - loss: 0.9635 - acc: 0.6328 - val_loss: 0.7468 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.74074\n",
      "Epoch 9/50\n",
      "32471/32471 [==============================] - 4s 114us/step - loss: 0.9077 - acc: 0.6586 - val_loss: 0.5980 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.74074 to 0.59795, saving model to weights.hdf5\n",
      "Epoch 10/50\n",
      "32471/32471 [==============================] - 4s 118us/step - loss: 0.8928 - acc: 0.6583 - val_loss: 0.6417 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.59795\n",
      "Epoch 11/50\n",
      "32471/32471 [==============================] - 4s 119us/step - loss: 0.8736 - acc: 0.6709 - val_loss: 0.6472 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.59795\n",
      "Epoch 12/50\n",
      "32471/32471 [==============================] - 4s 121us/step - loss: 0.8259 - acc: 0.6819 - val_loss: 0.6601 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.59795\n",
      "Epoch 13/50\n",
      "32471/32471 [==============================] - 4s 125us/step - loss: 0.8104 - acc: 0.6864 - val_loss: 0.8701 - val_acc: 0.6499\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.59795\n",
      "Epoch 14/50\n",
      "32471/32471 [==============================] - 4s 113us/step - loss: 0.8038 - acc: 0.6874 - val_loss: 0.6071 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.59795\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32471/32471 [==============================] - 4s 129us/step - loss: 0.7909 - acc: 0.6940 - val_loss: 0.5637 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.59795 to 0.56375, saving model to weights.hdf5\n",
      "Epoch 16/50\n",
      "32471/32471 [==============================] - 4s 118us/step - loss: 0.7490 - acc: 0.7116 - val_loss: 0.6240 - val_acc: 0.8354\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.56375\n",
      "Epoch 17/50\n",
      "32471/32471 [==============================] - 4s 116us/step - loss: 0.7434 - acc: 0.7117 - val_loss: 0.5714 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.56375\n",
      "Epoch 18/50\n",
      "32471/32471 [==============================] - 4s 127us/step - loss: 0.7429 - acc: 0.7172 - val_loss: 0.5032 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.56375 to 0.50317, saving model to weights.hdf5\n",
      "Epoch 19/50\n",
      "32471/32471 [==============================] - 3s 107us/step - loss: 0.7156 - acc: 0.7252 - val_loss: 0.4988 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.50317 to 0.49879, saving model to weights.hdf5\n",
      "Epoch 20/50\n",
      "32471/32471 [==============================] - 4s 127us/step - loss: 0.7144 - acc: 0.7239 - val_loss: 0.5489 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49879\n",
      "Epoch 21/50\n",
      "32471/32471 [==============================] - 4s 127us/step - loss: 0.6976 - acc: 0.7301 - val_loss: 0.4751 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.49879 to 0.47509, saving model to weights.hdf5\n",
      "Epoch 22/50\n",
      "32471/32471 [==============================] - 4s 120us/step - loss: 0.6875 - acc: 0.7354 - val_loss: 0.5055 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47509\n",
      "Epoch 23/50\n",
      "32471/32471 [==============================] - 4s 116us/step - loss: 0.6841 - acc: 0.7406 - val_loss: 0.6108 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47509\n",
      "Epoch 24/50\n",
      "32471/32471 [==============================] - 3s 107us/step - loss: 0.6892 - acc: 0.7364 - val_loss: 0.4343 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47509 to 0.43427, saving model to weights.hdf5\n",
      "Epoch 25/50\n",
      "32471/32471 [==============================] - 4s 112us/step - loss: 0.6527 - acc: 0.7505 - val_loss: 0.4511 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43427\n",
      "Epoch 26/50\n",
      "32471/32471 [==============================] - 4s 115us/step - loss: 0.6432 - acc: 0.7554 - val_loss: 0.4821 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43427\n",
      "Epoch 27/50\n",
      "32471/32471 [==============================] - 3s 106us/step - loss: 0.6519 - acc: 0.7511 - val_loss: 0.4000 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.43427 to 0.40004, saving model to weights.hdf5\n",
      "Epoch 28/50\n",
      "32471/32471 [==============================] - 4s 111us/step - loss: 0.6335 - acc: 0.7580 - val_loss: 0.4354 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.40004\n",
      "Epoch 29/50\n",
      "32471/32471 [==============================] - 4s 115us/step - loss: 0.6101 - acc: 0.7677 - val_loss: 0.4062 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.40004\n",
      "Epoch 30/50\n",
      "32471/32471 [==============================] - 4s 113us/step - loss: 0.6149 - acc: 0.7643 - val_loss: 0.4925 - val_acc: 0.8369\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.40004\n",
      "Epoch 31/50\n",
      "32471/32471 [==============================] - 4s 112us/step - loss: 0.6017 - acc: 0.7701 - val_loss: 0.4199 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.40004\n",
      "Epoch 32/50\n",
      "32471/32471 [==============================] - 4s 113us/step - loss: 0.5975 - acc: 0.7748 - val_loss: 0.3717 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.40004 to 0.37165, saving model to weights.hdf5\n",
      "Epoch 33/50\n",
      "32471/32471 [==============================] - 4s 113us/step - loss: 0.5841 - acc: 0.7787 - val_loss: 0.3999 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.37165\n",
      "Epoch 34/50\n",
      "32471/32471 [==============================] - 4s 117us/step - loss: 0.5877 - acc: 0.7800 - val_loss: 0.4071 - val_acc: 0.8770\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.37165\n",
      "Epoch 35/50\n",
      "32471/32471 [==============================] - 4s 128us/step - loss: 0.5672 - acc: 0.7885 - val_loss: 0.3952 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.37165\n",
      "Epoch 36/50\n",
      "32471/32471 [==============================] - 4s 116us/step - loss: 0.5715 - acc: 0.7864 - val_loss: 0.4092 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.37165\n",
      "Epoch 37/50\n",
      "32471/32471 [==============================] - 4s 117us/step - loss: 0.5578 - acc: 0.7885 - val_loss: 0.4589 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.37165\n",
      "Epoch 38/50\n",
      "32471/32471 [==============================] - 4s 115us/step - loss: 0.5537 - acc: 0.7938 - val_loss: 0.4062 - val_acc: 0.8770\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.37165\n",
      "Epoch 39/50\n",
      "32471/32471 [==============================] - 4s 119us/step - loss: 0.5339 - acc: 0.8000 - val_loss: 0.4502 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.37165\n",
      "Epoch 40/50\n",
      "32471/32471 [==============================] - 4s 119us/step - loss: 0.5464 - acc: 0.7958 - val_loss: 0.4140 - val_acc: 0.8853\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.37165\n",
      "Epoch 41/50\n",
      "32471/32471 [==============================] - 4s 117us/step - loss: 0.5618 - acc: 0.7922 - val_loss: 0.3441 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.37165 to 0.34407, saving model to weights.hdf5\n",
      "Epoch 42/50\n",
      "32471/32471 [==============================] - 4s 122us/step - loss: 0.5257 - acc: 0.8021 - val_loss: 0.4312 - val_acc: 0.8428\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.34407\n",
      "Epoch 43/50\n",
      "32471/32471 [==============================] - 4s 115us/step - loss: 0.5141 - acc: 0.8100 - val_loss: 0.5193 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.34407\n",
      "Epoch 44/50\n",
      "32471/32471 [==============================] - 4s 122us/step - loss: 0.5360 - acc: 0.7993 - val_loss: 0.3539 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.34407\n",
      "Epoch 45/50\n",
      "32471/32471 [==============================] - 4s 117us/step - loss: 0.5185 - acc: 0.8062 - val_loss: 0.3947 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.34407\n",
      "Epoch 46/50\n",
      " 7500/32471 [=====>........................] - ETA: 2s - loss: 0.5171 - acc: 0.8119"
     ]
    }
   ],
   "source": [
    "selected_model = 'RNN'\n",
    "\n",
    "raw_models = ['CNN', 'MLP', 'RNN']\n",
    "#LOO-Validation on each subject\n",
    "if selected_model in raw_models:\n",
    "    X = X_raw\n",
    "    y = y_raw\n",
    "    s = s_raw\n",
    "else:\n",
    "    X = X_prep\n",
    "    y = y_prep\n",
    "    s = s_prep\n",
    "    \n",
    "subject_list = np.unique(s_raw)\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "y_score_all = []\n",
    "np.random.seed(44)\n",
    "iterations = shuffle(np.array(list(itertools.combinations(subject_list, 2))))\n",
    "\n",
    "it_index = iterations[:20]\n",
    "for subject in it_index: # two combination of tested subjects\n",
    "    if os.path.exists('weights.hdf5'):\n",
    "        os.remove('weights.hdf5')\n",
    "    print(\"==================\")\n",
    "    print('Model tested on subjects with no : ', subject)\n",
    "    mask_test = np.logical_or(s==subject[0], s== subject[1])\n",
    "    mask_training = np.logical_and(s!=subject[0], s!= subject[1])\n",
    "    X_test = X[mask_test]\n",
    "    y_test = y[mask_test] -1\n",
    "    X_training = X[mask_training] \n",
    "    y_training = y[mask_training] -1\n",
    "    print(\"Test set and training set proportion : \",float(X_test.shape[0])/X_training.shape[0] )\n",
    "    \n",
    "    class_weight = get_class_weights(y_training)\n",
    "\n",
    "    if not selected_model in raw_models : # Models will train on processed features\n",
    "        if selected_model == \"LogReg\":\n",
    "            model = LogisticRegression(class_weight = class_weight)\n",
    "        elif selected_model == \"SVM\":\n",
    "            model = svm.SVC( class_weight = class_weight, probability=True)\n",
    "        elif selected_model == 'RForest':\n",
    "            model = RandomForestClassifier(max_depth=10, n_estimators=20, class_weight=class_weight)\n",
    "        \n",
    "        model.fit(X_training, y_training)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "    else: # Models will train on raw data\n",
    "        callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=15,\n",
    "                              verbose=0, mode='auto'),\n",
    "            keras.callbacks.ModelCheckpoint(filepath=\"weights.hdf5\", verbose=1, save_best_only=True)]\n",
    "        \n",
    "        X_training = keras.preprocessing.sequence.pad_sequences(X_training, maxlen=128, dtype='float64',\n",
    "            padding='pre', truncating='pre', value=0.)\n",
    "        X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=128, dtype='float64',\n",
    "            padding='pre', truncating='pre', value=0.)\n",
    "        \n",
    "        batch_size = 1500\n",
    "        n_classes = 6\n",
    "        \n",
    "        if selected_model =='CNN':\n",
    "            train_shape = X_training.shape\n",
    "            test_shape = X_test.shape\n",
    "            X_training = X_training.reshape(train_shape[0], -1,1)\n",
    "            X_test = X_test.reshape(test_shape[0], -1,1)\n",
    "            timesteps = len(X_training[0])\n",
    "            model = CNN()\n",
    "        elif selected_model =='RNN':\n",
    "            train_shape = X_training.shape\n",
    "            test_shape = X_test.shape\n",
    "            X_training = X_training.reshape(train_shape[0], -1,1)\n",
    "            X_test = X_test.reshape(test_shape[0], -1,1)\n",
    "            timesteps = len(X_training[0])\n",
    "            model = RNN()\n",
    "        elif selected_model =='MLP':\n",
    "            train_shape = X_training.shape\n",
    "            test_shape = X_test.shape\n",
    "            X_train = X_train.reshape(train_shape[0], -1)\n",
    "            X_test = X_test.reshape(test_shape[0], -1)\n",
    "            timesteps = len(X_training[0])\n",
    "            model = MLP()\n",
    "            \n",
    "        # Compile model\n",
    "#         multi_model = multi_gpu_model(model = model, gpus = 1)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "        history =model.fit(X_training,\n",
    "                  one_hot(y_training),\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(X_test, one_hot(y_test)),\n",
    "                  epochs=50,\n",
    "                  callbacks=callbacks)\n",
    "        \n",
    "        model.load_weights('weights.hdf5')\n",
    "        y_pred = model.predict(X_test).argmax(1)\n",
    "        y_score = model.predict(X_test)\n",
    "        print(\"\")\n",
    "        \n",
    "    y_score_all.extend(y_score)\n",
    "    y_test_all.extend(list(y_test))\n",
    "    y_pred_all.extend(list(np.argmax(pd.rolling_mean(y_score, 50,min_periods=1), axis=1)))\n",
    "    '''plt.figure()\n",
    "    plt.plot(pd.rolling_mean( y_score, 50,min_periods=1), \"-\")\n",
    "    plt.xlim([0, 1.5*len(y_score)])\n",
    "    plt.legend(['walking',\n",
    "            'walking upstairs',\n",
    "            'walking downstairs',\n",
    "            'sitting',\n",
    "            'standing',\n",
    "            'laying'], loc='upper_left')\n",
    "    a = np.asarray(y_test).astype(np.int)\n",
    "    y_ticks = np.asarray(['walking',\n",
    "            'walking upstairs',\n",
    "            'walking downstairs',\n",
    "            'sitting',\n",
    "            'standing',\n",
    "            'laying'])[a]\n",
    "    plt.plot(y_ticks)\n",
    "    plt.xlim([0, 1.5*len(y_score)])'''\n",
    "#     plt.savefig('Figures/'+str(subject[0])+'_'+str(subject[1])+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f0c014b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f05c9f160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_dict = {1:'walking',\n",
    "             2:'walking_upstairs',\n",
    "             3:'walking_downstairs',\n",
    "             4:'sitting',\n",
    "             5:'standing',\n",
    "             6:'laying'}\n",
    "class_names = list(label_dict.values())\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test_all, y_pred_all)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_cm(cnf_matrix, class_names, normalize=False)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_cm(cnf_matrix, class_names, normalize=True)\n",
    "# plt.savefig(save_with_name+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the Model 0.7906203840472673\n"
     ]
    }
   ],
   "source": [
    "true_pos = np.diag(cnf_matrix).sum()\n",
    "accuracy = true_pos/float(cnf_matrix.sum())\n",
    "print('Overall accuracy of the Model', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           walking       1.00      0.85      0.92       300\n",
      "  walking_upstairs       0.91      0.87      0.89       457\n",
      "walking_downstairs       0.89      0.84      0.87       443\n",
      "           sitting       0.64      1.00      0.78       752\n",
      "          standing       0.86      1.00      0.92       364\n",
      "            laying       0.00      0.00      0.00       392\n",
      "\n",
      "       avg / total       0.70      0.79      0.73      2708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_all, y_pred_all,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273fbc3048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auc_roc_plot(one_hot(np.array(y_test_all)), np.array(y_score_all), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weights.hdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-34902cef301e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights.hdf5'"
     ]
    }
   ],
   "source": [
    "os.remove('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train_used)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test_used)  # 2947 testing series\n",
    " \n",
    "timesteps = len(X_train_used[0]) # 128 timesteps per series\n",
    "# input_dim = len(X_train_used[0][0]) # 6 input parameters per timestep\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 300\n",
    "# Loop 300 times on the dataset\n",
    "batch_size = 1500\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X test shape, y test shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test_used.shape, y_test.shape, np.mean(X_test_used), np.std(X_test_used))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
